<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Introduction to Natural Language Processing for Research: Transformers for Natural Language Processing</title><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png"><link rel="manifest" href="site.webmanifest"><link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#ffffff"></head><body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="background-color: #FF4955; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #000">
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
            Pre-Alpha
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/05-transformers.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Introduction to Natural Language Processing for Research
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Introduction to Natural Language Processing for Research
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to Natural Language Processing for Research
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 51%" class="percentage">
    51%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 51%" aria-valuenow="51" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/05-transformers.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->
      
            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction to Natural Language Processing</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-text-preprocessing.html">2. Introduction to Text Preprocessing</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-text-analysis.html">3. Text Analysis</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-word-embedding.html">4. Word Embedding</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        5. Transformers for Natural Language Processing
        </span>
      </button>
    </div><!--/div.accordion-header-->
        
    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#introduction-to-artificial-neural-networks">5.1. Introduction to Artificial Neural Networks</a></li>
<li><a href="#transformers">5.2. Transformers</a></li>
<li><a href="#sentiment-analysis">5.3. Sentiment Analysis</a></li>
<li><a href="#text-summarization">5.4. Text Summarization</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-llms.html">6. Large Language Models</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-domain-specific-llms.html">7. Domain-Specific LLMs</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="08-conclusion-final-project.html">8. Wrap-up and Final Project</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width resources"><a href="aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">
            
            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="04-word-embedding.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="06-llms.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="04-word-embedding.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Word Embedding
        </a>
        <a class="chapter-link float-end" href="06-llms.html" rel="next">
          Next: Large Language... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Transformers for Natural Language Processing</h1>
        <p>Last updated on 2024-05-15 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/05-transformers.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
        
        
        
        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>How do Transformers work?</li>
<li>How can I use Transformers for text analysis?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>To be able to describe Transformers’ architecture.</li>
<li>To be able to implement sentiment analysis, and text summarization
using transformers.</li>
</ul></div>
</div>
</div>
</div>
</div>
<p>Transformers have revolutionized the field of NLP since their
introduction by the Google team in 2017. Unlike previous models that
processed text sequentially, Transformers use an attention mechanism to
process all words at once, allowing them to capture context more
effectively. This parallel processing capability enables Transformers to
handle long-range dependencies and understand the nuances of language
better than their predecessors. For now, try to recognize the building
blocks of the general structure of a transformer</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/7ec8bb01-aa7e-4378-af45-ae28fbe8916b" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><section id="introduction-to-artificial-neural-networks"><h2 class="section-heading">5.1. Introduction to Artificial Neural Networks<a class="anchor" aria-label="anchor" href="#introduction-to-artificial-neural-networks"></a>
</h2>
<hr class="half-width"><p>To understand how Transformers work we also need to learn about
artificial neural networks (ANNs). Imagine a neural network as a team of
workers in a factory. Each worker (neuron) has a specific task
(processing information), and they pass their work along to the next
person in line until the final product (output) is created.</p>
<p>Just like a well-organized assembly line, a neural network processes
information in stages, with each neuron contributing to the final
result.</p>
<div id="activity" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Take a look at the architecture of a simple ANN below.
Identify the underlying layers and components of this ANN and add the
correct name label to each one.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/445b963e-caf1-451d-8edc-e686f8950ae5" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure></div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/523924b5-055b-4125-8bce-aa2be2b38ca8" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure></div>
</div>
</div>
</div>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What is Multilayer Perceptron then?</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler1" aria-labelledby="headingSpoiler1">
<div class="accordion-body">
<p>In the context of machine learning, a multilayer perceptron (MLP) is
indeed a fully connected multi-layer neural network and is a classic
example of a feedforward artificial neural network (ANN). It typically
includes an input layer, one or more hidden layers, and an output layer.
When an MLP has more than one hidden layer, it can be considered a deep
ANN, part of a broader category known as deep learning.</p>
</div>
</div>
</div>
</div>
<div id="summation-and-activation-function" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="summation-and-activation-function" class="callout-inner">
<h3 class="callout-title">Summation and Activation Function<a class="anchor" aria-label="anchor" href="#summation-and-activation-function"></a>
</h3>
<div class="callout-content">
<p>If we zoom into a neuron in the hidden layer, we can see the
mathematical operations (weights summation and activation function). An
input is transformed at each hidden layer node through a process that
multiplies the input (x_i) by learned weights (w_i), adds a bias (b),
and then applies an activation function to determine the node’s output.
This output is either passed on to the next layer or contributes to the
final output of the network. Essentially, each node performs a small
calculation that, when combined with the operations of other nodes,
allows the network to process complex patterns and data.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/d1006506-ac54-43bc-b6e9-5181ca98be36" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure></div>
</div>
</div>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What happens next? How to optimize an ANN?</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler2" aria-labelledby="headingSpoiler2">
<div class="accordion-body">
<p>Backpropagation is an algorithmic cornerstone in the training of
ANNs, serving as a method for optimizing weights and biases through
gradient descent. Conceptually, it is akin to an iterative refinement
process where the network’s output error is propagated backward, layer
by layer, using the chain rule of calculus. This backward flow of error
information allows for the computation of gradients, which inform the
magnitude and direction of adjustments to be made to the network’s
parameters. The objective is to iteratively reduce the differences
between the predicted output and the actual target values. This
systematic adjustment of parameters, guided by error gradients,
incrementally leads to a more accurate ANN model.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/5ae4d845-c3d2-42df-87f0-e928be9ba64b" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure></div>
</div>
</div>
</div>
<div id="challenge" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge"></a>
</h3>
<div class="callout-content">
<p>Teamwork: When we talk about ANNs, we also consider their parameters.
But what are the parameters? Draw a small neural network with 3
following layers: x1</p>
<ul><li>Input Layer: 3 neurons</li>
<li>Hidden Layer: 4 neurons</li>
<li>Output Layer: 1 neurons</li>
</ul><ol style="list-style-type: decimal"><li>Connect each neuron in the input layer to every neuron in the hidden
layer (next layer). How many connections (weights) do we have?</li>
<li>Now, add a bias for each neuron in the hidden layer. How many biases
do we have?</li>
<li>Repeat the process for the hidden layer to the output layer.</li>
</ol></div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/144a8f7d-c1ac-4b57-8688-b5280cabd59c" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><ul><li>(3 { neurons} x 4 { neurons} + 4{ biases}) = 16</li>
<li>(4 { neurons} x 1 { neurons} + 1{ biases}) = 5</li>
<li>Total parameters for this network: (16 + 5 = 21)</li>
</ul></div>
</div>
</div>
</div>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-1"></a>
</h3>
<div class="callout-content">
<p>Q: Add another hidden layer with 4 neurons to the previous ANN and
calculate the number of parameters.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/39820eb0-7959-4ed1-bdfc-69131ab1a834" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>We would add: - (4 * 4) weights from the first to the second hidden
layer - (4) biases for the new hidden layer - (4 * 1) weights from the
second hidden layer to the output layer (we already counted the biases
for the output layer)</p>
<p>That’s an additional (16 + 4 = 20) parameters, bringing our total to
(21 + 20 = 41) parameters.</p>
</div>
</div>
</div>
</div>
</section><section id="transformers"><h2 class="section-heading">5.2. Transformers<a class="anchor" aria-label="anchor" href="#transformers"></a>
</h2>
<hr class="half-width"><p>As mentioned in the introduction, Most of the recent NLP models are
built based on Transformers. Building on our understanding of ANNs,
let’s explore the architecture of transformers. Transformers consist of
several key components that work together to process and generate
data.</p>
<div id="activity-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity-1" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: We go back to the first figure of this episode. In the
simplified schematic below, write the function of each component in the
allocated textbox:</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/e94c677a-53c5-4da3-87ac-e45960429986" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure></div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<p>A: <img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/25823a7c-6059-4c83-b592-a273fa59b9ec" alt="image" class="figure"></p>
<p>Briefly, we can say:</p>
<ul><li>Encoder: Processes input text into contextualized representations,
enabling the understanding of the context within the input sequence. It
is like the ‘listener’ in a conversation, taking in information and
understanding it.</li>
<li>Decoder: Generates output sequences by translating the
contextualized representations from the encoder into coherent text,
often using mechanisms like masked multi-head attention and
encoder-decoder attention to maintain sequence order and coherence. This
acts as the ‘speaker’ in the conversation, generating the output based
on the information processed by the encoder.</li>
<li>Positional Encoding: Adds unique information to each word embedding,
indicating the word’s position in the sequence, which is essential for
the model to maintain the order of words and understand their relative
positions within a sentence</li>
<li>Input Embedding: The input text is converted into vectors that the
model can understand. Think of it as translating words into a secret
code that the transformer can read.</li>
<li>Output Embedding: Similar to input embedding, but for the output
text. It translates the transformer’s secret code back into words we can
understand.</li>
<li>Softmax Output: Applies the softmax function to the final layer’s
outputs to convert them into a probability distribution, which helps in
tasks like classification and sequence generation by selecting the most
likely next word or class. It is like choosing the best response in a
conversation from many options.</li>
</ul></div>
</div>
</div>
</div>
<div id="attention-mechanism" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="attention-mechanism" class="callout-inner">
<h3 class="callout-title">Attention Mechanism<a class="anchor" aria-label="anchor" href="#attention-mechanism"></a>
</h3>
<div class="callout-content">
<p>So far, we have learned what the architecture of a transformer block
looks like. However, for simplicity, many parts of this architecture
have not been considered.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/559580aa-f2c9-4ec0-b87d-7e1438839431" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>In the following section, we will show the underlying components of a
transformer.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/2b325dc1-20d8-4bac-91b8-030067ee8097" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>For more details see <a href="https://arxiv.org/abs/1706.03762" class="external-link">source</a>.</p>
<p>Attention mechanisms in transformers, allow LLMs to focus on
different parts of the input text to understand context and
relationships between words. The concept of ‘attention’ in encoders and
decoders is akin to the selective focus of ‘fast reading,’ where one
zeroes in on crucial information and disregards the irrelevant. This
mechanism adapts to the context of a query, emphasizing different words
or tokens based on the query’s intent. For instance, in the sentence
“Sarah went to a restaurant to meet her friend that night,” the words
highlighted would vary depending on whether the question is about the
action (What?), location (Where?), individuals involved (Who?), or time
(When?).</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/f28a1a04-40f7-4ad3-a846-4f0d4c4ddee4" alt="image" class="figure"><a href="https://medium.com/@hunter-j-phillips/multi-head-attention-7924371d477a" class="external-link">source</a></p>
<p>In transformer models, this selective focus is achieved through
‘queries,’ ‘keys,’ and ‘values,’ all represented as vectors. A query
vector seeks out the closest key vectors, which are encoded
representations of values. The relationship between words, like ‘where’
and ‘restaurant,’ is determined by their frequency of co-occurrence in
sentences, allowing the model to assign greater attention to
‘restaurant’ when the query pertains to a location. This dynamic
adjustment of focus enables transformers to process language with a
nuanced understanding of context and relevance.</p>
</div>
</div>
</div>
<div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Have you heard of any other applications of the
Transformers rather than in NLPs? Explain why transformers can be useful
for other AI applications. Share your thoughts and findings with other
groups.</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">Show me the solution</h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" data-bs-parent="#accordionSolution5" aria-labelledby="headingSolution5">
<div class="accordion-body">
<p>A: Transformers, initially popular in NLP, have found applications
beyond text analysis. They excel in computer vision, speech recognition,
and even genomics. Their versatility extends to music generation and
recommendation systems. Transformers’ innovative architecture allows
them to adapt to diverse tasks, revolutionizing AI applications.</p>
</div>
</div>
</div>
</div>
<div id="transformers-in-text-translation" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="transformers-in-text-translation" class="callout-inner">
<h3 class="callout-title">Transformers in Text Translation<a class="anchor" aria-label="anchor" href="#transformers-in-text-translation"></a>
</h3>
<div class="callout-content">
<p>Imagine you want to translate the sentence “What time is it?” from
English to German using a transformer. The input embedding layer
converts each English word into a vector. The six layers of encoders
process these vectors, understanding the context of the sentence. The
six layers of decoders then start generating the German translation, one
word at a time.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/09e7ae0f-bfc1-4d7c-be5f-443b97c05bd3" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>For each word, the Softmax output predicts the most likely next word
in German. The output embedding layer converts these predictions back
into readable German words. By the end, you get the German translation
of <strong>“What time is it?”</strong> as <strong>“Wie spät ist
es?”</strong></p>
</div>
</div>
</div>
<div id="pipelines" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="pipelines" class="callout-inner">
<h3 class="callout-title">Pipelines<a class="anchor" aria-label="anchor" href="#pipelines"></a>
</h3>
<div class="callout-content">
<p>The pipeline module from Hugging Face’s <a href="https://huggingface.co/docs/transformers/en/main_classes/pipelines" class="external-link">transformers
library</a> is a high-level API that simplifies the use of complex
machine learning models for a variety of NLP tasks. It is a versatile
tool for NLP tasks, enabling users to perform <strong>text
generation</strong>, <strong>sentiment analysis</strong>,
<strong>question answering</strong>, <strong>summarization</strong>, and
<strong>translation</strong> with minimal code. By abstracting away the
intricacies of model selection, tokenization, and output generation, the
pipeline module makes state-of-the-art AI accessible to developers of
all skill levels, allowing them to harness the power of language models
efficiently and intuitively.</p>
</div>
</div>
</div>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What are other sequential learning models?</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler3" aria-labelledby="headingSpoiler3">
<div class="accordion-body">
<p>Transformers are essential for NLP tasks because they overcome the
limitations of earlier models like recurrent neural networks (RNNs) and
long short-term memory models (LSTMs), which struggled with long
sequences and were computationally intensive respectively. Transformers,
in contrast to the sequential input processing of RNNs, handle entire
sequences simultaneously. This parallel processing capability enables
data scientists to employ GPUs to train large language models (LLMs)
based on transformers, which markedly decreases the duration of
training.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/a0a429e4-c87e-4529-9151-781dd566c800" alt="rnn-transf-nlp" class="figure"><a href="https://thegradient.pub/transformers-are-graph-neural-networks/" class="external-link">source</a></p>
</div>
</div>
</div>
</div>
</section><section id="sentiment-analysis"><h2 class="section-heading">5.3. Sentiment Analysis<a class="anchor" aria-label="anchor" href="#sentiment-analysis"></a>
</h2>
<hr class="half-width"><p>Sentiment analysis is a powerful tool in NLP that helps determine the
emotional tone behind the text. It is used to understand opinions,
sentiments, emotions, and attitudes from various entities and classify
them according to their polarity.</p>
<div id="activity-2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="activity-2" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-2"></a>
</h3>
<div class="callout-content">
<p>Teamwork: How do you categorize the following text in terms of
positive and negative sounding? Select an Emoji.</p>
<p><em>“A research team has unveiled a novel ligand exchange technique
that enables the synthesis of organic cation-based perovskite quantum
dots (PQDs), ensuring exceptional stability while suppressing internal
defects in the photoactive layer of solar cells.”</em> <a href="https://www.sciencedaily.com/releases/2024/02/240221160400.htm" class="external-link">source</a></p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/897d0938-d84f-4189-afac-b8f244e16b46" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure></div>
</div>
</div>
<p>Computer models can do this job for us! Let’s see how it works
through a step-by-step example: First, install the required libraries
and pipelines:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>pip install transformers</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code></pre>
</div>
<p>Now, initialize the sentiment analysis pipeline and analyze the
sentiment of a sample text:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>sentiment_pipeline <span class="op">=</span> pipeline(<span class="st">'sentiment-analysis'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>text <span class="op">=</span> <span class="st">" A research team has unveiled a novel ligand exchange technique that enables the synthesis of organic cation-based perovskite quantum dots (PQDs), ensuring exceptional stability while suppressing internal defects in the photoactive layer of solar cells."</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>sentiment <span class="op">=</span> sentiment_pipeline(text)</span></code></pre>
</div>
<p>After the analysis is completed, you can print out the results:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sentiment: </span><span class="sc">{</span>sentiment[<span class="dv">0</span>][<span class="st">'label'</span>]<span class="sc">}</span><span class="ss">, Confidence: </span><span class="sc">{</span>sentiment[<span class="dv">0</span>][<span class="st">'score'</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>Output: Sentiment: POSITIVE, Confidence: <span class="fl">1.00</span></span></code></pre>
</div>
<p>In this example, the sentiment analysis pipeline from the Hugging
Face library is used to analyze the sentiment of a research paper
abstract. The model predicts the sentiment as positive, negative, or
neutral, along with a confidence score. This can be particularly useful
for gauging the reception of research papers in a field.</p>
<div id="activity-3" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="activity-3" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-3"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Fill in the blanks to complete the sentiment analysis
process: Install the __________ library for sentiment analysis. Use the
__________ function to create a sentiment analysis pipeline. The
sentiment analysis model will output a __________ and a __________
score.</p>
</div>
</div>
</div>
<div id="vadrer" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="vadrer" class="callout-inner">
<h3 class="callout-title">VADRER<a class="anchor" aria-label="anchor" href="#vadrer"></a>
</h3>
<div class="callout-content">
<p>Valence Aware Dictionary and sEntiment Reasoner (VADER) is a lexicon
and rule-based sentiment analysis tool that is particularly attuned to
sentiments expressed in social media. VADER analyzes the sentiment of
the text and returns a dictionary with scores for negative, neutral,
positive, and a compound score that aggregates them. It is useful for
quick sentiment analysis, especially on social media texts. Let’s how we
can use this framework.</p>
<p>First, we need to import the SentimentIntensityAnalyzer module from
VADER library:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="im">from</span> vaderSentiment.vaderSentiment <span class="im">import</span> SentimentIntensityAnalyzer</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co"># Initialize VADER sentiment intensity analyzer:</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>analyzer <span class="op">=</span> SentimentIntensityAnalyzer()</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co"># We use the same sample text:</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>text <span class="op">=</span> <span class="st">" A research team has unveiled a novel ligand exchange technique that enables the synthesis of organic cation-based perovskite quantum dots (PQDs), ensuring exceptional stability while suppressing internal defects in the photoactive layer of solar cells."</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="co"># Now we can analyze sentiment:</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>vader_sentiment <span class="op">=</span> analyzer.polarity_scores(text)</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a><span class="co"># Print the sentiment:</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sentiment: </span><span class="sc">{</span>vader_sentiment<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>Output: Sentiment: {<span class="st">'neg'</span>: <span class="fl">0.069</span>, <span class="st">'neu'</span>: <span class="fl">0.818</span>, <span class="st">'pos'</span>: <span class="fl">0.113</span>, <span class="st">'compound'</span>: <span class="fl">0.1779</span>}</span></code></pre>
</div>
</div>
</div>
</div>
<div id="discussion-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Which framework do you think could be more helpful for
research applications? Elaborate your opinion. Share your thoughts with
other team members.</p>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6">Show me the solution</h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" data-bs-parent="#accordionSolution6" aria-labelledby="headingSolution6">
<div class="accordion-body">
<p>A: Transformers use deep learning models that can understand context
and nuances of language, making them suitable for complex and lengthy
texts. They can be particularly useful for sentiment analysis of
research papers, as they can understand the complex language and context
often found in academic writing. This allows for a more nuanced
understanding of the sentiment conveyed in the papers. VADER, on the
other hand, is a rule-based model that excels in analyzing short texts
with clear sentiment expressions, often found in social media.</p>
</div>
</div>
</div>
</div>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-2"></a>
</h3>
<div class="callout-content">
<p>Use the transformers library to perform sentiment analysis on the
following text:</p>
<p><em>“Perovskite nanocrystals have emerged as a promising class of
materials for next-generation optoelectronic devices due to their unique
properties. Their crystal structure allows for tunable bandgaps, which
are the energy differences between occupied and unoccupied electronic
states. This tunability enables the creation of materials that can
absorb and emit light across a wide range of the electromagnetic
spectrum, making them suitable for applications like solar cells,
light-emitting diodes (LEDs), and lasers.”</em></p>
<p>Print the original text and the sentiment score and label. You can
use the following code to load the transformers library and the
pre-trained model and tokenizer for sentiment analysis:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>sentiment_analysis <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution7" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution7" aria-expanded="false" aria-controls="collapseSolution7">
  <h4 class="accordion-header" id="headingSolution7">Show me the solution</h4>
</button>
<div id="collapseSolution7" class="accordion-collapse collapse" data-bs-parent="#accordionSolution7" aria-labelledby="headingSolution7">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>sentiment_analysis <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"This book is amazing. It is well-written, engaging, and informative. I learned a lot from reading it and I highly recommend it to anyone interested in natural language processing."</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="bu">print</span>(text)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="bu">print</span>(sentiment_analysis(text))</span></code></pre>
</div>
<p>Output:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>output: <span class="st">"Perovskite nanocrystals have emerged as a promising class of materials for next-generation optoelectronic devices due to their unique properties. Their crystal structure allows for tunable bandgaps, which are the energy differences between occupied and unoccupied electronic states. This tunability enables the creation of materials that can absorb and emit light across a wide range of the electromagnetic spectrum, making them suitable for applications like solar cells, light-emitting diodes (LEDs), and lasers."</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>[{<span class="st">'label'</span>: <span class="st">'POSITIVE'</span>, <span class="st">'score'</span>: <span class="fl">0.9998656511306763</span>}]</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-3"></a>
</h3>
<div class="callout-content">
<p>Comparing Transformer with VADER on a large size text. Use the
Huggingface library database.</p>
</div>
</div>
</div>
<div id="accordionSolution8" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution8" aria-expanded="false" aria-controls="collapseSolution8">
  <h4 class="accordion-header" id="headingSolution8">Show me the solution</h4>
</button>
<div id="collapseSolution8" class="accordion-collapse collapse" data-bs-parent="#accordionSolution8" aria-labelledby="headingSolution8">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</section><section id="text-summarization"><h2 class="section-heading">5.4. Text Summarization<a class="anchor" aria-label="anchor" href="#text-summarization"></a>
</h2>
<hr class="half-width"><p>Text summarization is the process of distilling the most important
information from a source (or sources) to produce an abbreviated version
for a particular user and task. It can be broadly classified into two
types: extractive and abstractive summarization.</p>
<div id="discussion-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion-2" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-2"></a>
</h3>
<div class="callout-content">
<p>How extractive and abstractive summarization methods are different?
Connect the following text boxes to the correct category. Share your
results with other group members.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/1630ffcf-90c8-49df-9abe-98a739fd58ef" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure></div>
</div>
</div>
<div id="accordionSolution9" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution9" aria-expanded="false" aria-controls="collapseSolution9">
  <h4 class="accordion-header" id="headingSolution9">Show me the solution</h4>
</button>
<div id="collapseSolution9" class="accordion-collapse collapse" data-bs-parent="#accordionSolution9" aria-labelledby="headingSolution9">
<div class="accordion-body">
<p>A: <img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/166c2025-14b0-4f1b-aee5-3f46d4f3c8b4" alt="image" class="figure"></p>
</div>
</div>
</div>
</div>
<p>Now, let’s see how to use the Hugging Face Transformers library to
perform abstractive summarization. First, from the transformers import
pipeline:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="co"># Initialize the summarization pipeline</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span></code></pre>
</div>
<p>Input a sample text from an article from <a href="https://www.sciencedaily.com/releases/2024/02/240221160400.htm#:~:text=Summary%3A,photoactive%20layer%20of%20solar%20cells." class="external-link">source</a>:</p>
<div id="accordionSpoiler4" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler4" aria-expanded="false" aria-controls="collapseSpoiler4">
  <h3 class="accordion-header" id="headingSpoiler4">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Input Text</h3>
</button>
<div id="collapseSpoiler4" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler4" aria-labelledby="headingSpoiler4">
<div class="accordion-body">
<p>text = “A groundbreaking research breakthrough in solar energy has
propelled the development of the world’s most efficient quantum dot (QD)
solar cell, marking a significant leap towards the commercialization of
next-generation solar cells. This cutting-edge QD solution and device
have demonstrated exceptional performance, retaining their efficiency
even after long-term storage. Led by Professor Sung-Yeon Jang from the
School of Energy and Chemical Engineering at UNIST, a team of
researchers has unveiled a novel ligand exchange technique. This
innovative approach enables the synthesis of organic cation-based
perovskite quantum dots (PQDs), ensuring exceptional stability while
suppressing internal defects in the photoactive layer of solar cells.
Our developed technology has achieved an impressive 18.1% efficiency in
QD solar cells,” stated Professor Jang. This remarkable achievement
represents the highest efficiency among quantum dot solar cells
recognized by the National Renewable Energy Laboratory (NREL) in the
United States. The increasing interest in related fields is evident, as
last year, three scientists who discovered and developed QDs, as
advanced nanotechnology products, were awarded the Nobel Prize in
Chemistry. QDs are semiconducting nanocrystals with typical dimensions
ranging from several to tens of nanometers, capable of controlling
photoelectric properties based on their particle size. PQDs, in
particular, have garnered significant attention from researchers due to
their outstanding photoelectric properties. Furthermore, their
manufacturing process involves simple spraying or application to a
solvent, eliminating the need for the growth process on substrates. This
streamlined approach allows for high-quality production in various
manufacturing environments. However, the practical use of QDs as solar
cells necessitates a technology that reduces the distance between QDs
through ligand exchange, a process that binds a large molecule, such as
a ligand receptor, to the surface of a QD. Organic PQDs face notable
challenges, including defects in their crystals and surfaces during the
substitution process. As a result, inorganic PQDs with limited
efficiency of up to 16% have been predominantly utilized as materials
for solar cells. In this study, the research team employed an alkyl
ammonium iodide-based ligand exchange strategy, effectively substituting
ligands for organic PQDs with excellent solar utilization. This
breakthrough enables the creation of a photoactive layer of QDs for
solar cells with high substitution efficiency and controlled defects.
Consequently, the efficiency of organic PQDs, previously limited to 13%
using existing ligand substitution technology, has been significantly
improved to 18.1%. Moreover, these solar cells demonstrate exceptional
stability, maintaining their performance even after long-term storage
for over two years. The newly-developed organic PQD solar cells exhibit
both high efficiency and stability simultaneously. Previous research on
QD solar cells predominantly employed inorganic PQDs,” remarked Sang-Hak
Lee, the first author of the study. Through this study, we have
demonstrated the potential by addressing the challenges associated with
organic PQDs, which have proven difficult to utilize. This study
presents a new direction for the ligand exchange method in organic PQDs,
serving as a catalyst to revolutionize the field of QD solar cell
material research in the future,” commented Professor Jang. The findings
of this study, co-authored by Dr. Javid Aqoma Khoiruddin and Sang-Hak
Lee, have been published online in Nature Energy on January 27, 2024.
The research was made possible through the support of the ‘Basic
Research Laboratory (BRL)’ and ‘Mid-Career Researcher Program,’ as well
as the ‘Nano·Material Technology Development Program,’ funded by the
National Research Foundation of Korea (NRF) under the Ministry of
Science and ICT (MSIT). It has also received support through the ’Global
Basic Research Lab Project.”</p>
</div>
</div>
</div>
</div>
<p>Now we can perform summarization and print the results:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>summary <span class="op">=</span> summarizer(text, max_length<span class="op">=</span><span class="dv">130</span>, min_length<span class="op">=</span><span class="dv">30</span>, do_sample<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="co"># Print the summary:</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Summary:"</span>, summary[<span class="dv">0</span>][<span class="st">'summary_text'</span>])</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>Output: </span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span></code></pre>
</div>
<div id="sumy-for-summarization" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="sumy-for-summarization" class="callout-inner">
<h3 class="callout-title">Sumy for summarization<a class="anchor" aria-label="anchor" href="#sumy-for-summarization"></a>
</h3>
<div class="callout-content">
<p>Sumy is a Python library for extractive summarization. It uses
algorithms like LSA to rank sentences based on their importance and
creates a summary by selecting the top-ranked sentences. We can see how
it works in practice: We start with importing the PlaintextParser and
LsaSummarizer modules:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="im">from</span> sumy.parsers.plaintext <span class="im">import</span> PlaintextParser</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="im">from</span> sumy.nlp.tokenizers <span class="im">import</span> Tokenizer</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="im">from</span> sumy.summarizers.lsa <span class="im">import</span> LsaSummarizer</span></code></pre>
</div>
<p>To create a parser we use the same text sample from an article from
<a href="https://www.sciencedaily.com/releases/2024/02/240221160400.htm#:~:text=Summary%3A,photoactive%20layer%20of%20solar%20cells." class="external-link">source</a>:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>parser <span class="op">=</span> PlaintextParser.from_string(text, Tokenizer(<span class="st">"english"</span>))</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="co"># Next, we initialize the LSA summarize:</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>summarizer <span class="op">=</span> LsaSummarizer()</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="co"># Summarize the text and print the results</span></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>summary <span class="op">=</span> summarizer(parser.document, <span class="dv">5</span>)</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a><span class="cf">for</span> sentence <span class="kw">in</span> summary:</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>    <span class="bu">print</span>(sentence)</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>Output:</span></code></pre>
</div>
<p>Sumy extracts key sentences from the original text, which can be
quicker but may lack the cohesiveness of an abstractive summary. On the
other hand, Transformer is suitable for generating a new summary that
captures the text’s essence in a coherent and often more readable
form.</p>
</div>
</div>
</div>
<div id="activity-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity-4" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-4"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Which framework could be more useful for text
summarizations in your field of research? Explain why?</p>
</div>
</div>
</div>
<div id="accordionSolution10" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution10" aria-expanded="false" aria-controls="collapseSolution10">
  <h4 class="accordion-header" id="headingSolution10">Show me the solution</h4>
</button>
<div id="collapseSolution10" class="accordion-collapse collapse" data-bs-parent="#accordionSolution10" aria-labelledby="headingSolution10">
<div class="accordion-body">
<p>A: Transformers are particularly useful for summarizing research
papers and documents where understanding the context and generating a
coherent summary is crucial. They can produce summaries that are not
only concise but also maintain the narrative flow, making them more
readable. Sumy, while quicker and less resource-intensive, is best
suited for scenarios where extracting key information without the need
for narrative flow is acceptable.</p>
</div>
</div>
</div>
</div>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-4"></a>
</h3>
<div class="callout-content">
<p>Use the transformers library to perform text summarization on the
following text [generated by Copilot]:</p>
</div>
</div>
</div>
<div id="accordionSpoiler5" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler5" aria-expanded="false" aria-controls="collapseSpoiler5">
  <h3 class="accordion-header" id="headingSpoiler5">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Input Text</h3>
</button>
<div id="collapseSpoiler5" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler5" aria-labelledby="headingSpoiler5">
<div class="accordion-body">
<p>text: “Perovskite nanocrystals are a class of semiconductor
nanocrystals that have attracted a lot of attention in recent years due
to their unique optical and electronic properties. Perovskite
nanocrystals have an ABX3 composition, where A is a monovalent cation
(such as cesium, methylammonium, or formamidinium), B is a divalent
metal (such as lead or tin), and X is a halide (such as chloride,
bromide, or iodide). Perovskite nanocrystals can emit brightly across
the entire visible spectrum, with tunable colors depending on their
composition and size. They also have high quantum yields, fast radiative
decay rates, and narrow emission line widths, making them ideal
candidates for various optoelectronic applications. The first report of
perovskite nanocrystals was published in 2014 by Protesescu et al., who
synthesized cesium lead halide nanocrystals using a hot-injection
method. They demonstrated that the nanocrystals had cubic or
orthorhombic crystal structures, depending on the halide ratio, and that
they exhibited strong photoluminescence with quantum yields up to 90%.
They also showed that the emission wavelength could be tuned from 410 nm
to 700 nm by changing the halide composition or the nanocrystal size.
Since then, many other groups have developed various synthetic methods
and strategies to control the shape, size, composition, and surface
chemistry of perovskite nanocrystals. One of the remarkable features of
perovskite nanocrystals is their defect tolerance, which means that they
can maintain high luminescence even with a high density of surface or
bulk defects. This is in contrast to other semiconductor nanocrystals,
such as CdSe, which require surface passivation to prevent non-radiative
recombination and quenching of the emission. The defect tolerance of
perovskite nanocrystals is attributed to their electronic band
structure, which has a large density of states near the band edges and a
small effective mass of the charge carriers. These factors reduce the
formation energy and the localization of defects and enhance the
radiative recombination rate of the excitons. Another interesting aspect
of perovskite nanocrystals is their weak quantum confinement, which
means that their emission properties are not strongly affected by their
size. This is because the exciton binding energy of perovskite
nanocrystals is much larger than the quantum confinement energy, and
thus the excitons are localized within a few unit cells regardless of
the nanocrystal size. As a result, perovskite nanocrystals can exhibit
narrow emission line widths even with a large size distribution, which
simplifies the synthesis and purification processes. Moreover,
perovskite nanocrystals can show dual emission from both the band edge
and the surface states, which can be exploited for color tuning and
white light generation. Perovskite nanocrystals have been applied to a
wide range of photonic devices, such as light-emitting diodes, lasers,
solar cells, photodetectors, and scintillators. Perovskite nanocrystals
can offer high brightness, color purity, and stability as light
emitters, and can be integrated with various substrates and
architectures. Perovskite nanocrystals can also act as efficient light
absorbers and charge transporters and can be coupled with other
materials to enhance the performance and functionality of the devices.
Perovskite nanocrystals have shown promising results in terms of
efficiency, stability, and versatility in these applications. However,
perovskite nanocrystals also face some challenges and limitations, such
as the toxicity of lead, the instability under ambient conditions, the
hysteresis and degradation under electrical or optical stress, and the
reproducibility and scalability of the synthesis and fabrication
methods. These issues need to be addressed and overcome to realize the
full potential of perovskite nanocrystals in practical devices.
Therefore, further research and development are needed to improve the
material quality, stability, and compatibility of perovskite
nanocrystals, and to explore new compositions, structures, and
functionalities of these fascinating nanomaterials.”</p>
</div>
</div>
</div>
</div>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge<em>(continued)</em><a class="anchor" aria-label="anchor" href="#challenge-4"></a>
</h3>
<div class="callout-content">
<p>Print the summarized text.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>...</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution11" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution11" aria-expanded="false" aria-controls="collapseSolution11">
  <h4 class="accordion-header" id="headingSolution11">Show me the solution</h4>
</button>
<div id="collapseSolution11" class="accordion-collapse collapse" data-bs-parent="#accordionSolution11" aria-labelledby="headingSolution11">
<div class="accordion-body">
<p>A: You can use the following code to load the transformers library
and the pre-trained model and tokenizer for text summarization:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>text <span class="op">=</span> <span class="st">" Perovskite nanocrystals are a class of semiconductor nanocrystals that have attracted…</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a><span class="er">Output:</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul><li>Transformers revolutionized NLP by processing words in parallel
through an attention mechanism, capturing context more effectively than
sequential models</li>
<li>The summation and activation function within a neuron transform
inputs through weighted sums and biases, followed by an activation
function to produce an output.</li>
<li>Transformers consist of encoders, decoders, positional encoding,
input/output embedding, and softmax output, working together to process
and generate data.</li>
<li>Transformers are not limited to NLP and can be applied to other AI
applications due to their ability to handle complex data patterns.</li>
<li>Sentiment analysis and text summarization are practical applications
of transformers in NLP, enabling the analysis of emotional tone and the
creation of concise summaries from large texts.</li>
</ul></div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="04-word-embedding.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="06-llms.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="04-word-embedding.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Word Embedding
        </a>
        <a class="chapter-link float-end" href="06-llms.html" rel="next">
          Next: Large Language... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/05-transformers.md" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/" class="external-link">Source</a></p>
				<p><a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:training@qcif.edu.au">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.4" class="external-link">sandpaper (0.16.4)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.5" class="external-link">pegboard (0.7.5)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.2" class="external-link">varnish (1.0.2)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/05-transformers.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Transformers for Natural Language Processing",
  "creativeWorkStatus": "active",
  "url": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/05-transformers.html",
  "identifier": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/05-transformers.html",
  "dateCreated": "2024-05-10",
  "dateModified": "2024-05-15",
  "datePublished": "2024-05-21"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code --></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

