<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Introduction to Natural Language Processing for Research: Word Embedding</title><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="manifest" href="../site.webmanifest"><link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#ffffff"></head><body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="background-color: #FF4955; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #000">
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
            Pre-Alpha
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../04-word-embedding.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Introduction to Natural Language Processing for Research
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Introduction to Natural Language Processing for Research
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="../aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to Natural Language Processing for Research
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 35%" class="percentage">
    35%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 35%" aria-valuenow="35" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../04-word-embedding.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->
      
            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction to Natural Language Processing</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-text-preprocessing.html">2. Introduction to Text Preprocessing</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-text-analysis.html">3. Text Analysis</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        4. Word Embedding
        </span>
      </button>
    </div><!--/div.accordion-header-->
        
    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#introduction-to-vector-space-embeddings">4.1. Introduction to Vector Space &amp; Embeddings:</a></li>
<li><a href="#bag-of-words-tf-idf">4.2. Bag of Words &amp; TF-IDF:</a></li>
<li><a href="#word2vec-algorithm">4.3. Word2Vec Algorithm:</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-transformers.html">5. Transformers for Natural Language Processing</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-llms.html">6. Large Language Models</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-domain-specific-llms.html">7. Domain-Specific LLMs</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="08-conclusion-final-project.html">8. Wrap-up and Final Project</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width resources"><a href="../instructor/aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">
            
            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/03-text-analysis.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/05-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/03-text-analysis.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Text Analysis
        </a>
        <a class="chapter-link float-end" href="../instructor/05-transformers.html" rel="next">
          Next: Transformers for... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Word Embedding</h1>
        <p>Last updated on 2024-05-10 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/04-word-embedding.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
        
        
        
        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 16 minutes</p>
        
        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What is a vector space in the context of NLP?</li>
<li>How can I visualize vector space in a 2D model?</li>
<li>How can I use embeddings and how do embeddings capture the meaning
of words?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Be able to explain vector space and how it is related to text
analysis.</li>
<li>Identify the tools required for text embeddings.</li>
<li>To explore the Word2Vec algorithm and its advantages over
traditional models.</li>
</ul></div>
</div>
</div>
</div>
</div>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/c96878f9-4c4a-49d7-9ec8-346d75663b76" alt="image" class="figure"><a href="https://carpentries-incubator.github.io/python-text-analysis/06-lsa/index.html" class="external-link">source</a></p>
<section id="introduction-to-vector-space-embeddings"><h2 class="section-heading">4.1. Introduction to Vector Space &amp; Embeddings:<a class="anchor" aria-label="anchor" href="#introduction-to-vector-space-embeddings"></a>
</h2>
<hr class="half-width"><p>We have discussed how tokenization works and how it is important in
text analysis, however, this is not the whole story of preprocessing.
For conducting robust and reliable text analysis with NLP models,
vectorization and embedding are required after tokenization. To
understand this concept, we first talk about vector space.</p>
<p>Vector space models represent text data as vectors, which can be used
in various machine learning algorithms. Embeddings are dense vectors
that capture the semantic meanings of words based on their context.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/e22fea16-a593-4c75-988b-0ba81588f98b" alt="embedding_2" class="figure"><a href="https://www.deeplearning.ai/resources/natural-language-processing/" class="external-link">source</a></p>
<div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Discuss how tokenization affects the representation of text
in vector space models. Consider the impact of ignoring common words
(stop words) and the importance of word order.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p><img src="../fig/embedding_3.png" class="figure"><a href="https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037" class="external-link">source</a></p>
<p>A: Ignoring stop words might lead to loss of some contextual
information but can also reduce noise. Preserving word order can be
crucial for understanding the meaning, especially in languages with
flexible syntax.</p>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Tell me MORE!</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p>Tokenization is a fundamental step in the processing of text for
vector space models. It involves breaking down a string of text into
individual units, or “tokens,” which typically represent words or
phrases. Here’s how tokenization impacts the representation of text in
vector space models:</p>
<ul><li>
<em>Granularity</em>: Tokenization determines the granularity of
text representation. Finer granularity (e.g., splitting on punctuation)
can capture more nuances but may increase the dimensionality of the
vector space.</li>
<li>
<em>Dimensionality</em>: Each unique token becomes a dimension in
the vector space. The choice of tokenization can significantly affect
the number of dimensions, with potential implications for computational
efficiency and the “curse of dimensionality.”</li>
<li>
<em>Semantic Meaning</em>: Proper tokenization ensures that
semantically significant units are captured as tokens, which is crucial
for the model to understand the meaning of the text.</li>
</ul><p>Ignoring common words, or “stop words,” can also have a significant
impact:</p>
<p><em>Noise Reduction</em>: Stop words are often filtered out to reduce
noise since they usually don’t carry important meaning and are highly
frequent (e.g., “the,” “is,” “at”).</p>
<p><em>Focus on Content Words</em>: By removing stop words, the model
can focus on content words that carry the core semantic meaning,
potentially improving the performance of tasks like information
retrieval or topic modeling.</p>
<p><em>Computational Efficiency</em>: Ignoring stop words reduces the
dimensionality of the vector space, which can make computations more
efficient.</p>
<p>The importance of word order is another critical aspect:</p>
<p><em>Contextual Meaning</em>: Word order is essential for capturing
the syntactic structure and meaning of a sentence. Traditional
bag-of-words models ignore word order, which can lead to a loss of
contextual meaning.</p>
<p><em>Phrase Identification</em>: Preserving word order allows for the
identification of multi-word expressions and phrases that have distinct
meanings from their constituent words.</p>
<p><em>Word Embeddings</em>: Advanced models like word embeddings (e.g.,
Word2Vec) and contextual embeddings (e.g., BERT) can capture word order
to some extent, leading to a more nuanced understanding of text
semantics.</p>
<p>In summary, tokenization, the treatment of stop words, and the
consideration of word order are all crucial factors that influence how
text is represented in vector space models, affecting both the quality
of the representation and the performance of downstream tasks.</p>
</div>
</div>
</div>
</div>
<div id="tokenization-vs.-vectorization-vs.-embedding" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="tokenization-vs.-vectorization-vs.-embedding" class="callout-inner">
<h3 class="callout-title">Tokenization Vs. Vectorization Vs.
Embedding<a class="anchor" aria-label="anchor" href="#tokenization-vs.-vectorization-vs.-embedding"></a>
</h3>
<div class="callout-content">
<p>Initially, <strong>tokenization</strong> breaks down text into
discrete elements, or tokens, which can include words, phrases, symbols,
and even punctuation, each represented by a unique numerical identifier.
These tokens are then mapped to <strong>vectors</strong> of real numbers
within an n-dimensional space, a process that is part of
<strong>embedding</strong>. During model training, these vectors are
adjusted to reflect the semantic similarities between tokens,
positioning those with similar meanings closer together in the embedding
space. This allows the model to grasp the nuances of language and
transforms raw text into a format that machine learning algorithms can
interpret, paving the way for advanced text analysis and
understanding.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/5a669f97-1bce-4466-ba6b-2b5e792124f0" alt="image" class="figure"><a href="https://geoffrey-geofe.medium.com/tokenization-vs-embedding-understanding-the-differences-and-their-importance-in-nlp-b62718b5964a" class="external-link">source</a></p>
</div>
</div>
</div>
</section><section id="bag-of-words-tf-idf"><h2 class="section-heading">4.2. Bag of Words &amp; TF-IDF:<a class="anchor" aria-label="anchor" href="#bag-of-words-tf-idf"></a>
</h2>
<hr class="half-width"><p>Feature extraction in machine learning involves creating numerical
features that describe a document’s relationship to its corpus.
Traditional methods like Bag-of-Words and TF-IDF count words or n-grams,
with the latter assigning weights based on a word’s importance,
calculated by Term Frequency (TF) and Inverse Document Frequency (IDF).
TF measures a word’s importance within a document, while IDF assesses
its rarity across the corpus.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/2b3b5f37-667b-4fe8-bc31-e6798b6e2b61" alt="image" class="figure"><a href="https://www.deeplearning.ai/resources/natural-language-processing/" class="external-link">source</a></p>
<p>The product of TF and IDF gives the TF-IDF score, which balances a
word’s frequency in a document against its commonness in the corpus.
This approach helps to highlight significant words while diminishing the
impact of commonly used words like “the” or “a.”</p>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div>Instructor Note</h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" aria-labelledby="headingInstructor1" data-bs-parent="#accordionInstructor1">
<div class="accordion-body">
<ul><li>
<strong>BoW</strong> “encodes the total number of times a document
uses each word in the associated corpus through the
CounterVectorizer.”</li>
<li>
<strong>TF-IDF</strong> “creates features for each document based on
how often each word shows up in a document versus the entire
corpus.</li>
<li><a href="https://www.deeplearning.ai/resources/natural-language-processing/" class="external-link">source</a></li>
</ul></div>
</div>
</div>
</div>
<div id="discussion-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Discuss how each method represents the importance of words
and the potential impact on sentiment analysis.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>A: To compare the Bag of Words (BoW) and Term Frequency-Inverse
Document Frequency (TF-IDF) methods in representing text data and their
implications for sentiment analysis.</p>
<p>Data Collection: Gather a corpus of product reviews. For this
activity, let’s assume we have a list of reviews stored in a variable
called reviews. Clean the text data by removing punctuation, converting
to lowercase, and possibly removing stop words. Use a vectorizer to
convert the reviews into a BoW representation.</p>
<p>Discuss how BoW represents the frequency of words without considering
the context or rarity across documents. Use a vectorizer to convert the
same reviews into a TF-IDF representation. Discuss how TF-IDF represents
the importance of words by considering both the term frequency and how
unique the word is across all documents.</p>
</div>
</div>
</div>
</div>
<div id="teamwork" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="teamwork" class="callout-inner">
<h3 class="callout-title">Teamwork<a class="anchor" aria-label="anchor" href="#teamwork"></a>
</h3>
<div class="callout-content">
<p>Sentiment Analysis Implications:</p>
<p>Analyze a corpus of product reviews using both BoW and TF-IDF.
Consider how the lack of context in BoW might affect sentiment analysis.
Evaluate whether TF-IDF’s emphasis on unique words improves the model’s
ability to understand sentiment.</p>
<p>Share Findings: Groups should present their findings, highlighting
the strengths and weaknesses of each method.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer, TfidfVectorizer</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># Sample corpus of product reviews</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>reviews <span class="op">=</span> [</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="st">"Great product, really loved it!"</span>,</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="st">"Bad quality, totally disappointed."</span>,</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="st">"Decent product for the price."</span>,</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="st">"Excellent quality, will buy again!"</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>]</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># Initialize the CountVectorizer for BoW</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>bow_vectorizer <span class="op">=</span> CountVectorizer(stop_words<span class="op">=</span><span class="st">'english'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co"># Fit and transform the reviews</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>bow_matrix <span class="op">=</span> bow_vectorizer.fit_transform(reviews)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="co"># Display the BoW matrix</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bag of Words Matrix:"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="bu">print</span>(bow_matrix.toarray())</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="co"># Initialize the TfidfVectorizer for TF-IDF</span></span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>tfidf_vectorizer <span class="op">=</span> TfidfVectorizer(stop_words<span class="op">=</span><span class="st">'english'</span>)</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a><span class="co"># Fit and transform the reviews</span></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>tfidf_matrix <span class="op">=</span> tfidf_vectorizer.fit_transform(reviews)</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="co"># Display the TF-IDF matrix</span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">TF-IDF Matrix:"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a><span class="bu">print</span>(tfidf_matrix.toarray())</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>The BoW matrix shows the frequency of each word in the reviews,
disregarding context and word importance. The TF-IDF matrix shows the
weighted importance of words, giving less weight to common words and
more to unique ones.</p>
<p>In sentiment analysis, BoW might misinterpret sentiments due to
ignoring context, while TF-IDF might capture nuances better by
emphasizing words that are significant in a particular review.</p>
<p>By comparing BoW and TF-IDF, participants can gain insights into how
each method processes text data and their potential impact on NLP tasks
like sentiment analysis. This activity encourages critical thinking
about feature representation in machine learning models.</p>
</section><section id="word2vec-algorithm"><h2 class="section-heading">4.3. Word2Vec Algorithm:<a class="anchor" aria-label="anchor" href="#word2vec-algorithm"></a>
</h2>
<hr class="half-width"><p>More advanced techniques like Word2Vec and GLoVE, as well as feature
learning during neural network training, have also been developed to
improve feature extraction.</p>
<p>Word2Vec uses neural networks to learn word associations from large
text corpora. It has two architectures: Skip-Gram and Continuous
Bag-of-Words (CBOW).</p>
<p>After training, it discards the final layer and outputs word
embeddings that capture context. These embeddings capture the context of
words, making similar contexts yield similar embeddings. Post-data
preprocessing, these numerical features can be used in various NLP
models for tasks like classification or named entity recognition.</p>
<p>Now let’s see how this framework can be used in practice. First
import required libraries: Start by importing necessary libraries like
gensim for Word2Vec and nltk for tokenization. Next, prepare the data:
Tokenize your text data into words.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># Sample text</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Tokenization splits text into words. Embeddings capture semantic meaning."</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># Tokenize the text</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>tokens <span class="op">=</span> word_tokenize(text.lower())</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a></span></code></pre>
</div>
<p>Now train the model: Use the Word2Vec class from gensim to train your
model on the tokenized sentences.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># Sample text</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Tokenization splits text into words. Embeddings capture semantic meaning."</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co"># Tokenize the text</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>tokens <span class="op">=</span> word_tokenize(text.lower())</span></code></pre>
</div>
<p>Retrieve Vectors: After training, use the model to get vectors for
words of interest.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="co"># Display the vector</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="bu">print</span>(vector_embeddings)</span></code></pre>
</div>
<p>The code tokenizes the sample text, trains a Word2Vec model, and
retrieves the vector for the word ‘embeddings’.</p>
<p>The resulting vector is a 50-dimensional representation of
‘embeddings’, capturing its context within the sample text. This vector
can then be used in various NLP tasks to represent the semantic meaning
of the word ‘embeddings’.</p>
<p>By understanding the roles of tokenization and embedding, we can
better prepare text data for complex NLP tasks and build models that
more accurately interpret human language.</p>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What is GLoVE?</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" aria-labelledby="headingSpoiler2" data-bs-parent="#accordionSpoiler2">
<div class="accordion-body">
<p>Global Vectors for Word Representation (GLoVE)</p>
<p>GLoVE is a model for learning word embeddings, which are
representations of words in the form of high-dimensional vectors. Unlike
Word2Vec, which uses a neural network to learn word embeddings from
local context information, GLoVE is designed to capture both global
statistics and local context. Here’s how GLoVE stands out:</p>
<ul><li>Matrix Factorization: GLoVE uses matrix factorization on a word
co-occurrence matrix that reflects how often each word appears in the
context of every other word within a large corpus.</li>
<li>Global Word-Word Co-Occurrence: It focuses on word-to-word
co-occurrence globally across the entire corpus, rather than just within
a local context window as in Word2Vec.</li>
<li>Weighting Function: GLoVE employs a weighting function that helps to
address the disparity in word co-occurrence frequencies, giving less
weight to rare and frequent co-occurrences.</li>
</ul><p>The main difference between GLoVE and Word2Vec is that GLoVE is built
on the idea that word meanings can be derived from their co-occurrence
probabilities with other words, and hence it incorporates global corpus
statistics, whereas Word2Vec relies more on local context information.
This allows GLoVE to effectively capture both the semantic and syntactic
relationships between words, making it powerful for various natural
language processing tasks.</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul><li>Tokenization is crucial for converting text into a format usable by
machine learning models.</li>
<li>BoW and TF-IDF are fundamental techniques for feature extraction in
NLP.</li>
<li>Word2Vec and GloVE generate embeddings that encapsulate word
meanings based on context and co-occurrence, respectively.</li>
<li>Understanding these concepts is essential for building effective NLP
models that can interpret and process human language.</li>
</ul></div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/03-text-analysis.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/05-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/03-text-analysis.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Text Analysis
        </a>
        <a class="chapter-link float-end" href="../instructor/05-transformers.html" rel="next">
          Next: Transformers for... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/04-word-embedding.md" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/" class="external-link">Source</a></p>
				<p><a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:training@qcif.edu.au">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.4" class="external-link">sandpaper (0.16.4)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.5" class="external-link">pegboard (0.7.5)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.2" class="external-link">varnish (1.0.2)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/instructor/04-word-embedding.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Word Embedding",
  "creativeWorkStatus": "active",
  "url": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/instructor/04-word-embedding.html",
  "identifier": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/instructor/04-word-embedding.html",
  "dateCreated": "2024-05-10",
  "dateModified": "2024-05-10",
  "datePublished": "2024-06-04"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code --></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

