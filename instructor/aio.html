<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Introduction to Natural Language Processing for Research: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="manifest" href="../site.webmanifest">
<link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="background-color: #FF4955; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #000">
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
            Pre-Alpha
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Introduction to Natural Language Processing for Research
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Introduction to Natural Language Processing for Research
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="../aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to Natural Language Processing for Research
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->
      
            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction to Natural Language Processing</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-text-preprocessing.html">2. Introduction to Text Preprocessing</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-text-analysis.html">3. Text Analysis</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-word-embedding.html">4. Word Embedding</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-transformers.html">5. Transformers for Natural Language Processing</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-llms.html">6. Large Language Models</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-domain-specific-llms.html">7. Domain-Specific LLMs</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="08-conclusion-final-project.html">8. Wrap-up and Final Project</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="../instructor/aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">
            
            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-introduction"><p>Content from <a href="01-introduction.html">Introduction to Natural Language Processing</a></p>
<hr>
<p>Last updated on 2024-05-12 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/01-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 10 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are some common research applications of NLP?</li>
<li>What are the basic concepts and terminology of NLP?</li>
<li>How can I use NLP in my research field?</li>
<li>How can I acquire data for NLP tasks?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Define natural language processing and its goals.</li>
<li>Identify the main research applications and challenges of NLP.</li>
<li>Explain the basic concepts and terminology of NLP, such as tokens,
lemmas, and n-grams.</li>
<li>Use some popular datasets and libraries to acquire data for NLP
tasks.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="introduction-to-nlp-workshop"><h2 class="section-heading">1.1. Introduction to NLP Workshop<a class="anchor" aria-label="anchor" href="#introduction-to-nlp-workshop"></a>
</h2>
<hr class="half-width">
<p>Natural Language Processing (NLP) is becoming a popular and robust
tool for a wide range of research projects. In this episode, we embark
on a journey to explore the transformative power of NLP tools in the
realm of research.</p>
<p>It is tailored for researchers who are keen on harnessing the
capabilities of NLP to enhance and expedite their work. Whether you are
delving into text classification, extracting pivotal information,
discerning sentiments, summarizing extensive documents, translating
across languages, or developing sophisticated question-answering
systems, this session will lay the foundational knowledge you need to
leverage NLP effectively.</p>
<p>We will begin by delving into the <strong>Common Applications of NLP
in Research</strong>, showcasing how these tools are not just
theoretical concepts but practical instruments that drive forward
today’s innovative research projects. From analyzing public sentiment to
extracting critical data from a plethora of documents, NLP stands as a
pillar in the modern researcher’s toolkit.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/7a1d0e09-bbad-4b16-8de2-b312efe0db11" alt="NLP for Research" class="figure"> Prompt: “NLP for Research” [<em>DALL-E
3</em>]</p>
<p>Next, we’ll demystify the <strong>Basic Concepts and Terminology of
NLP</strong>. Understanding these fundamental terms is crucial, as they
form the building blocks of any NLP application. We’ll cover everything
from the basics of a corpus to the intricacies of transformers, ensuring
you have a solid grasp of the language used in NLP.</p>
<p>Finally, we’ll guide you through Data Acquisition: Dataset Libraries,
where you’ll learn about the treasure troves of data available at your
fingertips. We’ll compare different libraries and demonstrate how to
access and utilize these resources through hands-on examples.</p>
<p>By the end of this episode, you will not only understand the
significance of NLP in research but also be equipped with the knowledge
to start applying these tools to your own projects. Prepare to unlock
new potentials and streamline your research process with the power of
NLP!</p>
<div id="discussion" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: What are some examples of NLP in your everyday life? Think
of some situations where you interact with or use NLPs, such as online
search, voice assistants, social media, etc. How do these examples
demonstrate the usefulness of NLP in research projects?</p>
</div>
</div>
</div>
<div id="discussion-1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: What are some examples of NLP in your daily research tasks?
What are challenges of NLP that make it difficult, complex, and/or
inaccurate?</p>
</div>
</div>
</div>
</section><section id="common-applications-of-nlp-in-research"><h2 class="section-heading">1.2. Common Applications of NLP in Research<a class="anchor" aria-label="anchor" href="#common-applications-of-nlp-in-research"></a>
</h2>
<hr class="half-width">
<p><strong>Sentiment Analysis</strong> is a powerful tool for
researchers, especially in fields like market research, political
science, and public health. It involves the computational identification
of opinions expressed in text, categorizing them as positive, negative,
or neutral.</p>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p>In market research, for instance, sentiment analysis can be applied
to product reviews to gauge consumer satisfaction: a study could analyze
thousands of online reviews for a new smartphone model to determine the
overall public sentiment. This can help companies identify areas of
improvement or features that are well-received by consumers.</p>
</div>
</div>
</div>
</div>
<p><strong>Information Extraction</strong> is crucial for quickly
gathering specific information from large datasets. It is used
extensively in legal research, medical research, and scientific studies
to extract entities and relationships from texts.</p>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" aria-labelledby="headingSpoiler2" data-bs-parent="#accordionSpoiler2">
<div class="accordion-body">
<p>In legal research, for example, information extraction can be used to
sift through case law to find precedents related to a particular legal
issue. A researcher could use NLP to extract instances of “negligence”
from thousands of case files, aiding in the preparation of legal
arguments.</p>
</div>
</div>
</div>
</div>
<p><strong>Text Summarization</strong> helps researchers by providing
concise summaries of lengthy documents, such as research papers or
reports, allowing them to quickly understand the main points without
reading the entire text.</p>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" aria-labelledby="headingSpoiler3" data-bs-parent="#accordionSpoiler3">
<div class="accordion-body">
<p>In biomedical research, text summarization can assist in literature
reviews by providing summaries of research articles. For example, a
researcher could use an NLP model to summarize articles on gene therapy,
enabling them to quickly assimilate key findings from a vast array of
publications.</p>
</div>
</div>
</div>
</div>
<p><strong>Topic Modeling</strong> is used to uncover latent topics
within large volumes of text, which is particularly useful in fields
like sociology and history to identify trends and patterns in historical
documents or social media data.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/3d6bc7c3-dff0-430f-a44b-544d0944e59c" alt="image" class="figure"><a href="https://dl.acm.org/doi/10.1145/2133806.2133826" class="external-link">source</a></p>
<div id="accordionSpoiler4" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler4" aria-expanded="false" aria-controls="collapseSpoiler4">
  <h3 class="accordion-header" id="headingSpoiler4">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler4" class="accordion-collapse collapse" aria-labelledby="headingSpoiler4" data-bs-parent="#accordionSpoiler4">
<div class="accordion-body">
<p>For example, in historical research, topic modeling can reveal
prevalent themes in primary source documents from a particular era. A
historian might use NLP to analyze newspapers from the early 20th
century to study public discourse around significant events like World
War I.</p>
</div>
</div>
</div>
</div>
<p><strong>Named Entity Recognition</strong> is a process where an
algorithm takes a string of text (sentence or paragraph) and identifies
relevant nouns (people, places, and organizations) that are mentioned in
that string.</p>
<div id="accordionSpoiler5" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler5" aria-expanded="false" aria-controls="collapseSpoiler5">
  <h3 class="accordion-header" id="headingSpoiler5">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler5" class="accordion-collapse collapse" aria-labelledby="headingSpoiler5" data-bs-parent="#accordionSpoiler5">
<div class="accordion-body">
<p>NER is used in many fields in NLP, and it can help answer many
real-world questions, such as: Which companies were mentioned in the
news article? Were specified products mentioned in complaints or
reviews? Does the tweet (recently rebranded to X) contain the name of a
person? Does the tweet contain this person’s location?</p>
</div>
</div>
</div>
</div>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/27b2f4a2-f0a8-4061-8f52-223f2a0f8e31" alt="image" class="figure"><a href="https://www.turing.com/kb/a-comprehensive-guide-to-named-entity-recognition" class="external-link">source</a></p>
<div id="challenges-of-nlp" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="challenges-of-nlp" class="callout-inner">
<h3 class="callout-title">Challenges of NLP<a class="anchor" aria-label="anchor" href="#challenges-of-nlp"></a>
</h3>
<div class="callout-content">
<p>One of the significant challenges in NLP is dealing with the
ambiguity of language. Words or phrases can have multiple meanings, and
determining the correct one based on context can be difficult for NLP
systems. In a research paper discussing “bank erosion,” an NLP system
might confuse “bank” with a financial institution rather than the
geographical feature, leading to incorrect analysis.</p>
<p><strong>This challenge leads to the fact that the classical NLP
systems often struggle with contextual understanding which is crucial in
text analysis tasks.</strong> This can lead to misinterpretation of the
meaning and sentiment of the text. If a research paper mentions “novel
results,” an NLP system might interpret “novel” as a literary work
instead of “new” or “original,” which could mislead the analysis of the
paper’s contributions.</p>
</div>
</div>
</div>
<div id="accordionSpoiler6" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler6" aria-expanded="false" aria-controls="collapseSpoiler6">
  <h3 class="accordion-header" id="headingSpoiler6">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Suggested Resources:</h3>
</button>
<div id="collapseSpoiler6" class="accordion-collapse collapse" aria-labelledby="headingSpoiler6" data-bs-parent="#accordionSpoiler6">
<div class="accordion-body">
<ul>
<li>Python’s Natural Language Toolkit (NLTK) for sentiment analysis</li>
<li>TextBlob, a library for processing textual data</li>
<li>Stanford NER for named entity recognition</li>
<li>spaCy, an open-source software library for advanced NLP</li>
<li>Sumy, a Python library for automatic summarization of text
documents</li>
<li>BERT-based models for extractive and abstractive summarization</li>
<li>Gensim for topic modeling and document similarity analysis</li>
<li>MALLET, a Java-based package for statistical natural language
processing</li>
</ul>
</div>
</div>
</div>
</div>
</section><section id="basic-concepts-and-terminology-of-nlp"><h2 class="section-heading">1.3. Basic Concepts and Terminology of NLP<a class="anchor" aria-label="anchor" href="#basic-concepts-and-terminology-of-nlp"></a>
</h2>
<hr class="half-width">
<div id="discussion-2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-2" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-2"></a>
</h3>
<div class="callout-content">
<p>Teamwork: What are some of the basic concepts and terminology of
natural language processing that you are familiar with or want to learn
more about? Share your knowledge or questions with a partner or a small
group, and try to explain or understand some of the key terms of natural
language processing, such as tokens, lemmas, n-grams, etc.</p>
</div>
</div>
</div>
<p><strong>Corpus</strong>: A corpus is a collection of written texts,
especially the entire works of a particular author or a body of writing
on a particular subject. In NLP, a corpus is used as a large and
structured set of texts that can be used to perform statistical analysis
and hypothesis testing, check occurrences, or validate linguistic rules
within a specific language territory.</p>
<p><strong>Token and Tokenization</strong>: Tokenization is the process
of breaking a stream of text up into words, phrases, symbols, or other
meaningful elements called tokens. The list of tokens becomes input for
further processing such as parsing or text mining. Tokenization is
useful in situations where certain characters or words need to be
treated as a single entity, despite any spaces or punctuation that might
separate them.</p>
<p><strong>Stemming</strong>: Stemming is the process of reducing
inflected (or sometimes derived) words to their word stem, base, or root
form—generally a written word form. The idea is to remove affixes to get
to the root form of the word. Stemming is often used in search engines
for indexing words. Instead of storing all forms of a word, a search
engine can store only the stems, greatly reducing the size of the index
while increasing retrieval accuracy.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/42718630-80a0-483f-81a7-61be10f5aa99" alt="image" class="figure"><a href="https://nirajbhoi.medium.com/stemming-vs-lemmatization-in-nlp-efc280d4e845" class="external-link">source</a></p>
<p><strong>Lemmatization</strong>: Lemmatization, unlike Stemming,
reduces the inflected words properly ensuring that the root word belongs
to the language. In Lemmatization, the root word is called Lemma. A
lemma is the canonical form, dictionary form, or citation form of a set
of words. For example, runs, running, and running are all forms of the
word run, therefore run is the lemma of all these words.</p>
<p><strong>Part-of-Speech (PoS) Tagging</strong>: Part-of-speech tagging
is the process of marking up a word in a text as corresponding to a
particular part of speech, based on both its definition and its context.
It is a necessary step before performing more complex NLP tasks like
parsing or grammar checking.</p>
<p><strong>Chunking</strong>: Chunking is a process of extracting
phrases from unstructured text. Instead of just simple tokens that may
not represent the actual meaning of the text, it’s also interested in
extracting entities like noun phrases, verb phrases, etc. It’s basically
a meaningful grouping of words or tokens.</p>
<p><strong>Word Embeddings</strong>: Word embeddings are a type of word
representation that allows words with similar meanings to have a similar
representation. They are a distributed representation of text that is
perhaps one of the key breakthroughs for the impressive performance of
deep learning methods on challenging natural language processing
problems.</p>
<p><strong>Transformers</strong>: Transformers are models that handle
the ordering of words and other elements in a language. They are
designed to handle sequential data, such as natural language, for tasks
such as translation and text summarization. They are the foundation of
most recent advances in NLP, including models like BERT and GPT.</p>
</section><section id="data-acquisition-dataset-libraries"><h2 class="section-heading">1.4. Data Acquisition: Dataset Libraries:<a class="anchor" aria-label="anchor" href="#data-acquisition-dataset-libraries"></a>
</h2>
<hr class="half-width">
<p>Different data libraries offer various datasets that are useful for
training and testing NLP models. These libraries provide access to a
wide range of text data, from literary works to social media posts,
which can be used for tasks such as sentiment analysis, topic modeling,
and more.</p>
<p><strong>Natural Language Toolkit (NLTK)</strong>: NLTK is a leading
platform for building Python programs to work with human language data.
It provides easy-to-use interfaces to over 50 corpora and lexical
resources such as WordNet, along with a suite of text-processing
libraries for classification, tokenization, stemming, tagging, parsing,
and semantic reasoning.</p>
<p><strong>spaCy</strong>: spaCy is a free, open-source library for
advanced Natural Language Processing in Python. It’s designed
specifically for production use and helps you build applications that
process and “understand” large volumes of text. It can be used to build
information extraction or natural language understanding systems or to
pre-process text for deep learning.</p>
<p><strong>Gensim</strong>: Gensim is a Python library for topic
modeling, document indexing, and similarity retrieval with large
corpora. Targeted at the NLP and information retrieval communities,
Gensim is designed to handle large text collections using data streaming
and incremental online algorithms, which differentiates it from most
other machine learning software packages that only target batch and
in-memory processing.</p>
<p><strong>Hugging Face’s datasets</strong>: This library provides a
simple command-line interface to download and pre-process any of the
major public datasets (text datasets in 467 languages and dialects,
etc.) provided on the HuggingFace Datasets Hub. It’s designed to let the
community easily add and share new datasets. Hugging Face Datasets
simplifies working with data for machine learning, especially NLP tasks.
It provides a central hub to find and load pre-processed datasets,
saving you time on data preparation. You can explore a vast collection
of datasets for various tasks and easily integrate them with other
Hugging Face libraries.</p>
<p>Data acquisition using Hugging Face datasets library. First, we start
with installing the library:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>pip install datasets</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># To import the dataset, we can write:</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span></code></pre>
</div>
<p>Use load_dataset with the dataset identifier in quotes. For example,
to load the SQuAD question answering dataset:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>squad_dataset <span class="op">=</span> load_dataset(<span class="st">"squad"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="co"># Use the info attribute to view the trainset information:</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="bu">print</span>(squad_dataset[<span class="st">"train"</span>].info)</span></code></pre>
</div>
<p>Each data point is a dictionary with keys corresponding to data
elements (e.g., question, context). Access them using those keys within
square brackets:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>question <span class="op">=</span> squad_dataset[<span class="st">"train"</span>][<span class="dv">0</span>][<span class="st">"question"</span>]</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>context <span class="op">=</span> squad_dataset[<span class="st">"train"</span>][<span class="dv">0</span>][<span class="st">"context"</span>]</span></code></pre>
</div>
<p>We can use the <em>print()</em> function to see the output:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Context: </span><span class="sc">{</span>context<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div id="challenge" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge:<a class="anchor" aria-label="anchor" href="#challenge"></a>
</h3>
<div class="callout-content">
<p>Q: Use the nltk library to acquire data for natural language
processing tasks. You can use the following code to load the nltk
library and download some popular datasets:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>nltk.download()</span></code></pre>
</div>
<p>Choose one of the datasets from the nltk downloader, such as brown,
reuters, or gutenberg, and load it using the nltk.corpus module. Then,
print the name, size, and description of the dataset.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>A: You can use the following code to access the dataset information:
Use the nltk library to acquire data for NLP tasks. Import the necessary
libraries:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> gutenberg, brown</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="co"># Download the required data:</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>nltk.download(<span class="st">'gutenberg'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>nltk.download(<span class="st">'brown'</span>)</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a><span class="bu">print</span>(gutenberg.readme())</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a><span class="co"># Access the downloaded data:</span></span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>gutenberg_text <span class="op">=</span> gutenberg.raw(<span class="st">'austen-emma.txt'</span>)</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>brown_text <span class="op">=</span> brown.words()</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Image datasets can be found online or created uniquely for your
research question.</li>
<li>Images consist of pixels arranged in a particular order.</li>
<li>Image data is usually preprocessed before use in a CNN for
efficiency, consistency, and robustness.</li>
<li>Input data generally consists of three sets: a training set used to
fit model parameters; a validation set used to evaluate the model fit on
training data; and a test set used to evaluate the final model
performance.</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-02-text-preprocessing"><p>Content from <a href="02-text-preprocessing.html">Introduction to Text Preprocessing</a></p>
<hr>
<p>Last updated on 2024-05-12 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/02-text-preprocessing.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I prepare data for NLP text analysis?</li>
<li>How can I use spaCy for text preprocessing?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Define text preprocessing and its purpose for NLP tasks.</li>
<li>Perform sentence segmentation, tokenization lemmatization, and
stop-words removal, using spaCy.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Text preprocessing is the method of cleaning and preparing text data
for use in NLP. This step is vital because it transforms raw data into a
format that can be analyzed and used effectively by NLP algorithms.</p>
<section id="sentence-segmentation"><h2 class="section-heading">2.1. Sentence Segmentation<a class="anchor" aria-label="anchor" href="#sentence-segmentation"></a>
</h2>
<hr class="half-width">
<p>Sentence segmentation divides a text into its constituent sentences,
which is essential for understanding the structure and flow of the
content. We start with a field-specific text example and see how it
works. We can start with a paragraph about perovskite nanocrystals from
the context of material engineering. Divide it into sentences.</p>
<p>We can use the open-source library, spaCy, to perform this task.
First, we import the spaCy library:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> spacy</span></code></pre>
</div>
<p>Then we need to Load the English language model:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span></code></pre>
</div>
<p>We can store our text here:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>perovskite_text <span class="op">=</span> <span class="st">"Perovskite nanocrystals are a class of semiconductor nanocrystals with unique properties that distinguish them from traditional quantum dots. These nanocrystals have an ABX3 composition, where 'A' can be cesium, methylammonium (MA), or formamidinium (FA); 'B' is typically lead or tin; and 'X' is a halogen ion like chloride, bromide, or iodide. Their remarkable optoelectronic properties, such as high photoluminescence quantum yields and tunable emission across the visible spectrum, make them ideal for applications in light-emitting diodes, lasers, and solar cells."</span></span></code></pre>
</div>
<p>Now we process the text with spaCy:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>doc <span class="op">=</span> nlp(perovskite_text)</span></code></pre>
</div>
<p>To extract sentences from the processed text we use the
<em>list()</em> function:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>sentences <span class="op">=</span> <span class="bu">list</span>(doc.sents)</span></code></pre>
</div>
<p>We use <em>for loop</em> and <em>print()</em> function to output each
sentence to show the segmentation:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="cf">for</span> sentence <span class="kw">in</span> sentences:</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>   <span class="bu">print</span>(sentence.text)</span></code></pre>
</div>
<pre><code>Output: Perovskite nanocrystals are a class of semiconductor nanocrystals with unique properties that distinguish them from traditional quantum dots.
These nanocrystals have an ABX3 composition, where 'A' can be cesium, methylammonium (MA), or formamidinium (FA); 'B' is typically lead or tin; and 'X' is a halogen ion like chloride, bromide, or iodide.
Their remarkable optoelectronic properties, such as high photoluminescence quantum yields and tunable emission across the visible spectrum, make them ideal for applications in light-emitting diodes, lasers, and solar cells.</code></pre>
<div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Q: Let’s try again by completing the code below to segment sentences
from a paragraph about “your field of research”:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>nlp <span class="op">=</span> _____.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co"># Add the paragraph about your field of research here</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"___"</span> </span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>doc <span class="op">=</span> nlp(___)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="co"># Fill in the blank to extract sentences:</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>sentences <span class="op">=</span> <span class="bu">list</span>(______) </span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a><span class="co"># Fill in the blank to print each sentence</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a><span class="cf">for</span> sentence <span class="kw">in</span> sentences:</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>  <span class="bu">print</span>(______)  </span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="co"># Add the paragraph about your field of research here</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"***"</span> <span class="co"># varies based on your field of research</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a><span class="co"># Fill in the blank to extract sentences:</span></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>sentences <span class="op">=</span> <span class="bu">list</span>(doc.sents) </span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a><span class="co"># Fill in the blank to print each sentence</span></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a><span class="cf">for</span> sentence <span class="kw">in</span> sentences:</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>  <span class="bu">print</span>(sentence.text)  </span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="discussion-1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Why is text preprocessing necessary for NLP tasks? Think of
some examples of NLP tasks that require text preprocessing, such as
sentiment analysis, machine translation, or text summarization. How does
text preprocessing improve the performance and accuracy of these
tasks?</p>
</div>
</div>
</div>
<div id="challenge" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge"></a>
</h3>
<div class="callout-content">
<p>Q: Use the spaCy library to perform sentence segmentation and
tokenization on the following text:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>text: <span class="st">"The research (Ref. [1]) focuses on developing perovskite nanocrystals with a bandgap of 1.5 eV, suitable for solar cell applications!"</span>. </span></code></pre>
</div>
<p>Print the number of sentences and tokens in the text, and the list of
sentences and tokens. You can use the following code to load the
<em>spaCy</em> library and the English language model:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="co"># Load the English language model:</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="co"># Define the text with marks, letters, and numbers:</span></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"The research (Ref. [1]) focuses on developing perovskite nanocrystals with a bandgap of 1.5 eV, suitable for solar cell applications.!"</span></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a><span class="co"># Process the text with spaCy</span></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a><span class="co"># Print the original text:</span></span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original text:"</span>, text)</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a><span class="co"># Sentence segmentation:</span></span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>sentences <span class="op">=</span> <span class="bu">list</span>(doc.sents)</span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a><span class="co"># Print the sentences:</span></span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sentences:"</span>)</span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a><span class="cf">for</span> sentence <span class="kw">in</span> sentences:</span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a>    <span class="bu">print</span>(sentence.text)</span>
<span id="cb12-22"><a href="#cb12-22" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" tabindex="-1"></a><span class="co"># Tokenization:</span></span>
<span id="cb12-24"><a href="#cb12-24" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb12-25"><a href="#cb12-25" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" tabindex="-1"></a><span class="co"># Print the tokens:</span></span>
<span id="cb12-27"><a href="#cb12-27" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokens:"</span>)</span>
<span id="cb12-28"><a href="#cb12-28" tabindex="-1"></a><span class="bu">print</span>(tokens)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</section><section id="tokeniziation"><h2 class="section-heading">2.2. Tokeniziation<a class="anchor" aria-label="anchor" href="#tokeniziation"></a>
</h2>
<hr class="half-width">
<p>As already mentioned, in the first episode, Tokenization breaks down
text into individual words or tokens, which is a fundamental step for
many NLP tasks.</p>
<div id="discussion-2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-2" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-2"></a>
</h3>
<div class="callout-content">
<p>Teamwork: To better understand how it works let’s Match tokens from
the provided paragraph about perovskite nanocrystals with similar tokens
from another scientific text. This helps in understanding the common
vocabulary used in the scientific literature. Using the sentences we
listed in the previous section, we can see how Tokenization performs.
Assuming ‘sentences’ is a list of sentences from the previous example,
choose a sentence to tokenize:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>sentence_to_tokenize <span class="op">=</span> sentences[<span class="dv">0</span>]</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="co"># Tokenize the chosen sentence by using a list comprehension:</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>tokens <span class="op">=</span> [token.perovskite_text <span class="cf">for</span> token <span class="kw">in</span> sentence_to_tokenize]</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="co"># We can print the tokens:</span></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a><span class="bu">print</span>(tokens)</span></code></pre>
</div>
<p>Output: [‘Perovskite’, ‘nanocrystals’, ‘are’, ‘a’, ‘class’, ‘of’,
‘semiconductor’, ‘nanocrystals’, ‘with’, ‘unique’, ‘properties’, ‘that’,
‘distinguish’, ‘them’, ‘from’, ‘traditional’, ‘quantum’, ‘dots’,
‘.’]</p>
<p>Tokenization is not just about splitting text into words; it’s about
understanding the boundaries of words and symbols in different contexts,
which can vary greatly between languages and even within the same
language in different settings.</p>
</div>
</div>
</div>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>Tokenization is very important for text analysis tasks such as
sentiment analysis. Here we can compare two different texts from
different fields and see how their associated tokens are different:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>perovskite_tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> nlp(perovskite_text)]</span></code></pre>
</div>
<p>Now, we can add a new text from the trading context for comparison.
Tokenization of a trading text can be performed similarly to the
previous text.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>trading_text <span class="op">=</span> <span class="st">"Trading strategies often involve analyzing patterns and executing trades based on predicted market movements. Successful traders analyze trends and volatility to make informed decisions."</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>trading_tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> nlp(trading_text)]</span></code></pre>
</div>
<p>We can see the results by using the <em>print()</em> function. The
tokens from both texts:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Perovskite Tokens:"</span>, perovskite_tokens)</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trading Tokens:"</span>, trading_tokens)</span></code></pre>
</div>
<pre><code>Output: 
Perovskite Tokens: ['Perovskite', 'nanocrystals', 'are', 'a', 'class', 'of', 'semiconductor', 'nanocrystals', 'with', 'unique', 'properties', 'that', 'distinguish', 'them', 'from', 'traditional', 'quantum', 'dots', '.']
Trading Tokens: ['Trading', 'strategies', 'often', 'involve', 'analyzing', 'patterns', 'and', 'executing', 'trades', 'based', 'on', 'predicted', 'market', 'movements', '.', 'Successful', 'traders', 'analyze', 'trends', 'and', 'volatility', 'to', 'make', 'informed', 'decisions', '.']</code></pre>
<p>The tokens from the perovskite text will be specific to materials
science, while the trading tokens will include terms related to market
analysis. The scientific texts may use more complex and compound words
while trading texts might include more action-oriented and analytical
language. This comparison helps in understanding the specialized
language used in different fields.</p>
</div>
</div>
</div>
</section><section id="stemming-and-lemmatization"><h2 class="section-heading">2.3. Stemming and Lemmatization<a class="anchor" aria-label="anchor" href="#stemming-and-lemmatization"></a>
</h2>
<hr class="half-width">
<p>Stemming and lemmatization are techniques used to reduce words to
their base or root form, aiding in the normalization of text. As
discussed in the previous episode, these two methods are different.
Decide whether stemming or lemmatization would be more appropriate for
analyzing a set of research texts on perovskite nanocrystals.</p>
<div id="discussion-3" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-3" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-3"></a>
</h3>
<div class="callout-content">
<p>Teamwork: From the differences between lemmatization and stemming
that we learned in the last episode, which technique will you select to
get more accurate text analysis results? Explain why?</p>
</div>
</div>
</div>
<p>Following our initial example for Tokenization, we can see how
lemmatization works. We start with processing the text with
<em>spaCy</em> to perform lemmatization:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>lemmas <span class="op">=</span> [token.lemma_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span></code></pre>
</div>
<p>We can print the original text and the lemmatized text:</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original Text:"</span>, perovskite_text)</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Lemmatized Text:"</span>, <span class="st">' '</span>.join(lemmas))</span></code></pre>
</div>
<p>Output:</p>
<p><strong>Original Text</strong>: Perovskite nanocrystals are a class
of semiconductor nanocrystals with unique properties that distinguish
them from traditional quantum dots. These nanocrystals have an ABX3
composition, where ‘A’ can be cesium, methylammonium (MA), or
formamidinium (FA); ‘B’ is typically lead or tin; and ‘X’ is a halogen
ion like chloride, bromide, or iodide. Their remarkable optoelectronic
properties, such as high photoluminescence quantum yields and tunable
emission across the visible spectrum, make them ideal for applications
in light-emitting diodes, lasers, and solar cells.</p>
<p><strong>Lemmatized Text</strong>: Perovskite nanocrystal be a class
of semiconductor nanocrystal with unique property that distinguish they
from traditional quantum dot . these nanocrystal have an ABX3
composition , where ’ A ’ can be cesium , methylammonium ( MA ) , or
formamidinium ( FA ) ; ’ b ’ be typically lead or tin ; and ’ x ’ be a
halogen ion like chloride , bromide , or iodide . their remarkable
optoelectronic property , such as high photoluminescence quantum yield
and tunable emission across the visible spectrum , make they ideal for
application in light - emit diode , laser , and solar cell .</p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout2"></a>
</h3>
<div class="callout-content">
<p>The spaCy library does not have stemming capabilities and if we want
to compare stemming and lemmatization, we also need to use another
language processing library called NLTK (refer to episode 1).</p>
</div>
</div>
</div>
<p>Based on what we just learned let’s compare lemmatization and
stemming. First, we need to import the necessary libraries for stemming
and lemmatization:</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="im">from</span> nltk.stem.porter <span class="im">import</span> PorterStemmer</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>nltk.download(<span class="st">'punkt'</span>)</span></code></pre>
</div>
<p>Next, we can create an instance of the PorterStemmer for NLTK and
load the English language model for spaCy (similar to what we did
earlier in this episode).</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>stemmer <span class="op">=</span> PorterStemmer()</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span></code></pre>
</div>
<p>We can conduct stemming and lemmatization with identical text
data:</p>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>text</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler1" aria-labelledby="headingSpoiler1">
<div class="accordion-body">
<p>text = ” Perovskite nanocrystals are a class of semiconductor
nanocrystals with unique properties that distinguish them from
traditional quantum dots. These nanocrystals have an ABX3 composition,
where ‘A’ can be cesium, methylammonium (MA), or formamidinium (FA); ‘B’
is typically lead or tin; and ‘X’ is a halogen ion like chloride,
bromide, or iodide. Their remarkable optoelectronic properties, such as
high photoluminescence quantum yields and tunable emission across the
visible spectrum, make them ideal for applications in light-emitting
diodes, lasers, and solar cells.” Before we can stem or lemmatize, we
need to tokenize the text.</p>
</div>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>tokens <span class="op">=</span> word_tokenize(text)  </span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a><span class="co"># Apply stemming to each token:</span></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>stemmed_tokens <span class="op">=</span> [stemmer.stem(token) <span class="cf">for</span> token <span class="kw">in</span> tokens]</span></code></pre>
</div>
<p>For lemmatization, we process the text with spaCy and extract the
lemma for each token:</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a></span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>lemmatized_tokens <span class="op">=</span> [token.lemma_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span></code></pre>
</div>
<p>Finally, we can compare the stemmed and lemmatized tokens:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a></span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original Tokens:"</span>, tokens)</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Stemmed Tokens:"</span>, stemmed_tokens)</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Lemmatized Tokens:"</span>, lemmatized_tokens)</span></code></pre>
</div>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Output</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler2" aria-labelledby="headingSpoiler2">
<div class="accordion-body">
<ul>
<li><p>Original Tokens: [‘Perovskite’, ‘nanocrystals’, ‘are’, ‘a’,
‘class’, ‘of’, ‘semiconductor’, ‘nanocrystals’, ‘with’, ‘unique’,
‘properties’, ‘that’, ‘distinguish’, ‘them’, ‘from’, ‘traditional’,
‘quantum’, ‘dots’, ‘.’, ‘These’, ‘nanocrystals’, ‘have’, ‘an’, ‘ABX3’,
‘composition’, ‘,’, ‘where’, “‘“, ‘A’,”’”, ‘can’, ‘be’, ‘cesium’, ‘,’,
‘methylammonium’, ‘(’, ‘MA’, ‘)’, ‘,’, ‘or’, ‘formamidinium’, ‘(’, ‘FA’,
‘)’, ‘;’, “‘“, ‘B’,”’”, ‘is’, ‘typically’, ‘lead’, ‘or’, ‘tin’, ‘;’,
‘and’, “‘“, ‘X’,”’”, ‘is’, ‘a’, ‘halogen’, ‘ion’, ‘like’, ‘chloride’,
‘,’, ‘bromide’, ‘,’, ‘or’, ‘iodide’, ‘.’, ‘Their’, ‘remarkable’,
‘optoelectronic’, ‘properties’, ‘,’, ‘such’, ‘as’, ‘high’,
‘photoluminescence’, ‘quantum’, ‘yields’, ‘and’, ‘tunable’, ‘emission’,
‘across’, ‘the’, ‘visible’, ‘spectrum’, ‘,’, ‘make’, ‘them’, ‘ideal’,
‘for’, ‘applications’, ‘in’, ‘light-emitting’, ‘diodes’, ‘,’, ‘lasers’,
‘,’, ‘and’, ‘solar’, ‘cells’, ‘.’]</p></li>
<li><p>Stemmed Tokens: [‘perovskit’, ‘nanocryst’, ‘are’, ‘a’, ‘class’,
‘of’, ‘semiconductor’, ‘nanocryst’, ‘with’, ‘uniqu’, ‘properti’, ‘that’,
‘distinguish’, ‘them’, ‘from’, ‘tradit’, ‘quantum’, ‘dot’, ‘.’, ‘these’,
‘nanocryst’, ‘have’, ‘an’, ‘abx3’, ‘composit’, ‘,’, ‘where’, “‘“,
‘a’,”’”, ‘can’, ‘be’, ‘cesium’, ‘,’, ‘methylammonium’, ‘(’, ‘ma’, ‘)’,
‘,’, ‘or’, ‘formamidinium’, ‘(’, ‘fa’, ‘)’, ‘;’, “‘“, ‘b’,”’”, ‘is’,
‘typic’, ‘lead’, ‘or’, ‘tin’, ‘;’, ‘and’, “‘“, ‘x’,”’”, ‘is’, ‘a’,
‘halogen’, ‘ion’, ‘like’, ‘chlorid’, ‘,’, ‘bromid’, ‘,’, ‘or’, ‘iodid’,
‘.’, ‘their’, ‘remark’, ‘optoelectron’, ‘properti’, ‘,’, ‘such’, ‘as’,
‘high’, ‘photoluminesc’, ‘quantum’, ‘yield’, ‘and’, ‘tunabl’, ‘emiss’,
‘across’, ‘the’, ‘visibl’, ‘spectrum’, ‘,’, ‘make’, ‘them’, ‘ideal’,
‘for’, ‘applic’, ‘in’, ‘light-emit’, ‘diod’, ‘,’, ‘laser’, ‘,’, ‘and’,
‘solar’, ‘cell’, ‘.’]</p></li>
<li><p>Lemmatized Tokens: [‘Perovskite’, ‘nanocrystal’, ‘be’, ‘a’,
‘class’, ‘of’, ‘semiconductor’, ‘nanocrystal’, ‘with’, ‘unique’,
‘property’, ‘that’, ‘distinguish’, ‘they’, ‘from’, ‘traditional’,
‘quantum’, ‘dot’, ‘.’, ‘these’, ‘nanocrystal’, ‘have’, ‘an’, ‘ABX3’,
‘composition’, ‘,’, ‘where’, “‘“, ‘A’,”’”, ‘can’, ‘be’, ‘cesium’, ‘,’,
‘methylammonium’, ‘(’, ‘MA’, ‘)’, ‘,’, ‘or’, ‘formamidinium’, ‘(’, ‘FA’,
‘)’, ‘;’, “‘“, ‘b’,”’”, ‘be’, ‘typically’, ‘lead’, ‘or’, ‘tin’, ‘;’,
‘and’, “‘“, ‘x’,”’”, ‘be’, ‘a’, ‘halogen’, ‘ion’, ‘like’, ‘chloride’,
‘,’, ‘bromide’, ‘,’, ‘or’, ‘iodide’, ‘.’, ‘their’, ‘remarkable’,
‘optoelectronic’, ‘property’, ‘,’, ‘such’, ‘as’, ‘high’,
‘photoluminescence’, ‘quantum’, ‘yield’, ‘and’, ‘tunable’, ‘emission’,
‘across’, ‘the’, ‘visible’, ‘spectrum’, ‘,’, ‘make’, ‘they’, ‘ideal’,
‘for’, ‘application’, ‘in’, ‘light’, ‘-’, ‘emit’, ‘diode’, ‘,’, ‘laser’,
‘,’, ‘and’, ‘solar’, ‘cell’, ‘.’]</p></li>
</ul>
</div>
</div>
</div>
</div>
<p>We can see how stemming often cuts off the end of words, sometimes
resulting in non-words, while lemmatization returns the base or
dictionary form of the word. For example, stemming might reduce
<strong>“properties”</strong> to <strong>“properti”</strong> while
lemmatization would correctly identify the lemma as
<strong>“property”</strong>. Lemmatization provides a more readable and
meaningful result, which is particularly useful in NLP tasks that
require understanding the context and meaning of words.</p>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-1"></a>
</h3>
<div class="callout-content">
<p>Q: Use the spaCy library to perform lemmatization on the following
text: “Perovskite nanocrystals are a promising class of materials for
optoelectronic applications due to their tunable bandgaps and high
photoluminescence efficiencies.” Print the original text and the
lemmatized text. You can use the following code to load the spacy
library and the English language model:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a></span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a><span class="co"># Load the English language model:</span></span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a><span class="co"># Define the text:</span></span>
<span id="cb26-7"><a href="#cb26-7" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Perovskite nanocrystals are a promising class of materials for optoelectronic applications due to their tunable bandgaps and high photoluminescence efficiencies."</span></span>
<span id="cb26-8"><a href="#cb26-8" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" tabindex="-1"></a><span class="co"># Process the text with spaCy:</span></span>
<span id="cb26-10"><a href="#cb26-10" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb26-11"><a href="#cb26-11" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" tabindex="-1"></a><span class="co"># Print the original text:</span></span>
<span id="cb26-13"><a href="#cb26-13" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original text:"</span>, text)</span>
<span id="cb26-14"><a href="#cb26-14" tabindex="-1"></a><span class="co"># Print the lemmatized text:</span></span>
<span id="cb26-15"><a href="#cb26-15" tabindex="-1"></a>lemmatized_text <span class="op">=</span> <span class="st">" "</span>.join([token.lemma_ <span class="cf">for</span> token <span class="kw">in</span> doc])</span>
<span id="cb26-16"><a href="#cb26-16" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Lemmatized text:"</span>, lemmatized_text)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</section><section id="stop-words-removal"><h2 class="section-heading">2.4. Stop-words Removal<a class="anchor" aria-label="anchor" href="#stop-words-removal"></a>
</h2>
<hr class="half-width">
<p>Removing stop-words, which are common words that add little value to
the analysis (such as ‘and’ and ‘the’), helps focus on the important
content. Assuming ‘doc’ is the processed text from the previous example
for ‘perovskite nanocrystals’, we can define a list to hold non-stop
words list comprehensions:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a></span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a>filtered_sentence <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> doc <span class="cf">if</span> <span class="kw">not</span> word.is_stop]</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a><span class="co"># print the filtered sentence and see how it is changed:</span></span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Filtered sentence:"</span>, filtered_sentence)</span></code></pre>
</div>
<p>Output: Filtered sentence: [Perovskite, nanocrystals, class,
semiconductor, nanocrystals, unique, properties, distinguish,
traditional, quantum, dots, ., nanocrystals, ABX3, composition, ,, ‘,’,
cesium, ,, methylammonium, (, MA, ), ,, formamidinium, (, FA, ), ;, ‘,
B,’, typically, lead, tin, ;, ‘, X,’, halogen, ion, like, chloride, ,,
bromide, ,, iodide, ., remarkable, optoelectronic, properties, ,, high,
photoluminescence, quantum, yields, tunable, emission, visible,
spectrum, ,, ideal, applications, light, -, emitting, diodes, ,, lasers,
,, solar, cells, .]</p>
<p>List comprehensions provide a convenient method for rapidly
generating lists based on a straightforward condition.</p>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-2"></a>
</h3>
<div class="callout-content">
<p>Q: To see how list comprehensions are created, fill in the missing
parts of the code to remove stop-words from a given sentence.</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="im">import</span> _____</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>doc <span class="op">=</span> nlp(<span class="st">"This is a very simple and short sentence."</span>)</span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a>filtered_sentence <span class="op">=</span> [____ <span class="cf">for</span> ____ <span class="kw">in</span> doc <span class="cf">if</span> <span class="kw">not</span> ____]</span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Filtered sentence:"</span>, filtered_sentence)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>doc <span class="op">=</span> nlp(<span class="st">"This is a very simple and short sentence."</span>)</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>filtered_sentence <span class="op">=</span> [word <span class="cf">for</span> words <span class="kw">in</span> doc <span class="cf">if</span> <span class="kw">not</span> word.is_stop]</span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Filtered sentence:"</span>, filtered_sentence)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="warning" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="warning" class="callout-inner">
<h3 class="callout-title">Warning!<a class="anchor" aria-label="anchor" href="#warning"></a>
</h3>
<div class="callout-content">
<p>While stopwords are often removed to improve analysis, they can be
important for certain tasks like sentiment analysis, where the word
‘not’ can change the entire meaning of a sentence.</p>
</div>
</div>
</div>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What Else Might We Want to Know About Tokenization?</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler3" aria-labelledby="headingSpoiler3">
<div class="accordion-body">
<p>It is important to note that tokenization is just the beginning. In
modern NLP, vectorization, and embeddings play a pivotal role in
capturing the context and meaning of text.</p>
<p>Vectorization is the process of converting tokens into a numerical
format that machine learning models can understand. This often involves
creating a bag-of-words model, where each token is represented by a
unique number in a vector. Embeddings are advanced representations where
words are mapped to vectors of real numbers. They capture not just the
presence of tokens but also the semantic relationships between them.
This is achieved through techniques like Word2Vec, GloVe, or BERT, which
we will explore in the second part of our workshop.</p>
<p>These embeddings allow models to understand the text in a more
nuanced way, leading to better performance on tasks such as sentiment
analysis, machine translation, and more.</p>
<p><em>Stay tuned for our next session, where we will dive deeper into
how we can use vectorization and embeddings to enhance our NLP models
and truly capture the richness of language.</em></p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Text preprocessing is essential for cleaning and standardizing text
data.</li>
<li>Techniques like sentence segmentation, tokenization, stemming, and
lemmatization are fundamental to text preprocessing.</li>
<li>Removing stop-words helps in focusing on the important words in text
analysis.</li>
<li>Tokenization splits sentences into tokens, which are the basic units
for further processing.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-03-text-analysis"><p>Content from <a href="03-text-analysis.html">Text Analysis</a></p>
<hr>
<p>Last updated on 2024-05-15 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/03-text-analysis.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are text analysis methods?</li>
<li>How can I perform text analysis?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Define objectives associated with each one of the text analysis
techniques.</li>
<li>Implement named entity recognition, and topic modeling using Python
libraries and frameworks, such as NLTK, and Gensim.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="introduction-to-text-analysis"><h2 class="section-heading">3.1. Introduction to Text-Analysis<a class="anchor" aria-label="anchor" href="#introduction-to-text-analysis"></a>
</h2>
<hr class="half-width">
<p>In this episode, we will learn how to analyze text data for NLP
tasks. We will explore some common techniques and methods for text
analysis, such as named entity recognition, topic modeling, and text
summarization. We will use some popular libraries and frameworks, such
as spaCy, NLTK, and Gensim, to implement these techniques and
methods.</p>
<div id="discussion" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: What are some of the goals of text analysis for NLP tasks
in your research field (e.g. material science)? Think of some examples
of NLP tasks that require text analysis, such as literature review,
patent analysis, or material discovery. How does text analysis help to
achieve these goals?</p>
</div>
</div>
</div>
<div id="discussion-1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Name some of the common techniques in text analysis and
associating libraries. Briefly explain how they differ from each other
in terms of their objectives and required libraries.</p>
</div>
</div>
</div>
</section><section id="named-entity-recognition"><h2 class="section-heading">3.1. Named Entity Recognition<a class="anchor" aria-label="anchor" href="#named-entity-recognition"></a>
</h2>
<hr class="half-width">
<p>Named Entity Recognition is a process of identifying and classifying
key elements in text into predefined categories. The categories could be
names of persons, organizations, locations, expressions of times,
quantities, monetary values, percentages, etc. Next, let’s discuss how
it works.</p>
<div id="discussion-2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-2" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-2"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Discuss what tasks can be done with NER.</p>
<p>A: <em>NER can help with 1) categorizing resumes, 2) categorizing
customer feedback, 3) categorizing research papers, etc.</em></p>
</div>
</div>
</div>
<p>Using a text example from Wikipedia can help us to see how NER works.
Note that the spaCy library is a common framework here as well. Thus,
first, we make sure that the library is installed and imported:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>pip install spacy</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span></code></pre>
</div>
<p>Create an NLP model (<em>nlp</em>) and download the small English
model from spaCy that is suitable for general tasks.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span></code></pre>
</div>
<p>Create a variable to store your text and then apply the model to
process your text (text from <a href="https://en.wikipedia.org/wiki/Australian_Securities_Exchange" class="external-link">Wikipedia</a>):</p>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Text</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p>text = “Australian Shares Exchange Ltd (ASX) is an Australian public
company that operates Australia’s primary shares exchange, the
Australian Shares Exchange (sometimes referred to outside of Australia
as, or confused within Australia as, The Sydney Stock Exchange, a
separate entity). The ASX was formed on 1 April 1987, through
incorporation under legislation of the Australian Parliament as an
amalgamation of the six state securities exchanges, and merged with the
Sydney Futures Exchange in 2006. Today, ASX has an average daily
turnover of A$4.685 billion and a market capitalization of around A$1.6
trillion, making it one of the world’s top 20 listed exchange groups,
and the largest in the southern hemisphere. ASX Clear is the clearing
house for all shares, structured products, warrants and, ASX Equity
Derivatives.”</p>
</div>
</div>
</div>
</div>
<p>Use for loop to print all the named entities in the document:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>For ent <span class="kw">in</span> doc.ents:</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    Print(ent.text, ent.label_)</span></code></pre>
</div>
<p>The results will be:</p>
<pre><code>
output:

Australian Shares Exchange Ltd ORG
ASX ORG
Australian NORP
Australia GPE
the Australian Shares Exchange ORG
Australia GPE
Australia GPE
The Sydney Stock Exchange ORG
ASX ORG
1 April 1987 DATE
the Australian Parliament ORG
six CARDINAL
the Sydney Futures Exchange ORG
2006 DATE
Today DATE
ASX ORG
A$4.685 billion MONEY
around A$1.6 trillion MONEY
20 CARDINAL
</code></pre>
<div id="challenge" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge"></a>
</h3>
<div class="callout-content">
<p>Q: How can you interpret the labels in the output?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>A: You can use the following code to get information about each one
of the labels. For example, we want to know what GPE represents here. We
can use <em>explain()</em> to get the required information:
spacy.explain(‘GPE’)</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>spacy.explain(‘GPE’)</span></code></pre>
</div>
<pre><code>Output: ‘Countries, cities, states’
</code></pre>
</div>
</div>
</div>
</div>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-1"></a>
</h3>
<div class="callout-content">
<p>Q: Can we also use other libraries for NER analysis? Use the NLTK
library to perform named entity recognition on the following text:</p>
</div>
</div>
</div>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Text</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" aria-labelledby="headingSpoiler2" data-bs-parent="#accordionSpoiler2">
<div class="accordion-body">
<p>text = ” Perovskite nanocrystals have emerged as a promising class of
materials for next-generation optoelectronic devices due to their unique
properties. Their crystal structure allows for tunable bandgaps, which
are the energy differences between occupied and unoccupied electronic
states. This tunability enables the creation of materials that can
absorb and emit light across a wide range of the electromagnetic
spectrum, making them suitable for applications like solar cells,
light-emitting diodes (LEDs), and lasers. Additionally, perovskite
nanocrystals exhibit high photoluminescence efficiencies, meaning they
can efficiently convert absorbed light into emitted light, further
adding to their potential for various optoelectronic applications.”</p>
</div>
</div>
</div>
</div>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge<em>(continued)</em><a class="anchor" aria-label="anchor" href="#challenge-1"></a>
</h3>
<div class="callout-content">
<p>Print the original text and the list of named entities and their
types. You can use the following code to load the NLTK library and the
pre-trained model for named entity recognition:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>nltk.download(<span class="st">'maxent_ne_chunker'</span>)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>nltk.download(<span class="st">'words'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>...</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>A: Download the necessary NLTK resources and import the required
toolkit:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>nltk.download(<span class="st">'punkt'</span>)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>nltk.download(<span class="st">'averaged_perceptron_tagger'</span>)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>nltk.download(<span class="st">'maxent_ne_chunker'</span>)</span></code></pre>
</div>
<p>Store the text:</p>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Text</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" aria-labelledby="headingSpoiler3" data-bs-parent="#accordionSpoiler3">
<div class="accordion-body">
<p>text = “Perovskite nanocrystals have emerged as a promising class of
materials for next-generation optoelectronic devices due to their unique
properties. Their crystal structure allows for tunable bandgaps, which
are the energy differences between occupied and unoccupied electronic
states. This tunability enables the creation of materials that can
absorb and emit light across a wide range of the electromagnetic
spectrum, making them suitable for applications like solar cells,
light-emitting diodes (LEDs), and lasers. Additionally, perovskite
nanocrystals exhibit high photoluminescence efficiencies, meaning they
can efficiently convert absorbed light into emitted light, further
adding to their potential for various optoelectronic applications.”</p>
</div>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="co"># Tokenize the text:</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>tokens <span class="op">=</span> nltk.word_tokenize(text)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="co"># Assign part-of-speech tags:</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>pos_tags <span class="op">=</span> nltk.pos_tag(tokens)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a><span class="co"># Perform named entity recognition:</span></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>named_entities <span class="op">=</span> nltk.ne_chunk(pos_tags)</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a><span class="co"># Print the original text:</span></span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original Text:"</span>)</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a><span class="bu">print</span>(text)</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a><span class="co"># Print named entities and their types:</span></span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Named Entities:"</span>)</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a><span class="cf">for</span> entity <span class="kw">in</span> named_entities:</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>   <span class="cf">if</span> <span class="bu">type</span>(entity) <span class="op">==</span> nltk.Tree:</span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>   <span class="bu">print</span>(<span class="ss">f"Entity: </span><span class="sc">{</span><span class="st">''</span><span class="sc">.</span>join(word <span class="cf">for</span> word, _ <span class="kw">in</span> entity.leaves())<span class="sc">}</span><span class="ss">, Type: </span><span class="sc">{</span>entity<span class="sc">.</span>label()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div id="accordionSpoiler4" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler4" aria-expanded="false" aria-controls="collapseSpoiler4">
  <h3 class="accordion-header" id="headingSpoiler4">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Output</h3>
</button>
<div id="collapseSpoiler4" class="accordion-collapse collapse" aria-labelledby="headingSpoiler4" data-bs-parent="#accordionSpoiler4">
<div class="accordion-body">
<p>Original Text: Perovskite nanocrystals have emerged as a promising
class of materials for next-generation optoelectronic devices due to
their unique properties. Their crystal structure allows for tunable
bandgaps, which are the energy differences between occupied and
unoccupied electronic states. This tunability enables the creation of
materials that can absorb and emit light across a wide range of the
electromagnetic spectrum, making them suitable for applications like
solar cells, light-emitting diodes (LEDs), and lasers. Additionally,
perovskite nanocrystals exhibit high photoluminescence efficiencies,
meaning they can efficiently convert absorbed light into emitted light,
further adding to their potential for various optoelectronic
applications.</p>
<ul>
<li>Named Entities:</li>
<li>Entity: Perovskite, Type: ORGANIZATION</li>
<li>Entity: light-emitting diodes (LEDs), Type: ORGANIZATION</li>
</ul>
<p>[((‘Perovskite’, ‘NNP’), ‘ORGANIZATION’), (‘nanocrystals’, ‘NNP’),
(‘have’, ‘VBP’), (‘emerged’, ‘VBD’), (‘as’, ‘IN’), (‘a’, ‘DT’),
(‘promising’, ‘JJ’), (‘class’, ‘NN’), (‘of’, ‘IN’), (‘materials’,
‘NNS’), (‘for’, ‘IN’), (‘next-generation’, ‘JJ’), (‘optoelectronic’,
‘JJ’), (‘devices’, ‘NNS’), (‘due’, ‘IN’), (‘to’, ‘TO’), (‘their’,
‘PRP<span class="math inline">\('), ('unique',
'JJ'), ('properties', 'NNS'), ('.',
'.'), ('Their', 'PRP\)</span>’), (‘crystal’, ‘NN’),
(‘structure’, ‘NN’), (‘allows’, ‘VBZ’), (‘for’, ‘IN’), (‘tunable’,
‘JJ’), (‘bandgaps’, ‘NNS’), (‘,’, ‘,’), (‘which’, ‘WDT’), (‘are’,
‘VBP’), (‘the’, ‘DT’), (‘energy’, ‘NN’), (‘differences’, ‘NNS’),
(‘between’, ‘IN’), (‘occupied’, ‘VBN’), (‘and’, ‘CC’), (‘unoccupied’,
‘VBN’), (‘electronic’, ‘JJ’), (‘states’, ‘NNS’), (‘.’, ‘.’), (‘This’,
‘DT’), (‘tunability’, ‘NN’), (‘enables’, ‘VBZ’), (‘the’, ‘DT’),
(‘creation’, ‘NN’), (‘of’, ‘IN’), (‘materials’, ‘NNS’), (‘that’, ‘WDT’),
(‘can’, ‘MD’), (‘absorb’, ‘VB’), (‘and’, ‘CC’), (‘emit’, ‘VB’),
(‘light’, ‘NN’), (‘across’, ‘IN’), (‘a’, ‘DT’), (‘wide’, ‘JJ’),
(‘range’, ‘NN’), (‘of’, ‘IN’), (‘the’, ‘DT’), (‘electromagnetic’, ‘JJ’),
(‘spectrum’, ‘NN’), (‘,’, ‘,’), (‘making’, ‘VBG’), (‘them’, ‘PRP’),
(‘suitable’, ‘JJ’), (‘for’, ‘IN’), (‘applications’, ‘NNS’), (‘like’,
‘IN’), (‘solar’, ‘JJ’), (‘cells’, ‘NNS’), (‘,’, ‘,’), (‘light-emitting’,
‘JJ’), (‘diodes’, ‘NNS’), (‘(’, ‘(’, ‘LEDs’, ‘NNPS’), ‘)’, ‘)’), (‘and’,
‘CC’), (‘lasers’, ‘NNS’), (‘.’, ‘.’), (‘Additionally’, ‘RB’), (‘,’,
‘,’), (‘perovskite’, ‘NNP’), (‘nanocrystals’, ‘NNP’), (‘exhibit’,
‘VBP’), (‘high’, ‘JJ’), (‘photoluminescence’, ‘NN’), (‘efficiencies’,
‘NNS’), (‘,’, ‘,’), (‘meaning’, ‘VBG’), (‘they’, ‘PRP’), (‘can’, ‘MD’),
(‘efficiently’, ‘RB’), (‘convert’, ‘VB’), (‘absorbed’, ‘VBN’), (‘light’,
‘NN’), (‘into’, ‘IN’), (‘emitted’, ‘VBN’), (‘light’, ‘NN’), (‘,’, ‘,’),
(‘further’, ‘RB’), (‘adding’, ‘VBG’), (‘to’, ‘TO’), (‘their’, ‘PRP$’),
(‘potential’, ‘NN’), (‘for’, ‘IN’), (‘various’, ‘JJ’),
(‘optoelectronic’, ‘JJ’), (‘applications’, ‘NNS’), (‘.’, ‘.’)]</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="why-ner" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="why-ner" class="callout-inner">
<h3 class="callout-title">Why NER?<a class="anchor" aria-label="anchor" href="#why-ner"></a>
</h3>
<div class="callout-content">
<p>When do we need to perform NER for your research?</p>
<p>NER helps in quickly finding specific information in large datasets,
which is particularly useful in research fields for categorizing the
text based on the entities. NER is also called entity chunking and
entity extraction.</p>
</div>
</div>
</div>
</section><section id="topic-modeling"><h2 class="section-heading">3.2. Topic Modeling<a class="anchor" aria-label="anchor" href="#topic-modeling"></a>
</h2>
<hr class="half-width">
<p>Topic Modeling is an unsupervised model for discovering the abstract
“topics” that occur in a collection of documents. It is useful in
understanding the main themes of a large corpus of text. To better
understand this and to find the connection between concepts we have
learned so far, let’s match the following terms to their brief
definitions:</p>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-2"></a>
</h3>
<div class="callout-content">
<p>Teamwork: To better understand this and to find the connection
between concepts we have learned so far, let’s match the following terms
to their brief definitions:</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/2be22cfb-4ed8-4f65-85bf-31962a40835f" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/d7d9a9a4-c2cb-4f37-bc6c-7b2f99133fe3" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
</div>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout2"></a>
</h3>
<div class="callout-content">
<p>There are some new concepts in this section that are new to you.
Although a detailed explanation of these concepts is out of the scope of
this workshop we learn their basic definitions. You already learned a
few of them in the earlier activity. Another one is Bag-of-words. We
will learn more about Bag-of-words (BoW) in episode 5. BoW is defined as
a representation of text that describes the occurrence of words within a
document. It is needed in the topic modeling analysis to view the
frequency of the words in a document regardless of the order of the
words in the text.</p>
</div>
</div>
</div>
<p>To see how Topic Modeling can help us in action to classify a text,
let’s see the following example. We need to install the <em>Gensim</em>
library and import the necessary modules:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>pip install genism</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> corpora</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> LdaModel</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a></span></code></pre>
</div>
<p>at this stage, we preprocess including the text tokenization (text
from <a href="https://en.wikipedia.org/wiki/Australian_Securities_Exchange" class="external-link">Wikipedia</a>):</p>
<div id="accordionSpoiler5" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler5" aria-expanded="false" aria-controls="collapseSpoiler5">
  <h3 class="accordion-header" id="headingSpoiler5">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Text</h3>
</button>
<div id="collapseSpoiler5" class="accordion-collapse collapse" aria-labelledby="headingSpoiler5" data-bs-parent="#accordionSpoiler5">
<div class="accordion-body">
<p>text = “Australian Shares Exchange Ltd (ASX) is an Australian public
company that operates Australia’s primary shares exchange, the
Australian Shares Exchange (sometimes referred to outside of Australia
as, or confused within Australia as, The Sydney Stock Exchange, a
separate entity). The ASX was formed on 1 April 1987, through
incorporation under legislation of the Australian Parliament as an
amalgamation of the six state securities exchanges and merged with the
Sydney Futures Exchange in 2006. Today, ASX has an average daily
turnover of A$4.685 billion and a market capitalisation of around A$1.6
trillion, making it one of the world’s top 20 listed exchange groups,
and the largest in the southern hemisphere. ASX Clear is the clearing
house for all shares, structured products, warrants and ASX Equity
Derivatives. ASX Group[3] is a market operator, clearing house and
payments system facilitator. It also oversees compliance with its
operating rules, promotes standards of corporate governance among
Australia’s listed companies and helps to educate retail investors.
Australia’s capital markets. Financial development – Australia was
ranked 5th out of 57 of the world’s leading financial systems and
capital markets by the World Economic Forum; Equity market – the 8th
largest in the world (based on free-float market capitalisation) and the
2nd largest in Asia-Pacific, with A$1.2 trillion market capitalisation
and average daily secondary trading of over A$5 billion a day; Bond
market – 3rd largest debt market in the Asia Pacific; Derivatives market
– largest fixed income derivatives in the Asia-Pacific region; Foreign
exchange market – the Australian foreign exchange market is the 7th
largest in the world in terms of global turnover, while the Australian
dollar is the 5th most traded currency and the AUD/USD the 4th most
traded currency pair; Funds management – Due in large part to its
compulsory superannuation system, Australia has the largest pool of
funds under management in the Asia-Pacific region, and the 4th largest
in the world. Its primary markets are the AQUA Markets. Regulation. The
Australian Securities &amp; Investments Commission (ASIC) has
responsibility for the supervision of real-time trading on Australia’s
domestic licensed financial markets and the supervision of the conduct
by participants (including the relationship between participants and
their clients) on those markets. ASIC also supervises ASX’s own
compliance as a public company with ASX Listing Rules. ASX Compliance is
an ASX subsidiary company that is responsible for monitoring and
enforcing ASX-listed companies’ compliance with the ASX operating rules.
The Reserve Bank of Australia (RBA) has oversight of the ASX’s clearing
and settlement facilities for financial system stability. In November
1903 the first interstate conference was held to coincide with the
Melbourne Cup. The exchanges then met on an informal basis until 1937
when the Australian Associated Stock Exchanges (AASE) was established,
with representatives from each exchange. Over time the AASE established
uniform listing rules, broker rules, and commission rates. Trading was
conducted by a call system, where an exchange employee called the names
of each company and brokers bid or offered on each. In the 1960s this
changed to a post system. Exchange employees called”chalkies” wrote bids
and offers in chalk on blackboards continuously, and recorded
transactions made. The ASX (Australian Stock Exchange Limited) was
formed in 1987 by legislation of the Australian Parliament which enabled
the amalgamation of six independent stock exchanges that formerly
operated in the state capital cities. After demutualisation, the ASX was
the first exchange in the world to have its shares quoted on its own
market. The ASX was listed on 14 October 1998.[7] On 7 July 2006 the
Australian Stock Exchange merged with SFE Corporation, holding company
for the Sydney Futures Exchange. Trading system. ASX Group has two
trading platforms – ASX Trade,[12] which facilitates the trading of ASX
equity securities and ASX Trade24 for derivative securities trading. All
ASX equity securities are traded on screen on ASX Trade. ASX Trade is a
NASDAQ OMX ultra-low latency trading platform based on NASDAQ OMX’s
Genium INET system, which is used by many exchanges around the world. It
is one of the fastest and most functional multi-asset trading platforms
in the world, delivering latency down to ~250 microseconds. ASX Trade24
is ASX global trading platform for derivatives. It is globally
distributed with network access points (gateways) located in Chicago,
New York, London, Hong Kong, Singapore, Sydney and Melbourne. It also
allows for true 24-hour trading, and simultaneously maintains two active
trading days which enables products to be opened for trading in the new
trading day in one time zone while products are still trading under the
previous day. Opening times. The normal trading or business days of the
ASX are week-days, Monday to Friday. ASX does not trade on national
public holidays: New Year’s Day (1 January), Australia Day (26 January,
and observed on this day or the first business day after this date),
Good Friday (that varies each year), Easter Monday, Anzac day (25
April), Queen’s birthday (June), Christmas Day (25 December) and Boxing
Day (26 December). On each trading day there is a pre-market session
from 7:00 am to 10:00 am AEST and a normal trading session from 10:00 am
to 4:00 pm AEST. The market opens alphabetically in single-price
auctions, phased over the first ten minutes, with a small random time
built in to prevent exact prediction of the first trades. There is also
a single-price auction between 4:10 pm and 4:12 pm to set the daily
closing prices. Settlement. Security holders hold shares in one of two
forms, both of which operate as uncertificated holdings, rather than
through the issue of physical share certificates: Clearing House
Electronic Sub-register System (CHESS). The investor’s controlling
participant (normally a broker) sponsors the client into CHESS. The
security holder is given a “holder identification number” (HIN) and
monthly statements are sent to the security holder from the CHESS system
when there is a movement in their holding that month. Issuer-sponsored.
The company’s share register administers the security holder’s holding
and issues the investor with a security-holder reference number (SRN)
which may be quoted when selling. Holdings may be moved from
issuer-sponsored to CHESS or between different brokers by electronic
message initiated by the controlling participant. Short selling. Main
article: Short (finance). Short selling of shares is permitted on the
ASX, but only among designated stocks and with certain conditions: ASX
trading participants (brokers) must report all daily gross short sales
to ASX. The report will aggregate the gross short sales as reported by
each trading participant at an individual stock level. ASX publishes
aggregate gross short sales to ASX participants and the general
public.[13] Many brokers do not offer short selling to small private
investors. LEPOs can serve as an equivalent, while contracts for
difference (CFDs) offered by third-party providers are another
alternative. In September 2008, ASIC suspended nearly all forms of short
selling due to concerns about market stability in the ongoing global
financial crisis.[14][15] The ban on covered short selling was lifted in
May 2009.[16] Also, in the biggest change for ASX in 15 years, ASTC
Settlement Rule 10.11.12 was introduced, which requires the broker to
provide stocks when settlement is due, otherwise the broker must buy the
stock on the market to cover the shortfall. The rule requires that if a
Failed Settlement Shortfall exists on the second business day after the
day on which the Rescheduled Batch Instruction was originally scheduled
for settlement (that is, generally on T+5), the delivering settlement
participant must either: close out the Failed Settlement Shortfall on
the next business day by purchasing the number of Financial Products of
the relevant class equal to the shortfall; or acquire under a securities
lending arrangement the number of Financial Products of the relevant
class equal to the shortfall and deliver those Financial Products in
Batch Settlement no more than two business days later.[17] Options.
Options on leading shares are traded on the ASX, with standardised sets
of strike prices and expiry dates. Liquidity is provided by market
makers who are required to provide quotes. Each market maker is assigned
two or more stocks. A stock can have more than one market maker, and
they compete with one another. A market maker may choose one or both of:
Make a market continuously, on a set of 18 options. Make a market in
response to a quote request, in any option up to 9 months out. In both
cases there is a minimum quantity (5 or 10 contracts depending on the
shares) and a maximum spread permitted. Due to the higher risks in
options, brokers must check clients’ suitability before allowing them to
trade options. Clients may both take (i.e. buy) and write (i.e. sell)
options. For written positions, the client must put up margin. Interest
rate market. The ASX interest rate market is the set of corporate bonds,
floating rate notes, and bond-like preference shares listed on the
exchange. These securities are traded and settled in the same way as
ordinary shares, but the ASX provides information such as their
maturity, effective interest rate, etc., to aid comparison.[18] Futures.
The Sydney Futures Exchange (SFE) was the 10th largest derivatives
exchange in the world, providing derivatives in interest rates,
equities, currencies and commodities. The SFE is now part of ASX and its
most active products are: SPI 200 Futures – Futures contracts on an
index representing the largest 200 stocks on the Australian Stock
Exchange by market capitalisation. AU 90-day Bank Accepted Bill Futures
– Australia’s equivalent of T-Bill futures. 3-Year Bond Futures –
Futures contracts on Australian 3-year bonds. 10-Year Bond Futures –
Futures contracts on Australian 10-year bonds. The ASX trades futures
over the ASX 50, ASX 200 and ASX property indexes, and over grain,
electricity and wool. Options over grain futures are also traded. Market
indices. The ASX maintains stock indexes concerning stocks traded on the
exchange in conjunction with Standard &amp; Poor’s. There is a hierarchy
of index groups called the S&amp;P/ASX 20, S&amp;P/ASX 50, S&amp;P/ASX
100, S&amp;P/ASX 200 and S&amp;P/ASX 300, notionally containing the 20,
50, 100, 200 and 300 largest companies listed on the exchange, subject
to some qualifications. Sharemarket Game. The ASX Sharemarket Game give
members of the public and secondary school students the chance to learn
about investing in the sharemarket using real market prices.
Participants receive a hypothetical $50,000 to buy and sell shares in
150 companies and track the progress of their investments over the
duration of the game.[19] Merger talks with SGX. ASX was (25 October
2010) in merger talks with Singapore Exchange (SGX). While there was an
initial expectation that the merger would have created a bourse with a
market value of US$14 billion,[20] this was a misconception; the final
proposal intended that the ASX and SGX bourses would have continued
functioning separately. The merger was blocked by Treasurer of Australia
Wayne Swan on 8 April 2011, on advice from the Foreign Investment Review
Board that the proposed merger was not in the best interests of
Australia.[21]”</p>
</div>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>tokens <span class="op">=</span> simple_preprocess(text)</span></code></pre>
</div>
<p>For Topic Modeling we need to map each word to a unique ID through
creating a dictionary:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>dictionary <span class="op">=</span> corpora.Dictionary([tokens])</span></code></pre>
</div>
<p>And the created dictionary should be converted into a bag-of-words.
We do that with doc2bow().</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>corpus <span class="op">=</span> [dictionary.doc2bow(tokens)]</span></code></pre>
</div>
<p>Next, we use Latent Dirichlet Allocation (LDA) which is a popular
topic modeling technique because it assumes documents are produced from
a mixture of topics. These topics then generate words based on their
probability distribution. Set up the LDA model with the number of topics
and train it on the corpus:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>lda_model <span class="op">=</span> LdaModel(corpus<span class="op">=</span>corpus, id2word<span class="op">=</span>dictionary, num_topics<span class="op">=</span><span class="dv">1</span>)</span></code></pre>
</div>
<p>Finally, we can print the topics and their word distributions from
our text:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>topics <span class="op">=</span> lda_model.print_topics(num_words<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>    <span class="cf">for</span> topic <span class="kw">in</span> topics:</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>    <span class="bu">print</span>(topic)</span></code></pre>
</div>
<div id="discussion-3" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-3" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-3"></a>
</h3>
<div class="callout-content">
<p>Teamwork: How does the topic modeling method help researchers? What
about the text summarization? What are some of the challenges and
limitations of text analysis in your research field (material science)?
Consider some of the factors that affect the quality and accuracy of
text analysis, such as data availability, language diversity, and domain
specificity. How do these factors pose problems or difficulties for text
analysis in material science?</p>
</div>
</div>
</div>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-3"></a>
</h3>
<div class="callout-content">
<p>Q: Use the Gensim library to perform topic modeling on the following
text, print the original text and the list of topics and their
keywords.</p>
</div>
</div>
</div>
<div id="accordionSpoiler6" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler6" aria-expanded="false" aria-controls="collapseSpoiler6">
  <h3 class="accordion-header" id="headingSpoiler6">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Text</h3>
</button>
<div id="collapseSpoiler6" class="accordion-collapse collapse" aria-labelledby="headingSpoiler6" data-bs-parent="#accordionSpoiler6">
<div class="accordion-body">
<p>text = ” Perovskite nanocrystals have emerged as a promising class of
materials for next-generation optoelectronic devices due to their unique
properties. Their crystal structure allows for tunable bandgaps, which
are the energy differences between occupied and unoccupied electronic
states. This tunability enables the creation of materials that can
absorb and emit light across a wide range of the electromagnetic
spectrum, making them suitable for applications like solar cells,
light-emitting diodes (LEDs), and lasers. Additionally, perovskite
nanocrystals exhibit high photoluminescence efficiencies, meaning they
can efficiently convert absorbed light into emitted light, further
adding to their potential for various optoelectronic applications.”</p>
</div>
</div>
</div>
</div>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge<em>(continued)</em><a class="anchor" aria-label="anchor" href="#challenge-3"></a>
</h3>
<div class="callout-content">
<p>You can use the following code to load the Gensim library and the
pre-trained model for topic modeling:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> corpora</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> LdaModel</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> corpora</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> LdaModel</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>tokens <span class="op">=</span> simple_preprocess(text)</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>dictionary <span class="op">=</span> corpora.Dictionary([tokens])</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>corpus <span class="op">=</span> [dictionary.doc2bow(tokens)]</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>model <span class="op">=</span> LdaModel(corpus, num_topics<span class="op">=</span><span class="dv">2</span>, id2word<span class="op">=</span>dictionary)</span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a><span class="bu">print</span>(text)</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a><span class="bu">print</span>(model.print_topics())</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-4"></a>
</h3>
<div class="callout-content">
<p>Q: Use the genism to perform topic modeling on the following two
different texts and provide a comparison.</p>
</div>
</div>
</div>
<div id="accordionSpoiler7" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler7" aria-expanded="false" aria-controls="collapseSpoiler7">
  <h3 class="accordion-header" id="headingSpoiler7">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Text</h3>
</button>
<div id="collapseSpoiler7" class="accordion-collapse collapse" aria-labelledby="headingSpoiler7" data-bs-parent="#accordionSpoiler7">
<div class="accordion-body">
<p>text1 = “Perovskite nanocrystals have emerged as a promising class of
materials for next-generation optoelectronic devices due to their unique
properties. Their crystal structure allows for tunable bandgaps, which
are the energy differences between occupied and unoccupied electronic
states. This tunability enables the creation of materials that can
absorb and emit light across a wide range of the electromagnetic
spectrum, making them suitable for applications like solar cells,
light-emitting diodes (LEDs), and lasers. Additionally, perovskite
nanocrystals exhibit high photoluminescence efficiencies, meaning they
can efficiently convert absorbed light into emitted light, further
adding to their potential for various optoelectronic applications.”</p>
<p>text2 = “Graphene is a one-atom-thick sheet of carbon atoms arranged
in a honeycomb lattice. It is a remarkable material with unique
properties, including high electrical conductivity, thermal
conductivity, mechanical strength, and optical transparency. Graphene
has the potential to revolutionize various fields, including
electronics, photonics, and composite materials. Due to its excellent
electrical conductivity, graphene is a promising candidate for
next-generation electronic devices, such as transistors and sensors.
Additionally, its high thermal conductivity makes it suitable for heat
dissipation applications.”</p>
</div>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">Show me the solution</h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<p>A: After storing the two texts in text1 and text2, preprocess the
text (e.g., tokenization, stop word removal, stemming/lemmatization).
Split the texts into documents:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> corpora</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>documents <span class="op">=</span> [text1.split(), text2.split()]</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a><span class="co"># Create a dictionary:</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>dictionary <span class="op">=</span> corpora.Dictionary(documents)</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a><span class="co"># Create a corpus (bag of words representation):</span></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a>corpus <span class="op">=</span> [dictionary.doc2bow(doc) <span class="cf">for</span> doc <span class="kw">in</span> documents]</span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a><span class="co"># Train the LDA model (adjust num_topics as needed):</span></span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a>lda_model <span class="op">=</span> gensim.models.LdaModel(corpus, id2word<span class="op">=</span>dictionary, num_topics<span class="op">=</span><span class="dv">3</span>, passes<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a><span class="co"># Print the original texts:</span></span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original Texts:"</span>)</span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Text 1:</span><span class="ch">\n</span><span class="sc">{</span>text1<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Text 2:</span><span class="ch">\n</span><span class="sc">{</span>text2<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a><span class="co"># Identify shared and distinct keywords for each topic:</span></span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Topics and Keywords:"</span>)</span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a><span class="cf">for</span> topic <span class="kw">in</span> lda_model.show_topics(formatted<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Topic </span><span class="sc">{</span>topic[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a>    topic_words <span class="op">=</span> [w[<span class="dv">0</span>] <span class="cf">for</span> w <span class="kw">in</span> topic[<span class="dv">1</span>]]</span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Text 1 Keywords:"</span>, [w <span class="cf">for</span> w <span class="kw">in</span> topic_words <span class="cf">if</span> w <span class="kw">in</span> text1])</span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Text 2 Keywords:"</span>, [w <span class="cf">for</span> w <span class="kw">in</span> topic_words <span class="cf">if</span> w <span class="kw">in</span> text2])</span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a><span class="co"># Explain the conceptual similarity:</span></span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Conceptual Similarity:"</span>)</span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Both texts discuss novel materials (perovskite nanocrystals and graphene) with unique properties. While the specific applications and functionalities differ slightly, they both highlight the potential of these materials for various technological advancements."</span>)</span></code></pre>
</div>
<div id="accordionSpoiler8" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler8" aria-expanded="false" aria-controls="collapseSpoiler8">
  <h3 class="accordion-header" id="headingSpoiler8">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Output</h3>
</button>
<div id="collapseSpoiler8" class="accordion-collapse collapse" aria-labelledby="headingSpoiler8" data-bs-parent="#accordionSpoiler8">
<div class="accordion-body">
<p>Original Texts:</p>
<p>Text 1: Perovskite nanocrystals have emerged as a promising class of
materials for next-generation optoelectronic devices due to their unique
properties. Their crystal structure allows for tunable bandgaps, which
are the energy differences between occupied and unoccupied electronic
states. This tunability enables the creation of materials that can
absorb and emit light across a wide range of the electromagnetic
spectrum, making them suitable for applications like solar cells,
light-emitting diodes (LEDs), and lasers. Additionally, perovskite
nanocrystals exhibit high photoluminescence efficiencies, meaning they
can efficiently convert absorbed light into emitted light, further
adding to their potential for various optoelectronic applications.</p>
<p>Text 2: Graphene is a one-atom-thick sheet of carbon atoms arranged
in a honeycomb lattice. It is a remarkable material with unique
properties, including high electrical conductivity, thermal
conductivity, mechanical strength, and optical transparency. Graphene
has the potential to revolutionize various fields, including
electronics, photonics, and composite materials. Due to its excellent
electrical conductivity, graphene is a promising candidate for
next-generation electronic devices, such as transistors and sensors.
Additionally, its high thermal conductivity makes it suitable for heat
dissipation applications.</p>
<p>Topics and Keywords:</p>
<pre><code>Topic 0:

Text 1 Keywords: ['applications', 'devices', 'material', 'optoelectronic', 'properties']
Text 2 Keywords: ['applications', 'conductivity', 'electronic', 'graphene', 'material', 'potential']

Topic 1:
	
Text 1 Keywords: ['bandgaps', 'crystal', 'electronic', 'properties', 'structure']
Text 2 Keywords: ['conductivity', 'electrical', 'graphene', 'material', 'properties']

Topic 2:

Text 1 Keywords: ['absorption', 'emit', 'light', 'spectrum']
Text 2 Keywords: ['conductivity', 'graphene', 'material', 'optical', 'potential']</code></pre>
<p>Conceptual Similarity:</p>
<p>Both texts discuss novel materials (perovskite nanocrystals and
graphene) with unique properties. While the specific applications and
functionalities differ slightly (optoelectronic devices vs. electronic
devices), they both highlight the potential of these materials for
various technological advancements. Notably, both topics identify
“material,” “properties,” and “applications” as keywords, suggesting a
shared focus on the materials’ characteristics and their potential uses.
Additionally, keywords like “electronic,” “conductivity,” and
“potential” appear in both texts within different topics, indicating a
conceptual overlap in exploring the electronic properties and potential
applications of these materials.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="challenge-5" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-5" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-5"></a>
</h3>
<div class="callout-content">
<p>Q: Use the Gensim library to perform topic modeling on the following
text print the original text and the list of topics and their
keywords.</p>
</div>
</div>
</div>
<div id="accordionSpoiler9" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler9" aria-expanded="false" aria-controls="collapseSpoiler9">
  <h3 class="accordion-header" id="headingSpoiler9">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Text</h3>
</button>
<div id="collapseSpoiler9" class="accordion-collapse collapse" aria-labelledby="headingSpoiler9" data-bs-parent="#accordionSpoiler9">
<div class="accordion-body">
<p>text = “Natural language processing (NLP) is a subfield of computer
science, information engineering, and artificial intelligence concerned
with the interactions between computers and human (natural) languages,
in particular how to program computers to process and analyze large
amounts of natural language data. Challenges in natural language
processing frequently involve speech recognition, natural language
understanding, and natural language generation.”</p>
</div>
</div>
</div>
</div>
<div id="challenge-5" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-5" class="callout-inner">
<h3 class="callout-title">Challenge<em>(continued)</em><a class="anchor" aria-label="anchor" href="#challenge-5"></a>
</h3>
<div class="callout-content">
<p>You can use the following code to load the Gensim library and the
pre-trained model for topic modeling:</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> corpora</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> LdaModel</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6">Show me the solution</h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" aria-labelledby="headingSolution6" data-bs-parent="#accordionSolution6">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> corpora</span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> LdaModel</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>tokens <span class="op">=</span> simple_preprocess(text)</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>dictionary <span class="op">=</span> corpora.Dictionary([tokens])</span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>corpus <span class="op">=</span> [dictionary.doc2bow(tokens)]</span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a>model <span class="op">=</span> LdaModel(corpus, num_topics<span class="op">=</span><span class="dv">2</span>, id2word<span class="op">=</span>dictionary)</span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a><span class="bu">print</span>(text)</span>
<span id="cb21-12"><a href="#cb21-12" tabindex="-1"></a><span class="bu">print</span>(model.print_topics())</span></code></pre>
</div>
<pre><code>
output = Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.

[(0, '0.051*"natural" + 0.051*"language" + 0.051*"processing" + 0.027*"nlp" + 0.027*"challenges" + 0.027*"speech" + 0.027*"recognition" + 0.027*"understanding" + 0.027*"generation" + 0.027*"frequently"'), (1, '0.051*"natural" + 0.051*"language" + 0.051*"computers" + 0.027*"interactions" + 0.027*"between" + 0.027*"human" + 0.027*"languages" + 0.027*"particular" + 0.027*"program" + 0.027*"process"')]

</code></pre>
</div>
</div>
</div>
</div>
<div id="challenge-of-using-small-size-corpus" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="challenge-of-using-small-size-corpus" class="callout-inner">
<h3 class="callout-title">Challenge of using small size corpus<a class="anchor" aria-label="anchor" href="#challenge-of-using-small-size-corpus"></a>
</h3>
<div class="callout-content">
<p>The warning message “too few updates, training might not converge”)
arises when you are using a very small corpus for topic modeling with
Latent Dirichlet Allocation (LDA) in Gensim. LDA relies on statistical
analysis of documents to discover hidden topics. With a limited corpus
(one document in your case), there aren’t enough data points for the
model to learn robust topics. Increasing the number of documents (corpus
size) generally improves the accuracy and convergence of LDA models.</p>
</div>
</div>
</div>
</section><section id="text-summarization"><h2 class="section-heading">3.3. Text Summarization<a class="anchor" aria-label="anchor" href="#text-summarization"></a>
</h2>
<hr class="half-width">
<p>Text summarization in NLP is the process of creating a concise and
coherent version of a longer text document, preserving its key
information. There are two primary approaches to text summarization:</p>
<ol style="list-style-type: decimal">
<li>Extractive Summarization: This method involves identifying and
extracting key sentences or phrases directly from the original text to
form the summary. It is akin to creating a highlight reel of the most
important points.</li>
<li>Abstractive Summarization: This approach goes beyond mere
extraction; it involves understanding the main ideas and then generating
new, concise text that captures the essence of the original content. It
is similar to writing a synopsis or an abstract for a research
paper.</li>
</ol>
<p>In the next part of the workshop, we will explore advanced tools like
transformers, which can generate summaries that are more coherent and
closer to what a human might write. Transformers use models like BERT
and GPT to understand the context and semantics of the text, allowing
for more sophisticated abstractive summaries.</p>
<div id="challenge-6" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-6" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-6"></a>
</h3>
<div class="callout-content">
<p>Q: Fill in the blanks with the correct terms related to text
summarization:</p>
<ul>
<li>—— summarization selects sentences directly from the original text,
while —— summarization generates new sentences.</li>
<li>—— are advanced tools used for generating more coherent and
human-like summaries.</li>
<li>The —— and —— models are examples of transformers that understand
the context and semantics of the text.</li>
<li>—— summarization can often create summaries that are more —— and
coherent than —— methods.</li>
<li>Advanced summarization tools use —— and —— to interpret and condense
text.</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution7" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution7" aria-expanded="false" aria-controls="collapseSolution7">
  <h4 class="accordion-header" id="headingSolution7">Show me the solution</h4>
</button>
<div id="collapseSolution7" class="accordion-collapse collapse" aria-labelledby="headingSolution7" data-bs-parent="#accordionSolution7">
<div class="accordion-body">
<p>A:</p>
<ul>
<li><p>Extractive summarization selects sentences directly from the
original text, while abstractive summarization generates new
sentences.</p></li>
<li><p>Transformers are advanced tools used for generating more coherent
and human-like summaries.</p></li>
<li><p>The BERT and GPT models are examples of transformers that
understand the context and semantics of the text.</p></li>
<li><p>Abstractive summarization can often create summaries that are
more concise and coherent than extractive methods.</p></li>
<li><p>Advanced summarization tools use machine learning and natural
language processing to interpret and condense text.</p></li>
</ul>
</div>
</div>
</div>
</div>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout4"></a>
</h3>
<div class="callout-content">
<p>In the rapidly evolving field of NLP, summarization tasks are
increasingly being carried out using transformer-based models due to
their advanced capabilities in understanding context and generating
coherent summaries. Tools like Gensim’s summarization module</p>
<pre><code>from gensim.summarization import summarize</code></pre>
<p>have become outdated and were removed in its 4.0 release <a href="">source</a>, as they relied on extractive methods that simply
selected parts of the existing text, which is less effective compared to
the abstractive approach of transformers. These cutting-edge transformer
models, which can create concise and fluent summaries by generating new
sentences, are leading to the gradual disappearance of older, less
efficient summarization methods.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Named Entity Recognition (NER) is crucial for identifying and
categorizing key information in text, such as names of people,
organizations, and locations.</li>
<li>Topic Modeling helps uncover the underlying thematic structure in a
large corpus of text, which is beneficial for summarizing and
understanding large datasets.</li>
<li>Text Summarization provides a concise version of a longer text,
highlighting the main points, which is essential for quick comprehension
of extensive research material.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-04-word-embedding"><p>Content from <a href="04-word-embedding.html">Word Embedding</a></p>
<hr>
<p>Last updated on 2024-05-10 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/04-word-embedding.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 16 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a vector space in the context of NLP?</li>
<li>How can I visualize vector space in a 2D model?</li>
<li>How can I use embeddings and how do embeddings capture the meaning
of words?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Be able to explain vector space and how it is related to text
analysis.</li>
<li>Identify the tools required for text embeddings.</li>
<li>To explore the Word2Vec algorithm and its advantages over
traditional models.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/c96878f9-4c4a-49d7-9ec8-346d75663b76" alt="image" class="figure"><a href="https://carpentries-incubator.github.io/python-text-analysis/06-lsa/index.html" class="external-link">source</a></p>
<section id="introduction-to-vector-space-embeddings"><h2 class="section-heading">4.1. Introduction to Vector Space &amp; Embeddings:<a class="anchor" aria-label="anchor" href="#introduction-to-vector-space-embeddings"></a>
</h2>
<hr class="half-width">
<p>We have discussed how tokenization works and how it is important in
text analysis, however, this is not the whole story of preprocessing.
For conducting robust and reliable text analysis with NLP models,
vectorization and embedding are required after tokenization. To
understand this concept, we first talk about vector space.</p>
<p>Vector space models represent text data as vectors, which can be used
in various machine learning algorithms. Embeddings are dense vectors
that capture the semantic meanings of words based on their context.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/e22fea16-a593-4c75-988b-0ba81588f98b" alt="embedding_2" class="figure"><a href="https://www.deeplearning.ai/resources/natural-language-processing/" class="external-link">source</a></p>
<div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Discuss how tokenization affects the representation of text
in vector space models. Consider the impact of ignoring common words
(stop words) and the importance of word order.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p><img src="../fig/embedding_3.png" class="figure"><a href="https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037" class="external-link">source</a></p>
<p>A: Ignoring stop words might lead to loss of some contextual
information but can also reduce noise. Preserving word order can be
crucial for understanding the meaning, especially in languages with
flexible syntax.</p>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Tell me MORE!</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler1" aria-labelledby="headingSpoiler1">
<div class="accordion-body">
<p>Tokenization is a fundamental step in the processing of text for
vector space models. It involves breaking down a string of text into
individual units, or “tokens,” which typically represent words or
phrases. Here’s how tokenization impacts the representation of text in
vector space models:</p>
<ul>
<li>
<em>Granularity</em>: Tokenization determines the granularity of
text representation. Finer granularity (e.g., splitting on punctuation)
can capture more nuances but may increase the dimensionality of the
vector space.</li>
<li>
<em>Dimensionality</em>: Each unique token becomes a dimension in
the vector space. The choice of tokenization can significantly affect
the number of dimensions, with potential implications for computational
efficiency and the “curse of dimensionality.”</li>
<li>
<em>Semantic Meaning</em>: Proper tokenization ensures that
semantically significant units are captured as tokens, which is crucial
for the model to understand the meaning of the text.</li>
</ul>
<p>Ignoring common words, or “stop words,” can also have a significant
impact:</p>
<p><em>Noise Reduction</em>: Stop words are often filtered out to reduce
noise since they usually don’t carry important meaning and are highly
frequent (e.g., “the,” “is,” “at”).</p>
<p><em>Focus on Content Words</em>: By removing stop words, the model
can focus on content words that carry the core semantic meaning,
potentially improving the performance of tasks like information
retrieval or topic modeling.</p>
<p><em>Computational Efficiency</em>: Ignoring stop words reduces the
dimensionality of the vector space, which can make computations more
efficient.</p>
<p>The importance of word order is another critical aspect:</p>
<p><em>Contextual Meaning</em>: Word order is essential for capturing
the syntactic structure and meaning of a sentence. Traditional
bag-of-words models ignore word order, which can lead to a loss of
contextual meaning.</p>
<p><em>Phrase Identification</em>: Preserving word order allows for the
identification of multi-word expressions and phrases that have distinct
meanings from their constituent words.</p>
<p><em>Word Embeddings</em>: Advanced models like word embeddings (e.g.,
Word2Vec) and contextual embeddings (e.g., BERT) can capture word order
to some extent, leading to a more nuanced understanding of text
semantics.</p>
<p>In summary, tokenization, the treatment of stop words, and the
consideration of word order are all crucial factors that influence how
text is represented in vector space models, affecting both the quality
of the representation and the performance of downstream tasks.</p>
</div>
</div>
</div>
</div>
<div id="tokenization-vs.-vectorization-vs.-embedding" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="tokenization-vs.-vectorization-vs.-embedding" class="callout-inner">
<h3 class="callout-title">Tokenization Vs. Vectorization Vs.
Embedding<a class="anchor" aria-label="anchor" href="#tokenization-vs.-vectorization-vs.-embedding"></a>
</h3>
<div class="callout-content">
<p>Initially, <strong>tokenization</strong> breaks down text into
discrete elements, or tokens, which can include words, phrases, symbols,
and even punctuation, each represented by a unique numerical identifier.
These tokens are then mapped to <strong>vectors</strong> of real numbers
within an n-dimensional space, a process that is part of
<strong>embedding</strong>. During model training, these vectors are
adjusted to reflect the semantic similarities between tokens,
positioning those with similar meanings closer together in the embedding
space. This allows the model to grasp the nuances of language and
transforms raw text into a format that machine learning algorithms can
interpret, paving the way for advanced text analysis and
understanding.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/5a669f97-1bce-4466-ba6b-2b5e792124f0" alt="image" class="figure"><a href="https://geoffrey-geofe.medium.com/tokenization-vs-embedding-understanding-the-differences-and-their-importance-in-nlp-b62718b5964a" class="external-link">source</a></p>
</div>
</div>
</div>
</section><section id="bag-of-words-tf-idf"><h2 class="section-heading">4.2. Bag of Words &amp; TF-IDF:<a class="anchor" aria-label="anchor" href="#bag-of-words-tf-idf"></a>
</h2>
<hr class="half-width">
<p>Feature extraction in machine learning involves creating numerical
features that describe a document’s relationship to its corpus.
Traditional methods like Bag-of-Words and TF-IDF count words or n-grams,
with the latter assigning weights based on a word’s importance,
calculated by Term Frequency (TF) and Inverse Document Frequency (IDF).
TF measures a word’s importance within a document, while IDF assesses
its rarity across the corpus.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/2b3b5f37-667b-4fe8-bc31-e6798b6e2b61" alt="image" class="figure"><a href="https://www.deeplearning.ai/resources/natural-language-processing/" class="external-link">source</a></p>
<p>The product of TF and IDF gives the TF-IDF score, which balances a
word’s frequency in a document against its commonness in the corpus.
This approach helps to highlight significant words while diminishing the
impact of commonly used words like “the” or “a.”</p>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div>Instructor Note</h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" data-bs-parent="#accordionInstructor1" aria-labelledby="headingInstructor1">
<div class="accordion-body">
<ul>
<li>
<strong>BoW</strong> “encodes the total number of times a document
uses each word in the associated corpus through the
CounterVectorizer.”</li>
<li>
<strong>TF-IDF</strong> “creates features for each document based on
how often each word shows up in a document versus the entire
corpus.</li>
<li><a href="https://www.deeplearning.ai/resources/natural-language-processing/" class="external-link">source</a></li>
</ul>
</div>
</div>
</div>
</div>
<div id="discussion-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Discuss how each method represents the importance of words
and the potential impact on sentiment analysis.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>A: To compare the Bag of Words (BoW) and Term Frequency-Inverse
Document Frequency (TF-IDF) methods in representing text data and their
implications for sentiment analysis.</p>
<p>Data Collection: Gather a corpus of product reviews. For this
activity, let’s assume we have a list of reviews stored in a variable
called reviews. Clean the text data by removing punctuation, converting
to lowercase, and possibly removing stop words. Use a vectorizer to
convert the reviews into a BoW representation.</p>
<p>Discuss how BoW represents the frequency of words without considering
the context or rarity across documents. Use a vectorizer to convert the
same reviews into a TF-IDF representation. Discuss how TF-IDF represents
the importance of words by considering both the term frequency and how
unique the word is across all documents.</p>
</div>
</div>
</div>
</div>
<div id="teamwork" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="teamwork" class="callout-inner">
<h3 class="callout-title">Teamwork<a class="anchor" aria-label="anchor" href="#teamwork"></a>
</h3>
<div class="callout-content">
<p>Sentiment Analysis Implications:</p>
<p>Analyze a corpus of product reviews using both BoW and TF-IDF.
Consider how the lack of context in BoW might affect sentiment analysis.
Evaluate whether TF-IDF’s emphasis on unique words improves the model’s
ability to understand sentiment.</p>
<p>Share Findings: Groups should present their findings, highlighting
the strengths and weaknesses of each method.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer, TfidfVectorizer</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># Sample corpus of product reviews</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>reviews <span class="op">=</span> [</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="st">"Great product, really loved it!"</span>,</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="st">"Bad quality, totally disappointed."</span>,</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="st">"Decent product for the price."</span>,</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="st">"Excellent quality, will buy again!"</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>]</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># Initialize the CountVectorizer for BoW</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>bow_vectorizer <span class="op">=</span> CountVectorizer(stop_words<span class="op">=</span><span class="st">'english'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co"># Fit and transform the reviews</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>bow_matrix <span class="op">=</span> bow_vectorizer.fit_transform(reviews)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="co"># Display the BoW matrix</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bag of Words Matrix:"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="bu">print</span>(bow_matrix.toarray())</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="co"># Initialize the TfidfVectorizer for TF-IDF</span></span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>tfidf_vectorizer <span class="op">=</span> TfidfVectorizer(stop_words<span class="op">=</span><span class="st">'english'</span>)</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a><span class="co"># Fit and transform the reviews</span></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>tfidf_matrix <span class="op">=</span> tfidf_vectorizer.fit_transform(reviews)</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="co"># Display the TF-IDF matrix</span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">TF-IDF Matrix:"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a><span class="bu">print</span>(tfidf_matrix.toarray())</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>The BoW matrix shows the frequency of each word in the reviews,
disregarding context and word importance. The TF-IDF matrix shows the
weighted importance of words, giving less weight to common words and
more to unique ones.</p>
<p>In sentiment analysis, BoW might misinterpret sentiments due to
ignoring context, while TF-IDF might capture nuances better by
emphasizing words that are significant in a particular review.</p>
<p>By comparing BoW and TF-IDF, participants can gain insights into how
each method processes text data and their potential impact on NLP tasks
like sentiment analysis. This activity encourages critical thinking
about feature representation in machine learning models.</p>
</section><section id="word2vec-algorithm"><h2 class="section-heading">4.3. Word2Vec Algorithm:<a class="anchor" aria-label="anchor" href="#word2vec-algorithm"></a>
</h2>
<hr class="half-width">
<p>More advanced techniques like Word2Vec and GLoVE, as well as feature
learning during neural network training, have also been developed to
improve feature extraction.</p>
<p>Word2Vec uses neural networks to learn word associations from large
text corpora. It has two architectures: Skip-Gram and Continuous
Bag-of-Words (CBOW).</p>
<p>After training, it discards the final layer and outputs word
embeddings that capture context. These embeddings capture the context of
words, making similar contexts yield similar embeddings. Post-data
preprocessing, these numerical features can be used in various NLP
models for tasks like classification or named entity recognition.</p>
<p>Now let’s see how this framework can be used in practice. First
import required libraries: Start by importing necessary libraries like
gensim for Word2Vec and nltk for tokenization. Next, prepare the data:
Tokenize your text data into words.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># Sample text</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Tokenization splits text into words. Embeddings capture semantic meaning."</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># Tokenize the text</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>tokens <span class="op">=</span> word_tokenize(text.lower())</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a></span></code></pre>
</div>
<p>Now train the model: Use the Word2Vec class from gensim to train your
model on the tokenized sentences.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># Sample text</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Tokenization splits text into words. Embeddings capture semantic meaning."</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co"># Tokenize the text</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>tokens <span class="op">=</span> word_tokenize(text.lower())</span></code></pre>
</div>
<p>Retrieve Vectors: After training, use the model to get vectors for
words of interest.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="co"># Display the vector</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="bu">print</span>(vector_embeddings)</span></code></pre>
</div>
<p>The code tokenizes the sample text, trains a Word2Vec model, and
retrieves the vector for the word ‘embeddings’.</p>
<p>The resulting vector is a 50-dimensional representation of
‘embeddings’, capturing its context within the sample text. This vector
can then be used in various NLP tasks to represent the semantic meaning
of the word ‘embeddings’.</p>
<p>By understanding the roles of tokenization and embedding, we can
better prepare text data for complex NLP tasks and build models that
more accurately interpret human language.</p>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What is GLoVE?</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler2" aria-labelledby="headingSpoiler2">
<div class="accordion-body">
<p>Global Vectors for Word Representation (GLoVE)</p>
<p>GLoVE is a model for learning word embeddings, which are
representations of words in the form of high-dimensional vectors. Unlike
Word2Vec, which uses a neural network to learn word embeddings from
local context information, GLoVE is designed to capture both global
statistics and local context. Here’s how GLoVE stands out:</p>
<ul>
<li>Matrix Factorization: GLoVE uses matrix factorization on a word
co-occurrence matrix that reflects how often each word appears in the
context of every other word within a large corpus.</li>
<li>Global Word-Word Co-Occurrence: It focuses on word-to-word
co-occurrence globally across the entire corpus, rather than just within
a local context window as in Word2Vec.</li>
<li>Weighting Function: GLoVE employs a weighting function that helps to
address the disparity in word co-occurrence frequencies, giving less
weight to rare and frequent co-occurrences.</li>
</ul>
<p>The main difference between GLoVE and Word2Vec is that GLoVE is built
on the idea that word meanings can be derived from their co-occurrence
probabilities with other words, and hence it incorporates global corpus
statistics, whereas Word2Vec relies more on local context information.
This allows GLoVE to effectively capture both the semantic and syntactic
relationships between words, making it powerful for various natural
language processing tasks.</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Tokenization is crucial for converting text into a format usable by
machine learning models.</li>
<li>BoW and TF-IDF are fundamental techniques for feature extraction in
NLP.</li>
<li>Word2Vec and GloVE generate embeddings that encapsulate word
meanings based on context and co-occurrence, respectively.</li>
<li>Understanding these concepts is essential for building effective NLP
models that can interpret and process human language.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-05-transformers"><p>Content from <a href="05-transformers.html">Transformers for Natural Language Processing</a></p>
<hr>
<p>Last updated on 2024-05-13 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/05-transformers.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do Transformers work?</li>
<li>How can I use Transformers for text analysis?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>To be able to describe Transformers’ architecture.</li>
<li>To be able to implement sentiment analysis, and text summarization
using transformers.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Transformers have revolutionized the field of NLP since their
introduction by the Google team in 2017. Unlike previous models that
processed text sequentially, Transformers use an attention mechanism to
process all words at once, allowing them to capture context more
effectively. This parallel processing capability enables Transformers to
handle long-range dependencies and understand the nuances of language
better than their predecessors. For now, try to recognize the building
blocks of the general structure of a transformer</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/7ec8bb01-aa7e-4378-af45-ae28fbe8916b" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><section id="introduction-to-artificial-neural-networks"><h2 class="section-heading">5.1. Introduction to Artificial Neural Networks<a class="anchor" aria-label="anchor" href="#introduction-to-artificial-neural-networks"></a>
</h2>
<hr class="half-width">
<p>To understand how Transformers work we also need to learn about
artificial neural networks (ANNs). Imagine a neural network as a team of
workers in a factory. Each worker (neuron) has a specific task
(processing information), and they pass their work along to the next
person in line until the final product (output) is created.</p>
<p>Just like a well-organized assembly line, a neural network processes
information in stages, with each neuron contributing to the final
result.</p>
<div id="activity" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Take a look at the architecture of a simple ANN below.
Identify the underlying layers and components of this ANN and add the
correct name label to each one.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/445b963e-caf1-451d-8edc-e686f8950ae5" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/523924b5-055b-4125-8bce-aa2be2b38ca8" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What is Multilayer Perceptron then?</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler1" aria-labelledby="headingSpoiler1">
<div class="accordion-body">
<p>In the context of machine learning, a multilayer perceptron (MLP) is
indeed a fully connected multi-layer neural network and is a classic
example of a feedforward artificial neural network (ANN). It typically
includes an input layer, one or more hidden layers, and an output layer.
When an MLP has more than one hidden layer, it can be considered a deep
ANN, part of a broader category known as deep learning.</p>
</div>
</div>
</div>
</div>
<div id="summation-and-activation-function" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="summation-and-activation-function" class="callout-inner">
<h3 class="callout-title">Summation and Activation Function<a class="anchor" aria-label="anchor" href="#summation-and-activation-function"></a>
</h3>
<div class="callout-content">
<p>If we zoom into a neuron in the hidden layer, we can see the
mathematical operations (weights summation and activation function). An
input is transformed at each hidden layer node through a process that
multiplies the input (x_i) by learned weights (w_i), adds a bias (b),
and then applies an activation function to determine the node’s output.
This output is either passed on to the next layer or contributes to the
final output of the network. Essentially, each node performs a small
calculation that, when combined with the operations of other nodes,
allows the network to process complex patterns and data.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/d1006506-ac54-43bc-b6e9-5181ca98be36" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What happens next? How to optimize an ANN?</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler2" aria-labelledby="headingSpoiler2">
<div class="accordion-body">
<p>Backpropagation is an algorithmic cornerstone in the training of
ANNs, serving as a method for optimizing weights and biases through
gradient descent. Conceptually, it is akin to an iterative refinement
process where the network’s output error is propagated backward, layer
by layer, using the chain rule of calculus. This backward flow of error
information allows for the computation of gradients, which inform the
magnitude and direction of adjustments to be made to the network’s
parameters. The objective is to iteratively reduce the differences
between the predicted output and the actual target values. This
systematic adjustment of parameters, guided by error gradients,
incrementally leads to a more accurate ANN model.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/5ae4d845-c3d2-42df-87f0-e928be9ba64b" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
</div>
<div id="challenge" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge"></a>
</h3>
<div class="callout-content">
<p>Teamwork: When we talk about ANNs, we also consider their parameters.
But what are the parameters? Draw a small neural network with 3
following layers: x1</p>
<ul>
<li>Input Layer: 3 neurons</li>
<li>Hidden Layer: 4 neurons</li>
<li>Output Layer: 1 neurons</li>
</ul>
<ol style="list-style-type: decimal">
<li>Connect each neuron in the input layer to every neuron in the hidden
layer (next layer). How many connections (weights) do we have?</li>
<li>Now, add a bias for each neuron in the hidden layer. How many biases
do we have?</li>
<li>Repeat the process for the hidden layer to the output layer.</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/144a8f7d-c1ac-4b57-8688-b5280cabd59c" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><ul>
<li>(3 { neurons} x 4 { neurons} + 4{ biases}) = 16</li>
<li>(4 { neurons} x 1 { neurons} + 1{ biases}) = 5</li>
<li>Total parameters for this network: (16 + 5 = 21)</li>
</ul>
</div>
</div>
</div>
</div>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-1"></a>
</h3>
<div class="callout-content">
<p>Q: Add another hidden layer with 4 neurons to the previous ANN and
calculate the number of parameters.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/39820eb0-7959-4ed1-bdfc-69131ab1a834" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>We would add: - (4 * 4) weights from the first to the second hidden
layer - (4) biases for the new hidden layer - (4 * 1) weights from the
second hidden layer to the output layer (we already counted the biases
for the output layer)</p>
<p>That’s an additional (16 + 4 = 20) parameters, bringing our total to
(21 + 20 = 41) parameters.</p>
</div>
</div>
</div>
</div>
</section><section id="transformers"><h2 class="section-heading">5.2. Transformers<a class="anchor" aria-label="anchor" href="#transformers"></a>
</h2>
<hr class="half-width">
<p>As mentioned in the introduction, Most of the recent NLP models are
built based on Transformers. Building on our understanding of ANNs,
let’s explore the architecture of transformers. Transformers consist of
several key components that work together to process and generate
data.</p>
<div id="activity-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity-1" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: We go back to the first figure of this episode. In the
simplified schematic below, write the function of each component in the
allocated textbox:</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/e94c677a-53c5-4da3-87ac-e45960429986" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<p>A: <img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/25823a7c-6059-4c83-b592-a273fa59b9ec" alt="image" class="figure"></p>
<p>Briefly, we can say:</p>
<ul>
<li>Encoder: Processes input text into contextualized representations,
enabling the understanding of the context within the input sequence. It
is like the ‘listener’ in a conversation, taking in information and
understanding it.</li>
<li>Decoder: Generates output sequences by translating the
contextualized representations from the encoder into coherent text,
often using mechanisms like masked multi-head attention and
encoder-decoder attention to maintain sequence order and coherence. This
acts as the ‘speaker’ in the conversation, generating the output based
on the information processed by the encoder.</li>
<li>Positional Encoding: Adds unique information to each word embedding,
indicating the word’s position in the sequence, which is essential for
the model to maintain the order of words and understand their relative
positions within a sentence</li>
<li>Input Embedding: The input text is converted into vectors that the
model can understand. Think of it as translating words into a secret
code that the transformer can read.</li>
<li>Output Embedding: Similar to input embedding, but for the output
text. It translates the transformer’s secret code back into words we can
understand.</li>
<li>Softmax Output: Applies the softmax function to the final layer’s
outputs to convert them into a probability distribution, which helps in
tasks like classification and sequence generation by selecting the most
likely next word or class. It is like choosing the best response in a
conversation from many options.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="attention-mechanism" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="attention-mechanism" class="callout-inner">
<h3 class="callout-title">Attention Mechanism<a class="anchor" aria-label="anchor" href="#attention-mechanism"></a>
</h3>
<div class="callout-content">
<p>So far, we have learned what the architecture of a transformer block
looks like. However, for simplicity, many parts of this architecture
have not been considered.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/559580aa-f2c9-4ec0-b87d-7e1438839431" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>In the following section, we will show the underlying components of a
transformer.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/2b325dc1-20d8-4bac-91b8-030067ee8097" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>For more details see <a href="https://arxiv.org/abs/1706.03762" class="external-link">source</a>.</p>
<p>Attention mechanisms in transformers, allow LLMs to focus on
different parts of the input text to understand context and
relationships between words. The concept of ‘attention’ in encoders and
decoders is akin to the selective focus of ‘fast reading,’ where one
zeroes in on crucial information and disregards the irrelevant. This
mechanism adapts to the context of a query, emphasizing different words
or tokens based on the query’s intent. For instance, in the sentence
“Sarah went to a restaurant to meet her friend that night,” the words
highlighted would vary depending on whether the question is about the
action (What?), location (Where?), individuals involved (Who?), or time
(When?).</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/f28a1a04-40f7-4ad3-a846-4f0d4c4ddee4" alt="image" class="figure"><a href="https://medium.com/@hunter-j-phillips/multi-head-attention-7924371d477a" class="external-link">source</a></p>
<p>In transformer models, this selective focus is achieved through
‘queries,’ ‘keys,’ and ‘values,’ all represented as vectors. A query
vector seeks out the closest key vectors, which are encoded
representations of values. The relationship between words, like ‘where’
and ‘restaurant,’ is determined by their frequency of co-occurrence in
sentences, allowing the model to assign greater attention to
‘restaurant’ when the query pertains to a location. This dynamic
adjustment of focus enables transformers to process language with a
nuanced understanding of context and relevance.</p>
</div>
</div>
</div>
<div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Have you heard of any other applications of the
Transformers rather than in NLPs? Explain why transformers can be useful
for other AI applications. Share your thoughts and findings with other
groups.</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">Show me the solution</h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" data-bs-parent="#accordionSolution5" aria-labelledby="headingSolution5">
<div class="accordion-body">
<p>A: Transformers, initially popular in NLP, have found applications
beyond text analysis. They excel in computer vision, speech recognition,
and even genomics. Their versatility extends to music generation and
recommendation systems. Transformers’ innovative architecture allows
them to adapt to diverse tasks, revolutionizing AI applications.</p>
</div>
</div>
</div>
</div>
<div id="transformers-in-text-translation" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="transformers-in-text-translation" class="callout-inner">
<h3 class="callout-title">Transformers in Text Translation<a class="anchor" aria-label="anchor" href="#transformers-in-text-translation"></a>
</h3>
<div class="callout-content">
<p>Imagine you want to translate the sentence “What time is it?” from
English to German using a transformer. The input embedding layer
converts each English word into a vector. The six layers of encoders
process these vectors, understanding the context of the sentence. The
six layers of decoders then start generating the German translation, one
word at a time.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/09e7ae0f-bfc1-4d7c-be5f-443b97c05bd3" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>For each word, the Softmax output predicts the most likely next word
in German. The output embedding layer converts these predictions back
into readable German words. By the end, you get the German translation
of <strong>“What time is it?”</strong> as <strong>“Wie spät ist
es?”</strong></p>
</div>
</div>
</div>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What are other sequential learning models?</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler3" aria-labelledby="headingSpoiler3">
<div class="accordion-body">
<p>Transformers are essential for NLP tasks because they overcome the
limitations of earlier models like recurrent neural networks (RNNs) and
long short-term memory models (LSTMs), which struggled with long
sequences and were computationally intensive respectively. Transformers,
in contrast to the sequential input processing of RNNs, handle entire
sequences simultaneously. This parallel processing capability enables
data scientists to employ GPUs to train large language models (LLMs)
based on transformers, which markedly decreases the duration of
training.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/a0a429e4-c87e-4529-9151-781dd566c800" alt="rnn-transf-nlp" class="figure"><a href="https://thegradient.pub/transformers-are-graph-neural-networks/" class="external-link">source</a></p>
</div>
</div>
</div>
</div>
</section><section id="sentiment-analysis"><h2 class="section-heading">5.3. Sentiment Analysis<a class="anchor" aria-label="anchor" href="#sentiment-analysis"></a>
</h2>
<hr class="half-width">
<p>Sentiment analysis is a powerful tool in NLP that helps determine the
emotional tone behind the text. It is used to understand opinions,
sentiments, emotions, and attitudes from various entities and classify
them according to their polarity.</p>
<div id="activity-2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="activity-2" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-2"></a>
</h3>
<div class="callout-content">
<p>Teamwork: How do you categorize the following text in terms of
positive and negative sounding? Select an Emoji.</p>
<p><em>“A research team has unveiled a novel ligand exchange technique
that enables the synthesis of organic cation-based perovskite quantum
dots (PQDs), ensuring exceptional stability while suppressing internal
defects in the photoactive layer of solar cells.”</em> <a href="https://www.sciencedaily.com/releases/2024/02/240221160400.htm" class="external-link">source</a></p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/897d0938-d84f-4189-afac-b8f244e16b46" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<p>Computer models can do this job for us! Let’s see how it works
through a step-by-step example: First, install the required libraries
and pipelines:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>pip install transformers</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code></pre>
</div>
<p>Now, initialize the sentiment analysis pipeline and analyze the
sentiment of a sample text:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>sentiment_pipeline <span class="op">=</span> pipeline(<span class="st">'sentiment-analysis'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>text <span class="op">=</span> <span class="st">" A research team has unveiled a novel ligand exchange technique that enables the synthesis of organic cation-based perovskite quantum dots (PQDs), ensuring exceptional stability while suppressing internal defects in the photoactive layer of solar cells."</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>sentiment <span class="op">=</span> sentiment_pipeline(text)</span></code></pre>
</div>
<p>After the analysis is completed, you can print out the results:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sentiment: </span><span class="sc">{</span>sentiment[<span class="dv">0</span>][<span class="st">'label'</span>]<span class="sc">}</span><span class="ss">, Confidence: </span><span class="sc">{</span>sentiment[<span class="dv">0</span>][<span class="st">'score'</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>Output: Sentiment: POSITIVE, Confidence: <span class="fl">1.00</span></span></code></pre>
</div>
<p>In this example, the sentiment analysis pipeline from the Hugging
Face library is used to analyze the sentiment of a research paper
abstract. The model predicts the sentiment as positive, negative, or
neutral, along with a confidence score. This can be particularly useful
for gauging the reception of research papers in a field.</p>
<div id="activity-3" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="activity-3" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-3"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Fill in the blanks to complete the sentiment analysis
process: Install the __________ library for sentiment analysis. Use the
__________ function to create a sentiment analysis pipeline. The
sentiment analysis model will output a __________ and a __________
score.</p>
</div>
</div>
</div>
<div id="vadrer" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="vadrer" class="callout-inner">
<h3 class="callout-title">VADRER<a class="anchor" aria-label="anchor" href="#vadrer"></a>
</h3>
<div class="callout-content">
<p>Valence Aware Dictionary and sEntiment Reasoner (VADER) is a lexicon
and rule-based sentiment analysis tool that is particularly attuned to
sentiments expressed in social media. VADER analyzes the sentiment of
the text and returns a dictionary with scores for negative, neutral,
positive, and a compound score that aggregates them. It is useful for
quick sentiment analysis, especially on social media texts. Let’s how we
can use this framework.</p>
<p>First, we need to import the SentimentIntensityAnalyzer module from
VADER library:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="im">from</span> vaderSentiment.vaderSentiment <span class="im">import</span> SentimentIntensityAnalyzer</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co"># Initialize VADER sentiment intensity analyzer:</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>analyzer <span class="op">=</span> SentimentIntensityAnalyzer()</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co"># We use the same sample text:</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>text <span class="op">=</span> <span class="st">" A research team has unveiled a novel ligand exchange technique that enables the synthesis of organic cation-based perovskite quantum dots (PQDs), ensuring exceptional stability while suppressing internal defects in the photoactive layer of solar cells."</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="co"># Now we can analyze sentiment:</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>vader_sentiment <span class="op">=</span> analyzer.polarity_scores(text)</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a><span class="co"># Print the sentiment:</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sentiment: </span><span class="sc">{</span>vader_sentiment<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>Output: Sentiment: {<span class="st">'neg'</span>: <span class="fl">0.069</span>, <span class="st">'neu'</span>: <span class="fl">0.818</span>, <span class="st">'pos'</span>: <span class="fl">0.113</span>, <span class="st">'compound'</span>: <span class="fl">0.1779</span>}</span></code></pre>
</div>
</div>
</div>
</div>
<div id="discussion-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Which framework do you think could be more helpful for
research applications? Elaborate your opinion. Share your thoughts with
other team members.</p>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6">Show me the solution</h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" data-bs-parent="#accordionSolution6" aria-labelledby="headingSolution6">
<div class="accordion-body">
<p>A: Transformers use deep learning models that can understand context
and nuances of language, making them suitable for complex and lengthy
texts. They can be particularly useful for sentiment analysis of
research papers, as they can understand the complex language and context
often found in academic writing. This allows for a more nuanced
understanding of the sentiment conveyed in the papers. VADER, on the
other hand, is a rule-based model that excels in analyzing short texts
with clear sentiment expressions, often found in social media.</p>
</div>
</div>
</div>
</div>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-2"></a>
</h3>
<div class="callout-content">
<p>Use the transformers library to perform sentiment analysis on the
following text:</p>
<p><em>“Perovskite nanocrystals have emerged as a promising class of
materials for next-generation optoelectronic devices due to their unique
properties. Their crystal structure allows for tunable bandgaps, which
are the energy differences between occupied and unoccupied electronic
states. This tunability enables the creation of materials that can
absorb and emit light across a wide range of the electromagnetic
spectrum, making them suitable for applications like solar cells,
light-emitting diodes (LEDs), and lasers.”</em></p>
<p>Print the original text and the sentiment score and label. You can
use the following code to load the transformers library and the
pre-trained model and tokenizer for sentiment analysis:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>sentiment_analysis <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution7" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution7" aria-expanded="false" aria-controls="collapseSolution7">
  <h4 class="accordion-header" id="headingSolution7">Show me the solution</h4>
</button>
<div id="collapseSolution7" class="accordion-collapse collapse" data-bs-parent="#accordionSolution7" aria-labelledby="headingSolution7">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>sentiment_analysis <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"This book is amazing. It is well-written, engaging, and informative. I learned a lot from reading it and I highly recommend it to anyone interested in natural language processing."</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="bu">print</span>(text)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="bu">print</span>(sentiment_analysis(text))</span></code></pre>
</div>
<p>Output:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>output: <span class="st">"Perovskite nanocrystals have emerged as a promising class of materials for next-generation optoelectronic devices due to their unique properties. Their crystal structure allows for tunable bandgaps, which are the energy differences between occupied and unoccupied electronic states. This tunability enables the creation of materials that can absorb and emit light across a wide range of the electromagnetic spectrum, making them suitable for applications like solar cells, light-emitting diodes (LEDs), and lasers."</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>[{<span class="st">'label'</span>: <span class="st">'POSITIVE'</span>, <span class="st">'score'</span>: <span class="fl">0.9998656511306763</span>}]</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-3"></a>
</h3>
<div class="callout-content">
<p>Comparing Transformer with VADER on a large size text. Use the
Huggingface library database.</p>
</div>
</div>
</div>
<div id="accordionSolution8" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution8" aria-expanded="false" aria-controls="collapseSolution8">
  <h4 class="accordion-header" id="headingSolution8">Show me the solution</h4>
</button>
<div id="collapseSolution8" class="accordion-collapse collapse" data-bs-parent="#accordionSolution8" aria-labelledby="headingSolution8">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</section><section id="text-summarization"><h2 class="section-heading">5.4. Text Summarization<a class="anchor" aria-label="anchor" href="#text-summarization"></a>
</h2>
<hr class="half-width">
<p>Text summarization is the process of distilling the most important
information from a source (or sources) to produce an abbreviated version
for a particular user and task. It can be broadly classified into two
types: extractive and abstractive summarization.</p>
<div id="discussion-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion-2" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-2"></a>
</h3>
<div class="callout-content">
<p>How extractive and abstractive summarization methods are different?
Connect the following text boxes to the correct category. Share your
results with other group members.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/1630ffcf-90c8-49df-9abe-98a739fd58ef" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<div id="accordionSolution9" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution9" aria-expanded="false" aria-controls="collapseSolution9">
  <h4 class="accordion-header" id="headingSolution9">Show me the solution</h4>
</button>
<div id="collapseSolution9" class="accordion-collapse collapse" data-bs-parent="#accordionSolution9" aria-labelledby="headingSolution9">
<div class="accordion-body">
<p>A: <img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/166c2025-14b0-4f1b-aee5-3f46d4f3c8b4" alt="image" class="figure"></p>
</div>
</div>
</div>
</div>
<p>Now, let’s see how to use the Hugging Face Transformers library to
perform abstractive summarization. First, from the transformers import
pipeline:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="co"># Initialize the summarization pipeline</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span></code></pre>
</div>
<p>Input a sample text from an article from <a href="https://www.sciencedaily.com/releases/2024/02/240221160400.htm#:~:text=Summary%3A,photoactive%20layer%20of%20solar%20cells." class="external-link">source</a>:</p>
<div id="accordionSpoiler4" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler4" aria-expanded="false" aria-controls="collapseSpoiler4">
  <h3 class="accordion-header" id="headingSpoiler4">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Input Text</h3>
</button>
<div id="collapseSpoiler4" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler4" aria-labelledby="headingSpoiler4">
<div class="accordion-body">
<p>text = “A groundbreaking research breakthrough in solar energy has
propelled the development of the world’s most efficient quantum dot (QD)
solar cell, marking a significant leap towards the commercialization of
next-generation solar cells. This cutting-edge QD solution and device
have demonstrated exceptional performance, retaining their efficiency
even after long-term storage. Led by Professor Sung-Yeon Jang from the
School of Energy and Chemical Engineering at UNIST, a team of
researchers has unveiled a novel ligand exchange technique. This
innovative approach enables the synthesis of organic cation-based
perovskite quantum dots (PQDs), ensuring exceptional stability while
suppressing internal defects in the photoactive layer of solar cells.
Our developed technology has achieved an impressive 18.1% efficiency in
QD solar cells,” stated Professor Jang. This remarkable achievement
represents the highest efficiency among quantum dot solar cells
recognized by the National Renewable Energy Laboratory (NREL) in the
United States. The increasing interest in related fields is evident, as
last year, three scientists who discovered and developed QDs, as
advanced nanotechnology products, were awarded the Nobel Prize in
Chemistry. QDs are semiconducting nanocrystals with typical dimensions
ranging from several to tens of nanometers, capable of controlling
photoelectric properties based on their particle size. PQDs, in
particular, have garnered significant attention from researchers due to
their outstanding photoelectric properties. Furthermore, their
manufacturing process involves simple spraying or application to a
solvent, eliminating the need for the growth process on substrates. This
streamlined approach allows for high-quality production in various
manufacturing environments. However, the practical use of QDs as solar
cells necessitates a technology that reduces the distance between QDs
through ligand exchange, a process that binds a large molecule, such as
a ligand receptor, to the surface of a QD. Organic PQDs face notable
challenges, including defects in their crystals and surfaces during the
substitution process. As a result, inorganic PQDs with limited
efficiency of up to 16% have been predominantly utilized as materials
for solar cells. In this study, the research team employed an alkyl
ammonium iodide-based ligand exchange strategy, effectively substituting
ligands for organic PQDs with excellent solar utilization. This
breakthrough enables the creation of a photoactive layer of QDs for
solar cells with high substitution efficiency and controlled defects.
Consequently, the efficiency of organic PQDs, previously limited to 13%
using existing ligand substitution technology, has been significantly
improved to 18.1%. Moreover, these solar cells demonstrate exceptional
stability, maintaining their performance even after long-term storage
for over two years. The newly-developed organic PQD solar cells exhibit
both high efficiency and stability simultaneously. Previous research on
QD solar cells predominantly employed inorganic PQDs,” remarked Sang-Hak
Lee, the first author of the study. Through this study, we have
demonstrated the potential by addressing the challenges associated with
organic PQDs, which have proven difficult to utilize. This study
presents a new direction for the ligand exchange method in organic PQDs,
serving as a catalyst to revolutionize the field of QD solar cell
material research in the future,” commented Professor Jang. The findings
of this study, co-authored by Dr. Javid Aqoma Khoiruddin and Sang-Hak
Lee, have been published online in Nature Energy on January 27, 2024.
The research was made possible through the support of the ‘Basic
Research Laboratory (BRL)’ and ‘Mid-Career Researcher Program,’ as well
as the ‘Nano·Material Technology Development Program,’ funded by the
National Research Foundation of Korea (NRF) under the Ministry of
Science and ICT (MSIT). It has also received support through the ’Global
Basic Research Lab Project.”</p>
</div>
</div>
</div>
</div>
<p>Now we can perform summarization and print the results:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>summary <span class="op">=</span> summarizer(text, max_length<span class="op">=</span><span class="dv">130</span>, min_length<span class="op">=</span><span class="dv">30</span>, do_sample<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="co"># Print the summary:</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Summary:"</span>, summary[<span class="dv">0</span>][<span class="st">'summary_text'</span>])</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>Output: </span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span></code></pre>
</div>
<div id="sumy-for-summarization" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="sumy-for-summarization" class="callout-inner">
<h3 class="callout-title">Sumy for summarization<a class="anchor" aria-label="anchor" href="#sumy-for-summarization"></a>
</h3>
<div class="callout-content">
<p>Sumy is a Python library for extractive summarization. It uses
algorithms like LSA to rank sentences based on their importance and
creates a summary by selecting the top-ranked sentences. We can see how
it works in practice: We start with importing the PlaintextParser and
LsaSummarizer modules:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="im">from</span> sumy.parsers.plaintext <span class="im">import</span> PlaintextParser</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="im">from</span> sumy.nlp.tokenizers <span class="im">import</span> Tokenizer</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="im">from</span> sumy.summarizers.lsa <span class="im">import</span> LsaSummarizer</span></code></pre>
</div>
<p>To create a parser we use the same text sample from an article from
<a href="https://www.sciencedaily.com/releases/2024/02/240221160400.htm#:~:text=Summary%3A,photoactive%20layer%20of%20solar%20cells." class="external-link">source</a>:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>parser <span class="op">=</span> PlaintextParser.from_string(text, Tokenizer(<span class="st">"english"</span>))</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="co"># Next, we initialize the LSA summarize:</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>summarizer <span class="op">=</span> LsaSummarizer()</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="co"># Summarize the text and print the results</span></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>summary <span class="op">=</span> summarizer(parser.document, <span class="dv">5</span>)</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a><span class="cf">for</span> sentence <span class="kw">in</span> summary:</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>    <span class="bu">print</span>(sentence)</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>Output:</span></code></pre>
</div>
<p>Sumy extracts key sentences from the original text, which can be
quicker but may lack the cohesiveness of an abstractive summary. On the
other hand, Transformer is suitable for generating a new summary that
captures the text’s essence in a coherent and often more readable
form.</p>
</div>
</div>
</div>
<div id="activity-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity-4" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-4"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Which framework could be more useful for text
summarizations in your field of research? Explain why?</p>
</div>
</div>
</div>
<div id="accordionSolution10" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution10" aria-expanded="false" aria-controls="collapseSolution10">
  <h4 class="accordion-header" id="headingSolution10">Show me the solution</h4>
</button>
<div id="collapseSolution10" class="accordion-collapse collapse" data-bs-parent="#accordionSolution10" aria-labelledby="headingSolution10">
<div class="accordion-body">
<p>A: Transformers are particularly useful for summarizing research
papers and documents where understanding the context and generating a
coherent summary is crucial. They can produce summaries that are not
only concise but also maintain the narrative flow, making them more
readable. Sumy, while quicker and less resource-intensive, is best
suited for scenarios where extracting key information without the need
for narrative flow is acceptable.</p>
</div>
</div>
</div>
</div>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-4"></a>
</h3>
<div class="callout-content">
<p>Use the transformers library to perform text summarization on the
following text [generated by Copilot]:</p>
</div>
</div>
</div>
<div id="accordionSpoiler5" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler5" aria-expanded="false" aria-controls="collapseSpoiler5">
  <h3 class="accordion-header" id="headingSpoiler5">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Input Text</h3>
</button>
<div id="collapseSpoiler5" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler5" aria-labelledby="headingSpoiler5">
<div class="accordion-body">
<p>text: “Perovskite nanocrystals are a class of semiconductor
nanocrystals that have attracted a lot of attention in recent years due
to their unique optical and electronic properties. Perovskite
nanocrystals have an ABX3 composition, where A is a monovalent cation
(such as cesium, methylammonium, or formamidinium), B is a divalent
metal (such as lead or tin), and X is a halide (such as chloride,
bromide, or iodide). Perovskite nanocrystals can emit brightly across
the entire visible spectrum, with tunable colors depending on their
composition and size. They also have high quantum yields, fast radiative
decay rates, and narrow emission line widths, making them ideal
candidates for various optoelectronic applications. The first report of
perovskite nanocrystals was published in 2014 by Protesescu et al., who
synthesized cesium lead halide nanocrystals using a hot-injection
method. They demonstrated that the nanocrystals had cubic or
orthorhombic crystal structures, depending on the halide ratio, and that
they exhibited strong photoluminescence with quantum yields up to 90%.
They also showed that the emission wavelength could be tuned from 410 nm
to 700 nm by changing the halide composition or the nanocrystal size.
Since then, many other groups have developed various synthetic methods
and strategies to control the shape, size, composition, and surface
chemistry of perovskite nanocrystals. One of the remarkable features of
perovskite nanocrystals is their defect tolerance, which means that they
can maintain high luminescence even with a high density of surface or
bulk defects. This is in contrast to other semiconductor nanocrystals,
such as CdSe, which require surface passivation to prevent non-radiative
recombination and quenching of the emission. The defect tolerance of
perovskite nanocrystals is attributed to their electronic band
structure, which has a large density of states near the band edges and a
small effective mass of the charge carriers. These factors reduce the
formation energy and the localization of defects and enhance the
radiative recombination rate of the excitons. Another interesting aspect
of perovskite nanocrystals is their weak quantum confinement, which
means that their emission properties are not strongly affected by their
size. This is because the exciton binding energy of perovskite
nanocrystals is much larger than the quantum confinement energy, and
thus the excitons are localized within a few unit cells regardless of
the nanocrystal size. As a result, perovskite nanocrystals can exhibit
narrow emission line widths even with a large size distribution, which
simplifies the synthesis and purification processes. Moreover,
perovskite nanocrystals can show dual emission from both the band edge
and the surface states, which can be exploited for color tuning and
white light generation. Perovskite nanocrystals have been applied to a
wide range of photonic devices, such as light-emitting diodes, lasers,
solar cells, photodetectors, and scintillators. Perovskite nanocrystals
can offer high brightness, color purity, and stability as light
emitters, and can be integrated with various substrates and
architectures. Perovskite nanocrystals can also act as efficient light
absorbers and charge transporters and can be coupled with other
materials to enhance the performance and functionality of the devices.
Perovskite nanocrystals have shown promising results in terms of
efficiency, stability, and versatility in these applications. However,
perovskite nanocrystals also face some challenges and limitations, such
as the toxicity of lead, the instability under ambient conditions, the
hysteresis and degradation under electrical or optical stress, and the
reproducibility and scalability of the synthesis and fabrication
methods. These issues need to be addressed and overcome to realize the
full potential of perovskite nanocrystals in practical devices.
Therefore, further research and development are needed to improve the
material quality, stability, and compatibility of perovskite
nanocrystals, and to explore new compositions, structures, and
functionalities of these fascinating nanomaterials.”</p>
</div>
</div>
</div>
</div>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge<em>(continued)</em><a class="anchor" aria-label="anchor" href="#challenge-4"></a>
</h3>
<div class="callout-content">
<p>Print the summarized text.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>...</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution11" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution11" aria-expanded="false" aria-controls="collapseSolution11">
  <h4 class="accordion-header" id="headingSolution11">Show me the solution</h4>
</button>
<div id="collapseSolution11" class="accordion-collapse collapse" data-bs-parent="#accordionSolution11" aria-labelledby="headingSolution11">
<div class="accordion-body">
<p>A: You can use the following code to load the transformers library
and the pre-trained model and tokenizer for text summarization:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>text <span class="op">=</span> <span class="st">" Perovskite nanocrystals are a class of semiconductor nanocrystals that have attracted…</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a><span class="er">Output:</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Transformers revolutionized NLP by processing words in parallel
through an attention mechanism, capturing context more effectively than
sequential models</li>
<li>The summation and activation function within a neuron transform
inputs through weighted sums and biases, followed by an activation
function to produce an output.</li>
<li>Transformers consist of encoders, decoders, positional encoding,
input/output embedding, and softmax output, working together to process
and generate data.</li>
<li>Transformers are not limited to NLP and can be applied to other AI
applications due to their ability to handle complex data patterns.</li>
<li>Sentiment analysis and text summarization are practical applications
of transformers in NLP, enabling the analysis of emotional tone and the
creation of concise summaries from large texts.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-06-llms"><p>Content from <a href="06-llms.html">Large Language Models</a></p>
<hr>
<p>Last updated on 2024-05-12 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/06-llms.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are the main features of large language models?</li>
<li>How is BERT different from GPT models?</li>
<li>How can I use open-source LLMs, such as LLM examples in huggingface,
for research tasks?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Be able to explain the structure of large language models and their
main components</li>
<li>Identify differences between BERT and GPT.</li>
<li>Be able to use open-source LLMs, such as huggingface, for text
summarization, classification, and generation.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="introduction-to-llms"><h2 class="section-heading">6.1. Introduction to LLMs<a class="anchor" aria-label="anchor" href="#introduction-to-llms"></a>
</h2>
<hr class="half-width">
<p>Large Language Models (LLMs) have become a cornerstone of modern
natural language processing (NLP). Since the introduction of the
transformer architecture in 2017, LLMs have leveraged this design to
achieve remarkable language understanding and generation capabilities.
In the previous episode, we discussed the transformer architecture,
which is integral to all LLMs, utilizing its encoder and decoder
components to process language.</p>
<p>LLMs have several key features.</p>
<figure><img src="../fig/llms_1.png" class="figure mx-auto d-block"></figure><div id="challenge" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge"></a>
</h3>
<div class="callout-content">
<p>Fill in the above feature placeholders. Discuss what are these key
components. Explain the key features in detail and compare your thoughts
with the other group members:</p>
<ol style="list-style-type: decimal">
<li><p>Transformer Architecture: A neural network design that uses
self-attention mechanisms to weigh the influence of different parts of
the input data.</p></li>
<li><p>Pre-training: involves teaching LLMs to anticipate words in
sentences, using either bi-directional or uni-directional approaches,
(based on the LLM type), without the need for understanding or
experience.</p></li>
<li><hr></li>
<li><hr></li>
<li><hr></li>
<li><hr></li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<figure><img src="../fig/llms_4.png" class="figure mx-auto d-block"></figure><ol style="list-style-type: decimal">
<li><p>Transformer Architecture: A neural network design that uses
self-attention mechanisms to weigh the influence of different parts of
the input data.</p></li>
<li><p>Pre-training: involves teaching LLMs to anticipate words in
sentences, using either bi-directional or uni-directional approaches,
(based on the LLM type), without the need for understanding or
experience.</p></li>
<li><p>Word/Token Embedding: The process of converting words or phrases
into numerical form (vectors) that computers can understand.</p></li>
</ol>
<p><strong>RECALL</strong> embedding?</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/06a4c996-cfe4-414e-b91c-9048c1006fc6" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/6e7a5d15-94a4-4522-910f-3ad67dd2ee69" alt="image" class="figure"><a href="https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/" class="external-link">source</a></p>
<ol start="4" style="list-style-type: decimal">
<li><p>Context Window: The range of words the model considers for
predicting the next word or understanding the current word within a
sentence.</p></li>
<li><p>Parameters: The aspects of the model that are learned from
training data and determine the model’s behavior.</p></li>
<li><p>Transfer Learning: The process LLMs use to apply their prior
knowledge to new tasks.</p></li>
</ol>
<p>Thus, the completed graph will be:</p>
</div>
</div>
</div>
</div>
<p>We can categorize LLMs based on the transformer architecture. Let’s
have another look into the transformer architecture, this time we
categorize them based on the two main components: <strong>Encoder and
Decoder</strong>. LLMs can be designed to handle different tasks based
on their underlying transformer blocks and whether they have
encoder-only, decoder-only, or encoder-decoder layers.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/db7f7677-67eb-475d-bb3e-3b703b2fbb0f" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-1"></a>
</h3>
<div class="callout-content">
<p>How do you think we should connect each one of the following
transformers to the correct color?</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/7b5bd803-c075-4b66-9d8e-b989172f50d8" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/9d4afc99-d8f4-4a17-9af9-5fab4fac5dfd" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
</div>
<p>• Encoders are used for understanding tasks like sentence
classification.</p>
<p>• Decoders excel in generative tasks like text generation.</p>
<p>• The combination of encoders and decoders in transformers allows
them to be versatile and perform a variety of tasks, from translation to
summarization, depending on the specific requirements of the task at
hand.</p>
<div id="encoder-vs.-decoder-andor-bert-vs.-gpt" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="encoder-vs.-decoder-andor-bert-vs.-gpt" class="callout-inner">
<h3 class="callout-title">Encoder Vs. Decoder and/or BERT Vs. GPT<a class="anchor" aria-label="anchor" href="#encoder-vs.-decoder-andor-bert-vs.-gpt"></a>
</h3>
<div class="callout-content">
<p>We will see models like BERT use encoders for bidirectional
understanding, and models like GPT use decoders for generating coherent
text, making them suitable for chatbots or virtual assistants.</p>
</div>
</div>
</div>
<div id="discussion" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Think of some examples of traditional NLP models, such as
n-gram models, hidden Markov models, LSTMs, and RNNs. How do large
language models differ from them in terms of architecture, data, and
performance?</p>
<p>A: Traditional NLP models, such as n-gram models, hidden Markov
models (HMMs), Long Short-Term Memory Networks (LSTMs), and Recurrent
Neural Networks (RNNs), differ significantly from the recent LLMs.
N-gram models predict the next item in a sequence based on the previous
‘n-1’ items without any deep understanding of context. HMMs are
statistical models that output probabilities of sequences and are often
used for tasks like part-of-speech tagging. LSTMs and RNNs are types of
neural networks that can process sequences of data and are capable of
learning order dependence in sequence prediction.</p>
<p>Compared to these traditional models, LLMs have several key
differences: - <strong>Architecture</strong>: Novel LLMs use transformer
architectures, which are more advanced than the simple recurrent units
of RNNs or the gated units of LSTMs. Transformers use self-attention to
weigh the influence of different parts of the input data, which is more
effective for understanding context. - <strong>Data</strong>: Novel LLMs
are trained on massive datasets, often sourced from the internet, which
allows them to learn a wide variety of language patterns, common
knowledge, and even reasoning abilities. Traditional models typically
use smaller, more curated datasets. - <strong>Performance</strong>:
Novel LLMs generally outperform traditional models in a wide range of
language tasks due to their ability to understand and generate
human-like text. They can capture subtleties and complexities of
language that simpler models cannot, leading to more accurate and
coherent outputs.</p>
</div>
</div>
</div>
</section><section id="bert"><h2 class="section-heading">6.2. BERT<a class="anchor" aria-label="anchor" href="#bert"></a>
</h2>
<hr class="half-width">
<p>Bidirectional Encoder Representations from Transformers (BERT) is an
LLM that uses an encoder-only architecture from transformers. It is
designed to understand the context of a word based on all of its
surroundings (bidirectional context). Let’s guess the missing words in
the text below to comprehend the workings of BERT:</p>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-2"></a>
</h3>
<div class="callout-content">
<p>Complete the following paragraph:</p>
<p>“BERT is a revolutionary language model that uses an ______ (encoder)
to process words in a sentence. Unlike traditional models, it predicts
words based on the ______ rather than in sequence. Its training involves
______, where words are intentionally hidden, or ______, and the model
learns to predict them. This results in rich ______ that capture the
nuanced meanings of words.”</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>“BERT is a revolutionary language model that uses an
<strong>encoder</strong> to process words in a sentence. Unlike
traditional models, it predicts words based on the
<strong>context</strong> rather than in sequence. Its training involves
<strong>self-supervised learning</strong>, where words are intentionally
hidden, or <strong>‘masked’</strong>, and the model learns to predict
them. This results in rich <strong>embeddings</strong> that capture the
nuanced meanings of words.”</p>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Heads-up: MLM &amp; NSP</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p>Pre-training of language models involves a process where models like
BERT and GPT learn to predict words in sentences without specific task
training. This is achieved through methods like the Masked Language
Model (MLM) for bi-directional models, which predict masked words using
surrounding context. MLM in BERT predicts missing words in a sentence by
masking them during training.</p>
<p>For Next Sentence Prediction (NSP) BERT learns to predict if two
sentences logically follow each other.</p>
</div>
</div>
</div>
</div>
</section><section id="gpt"><h2 class="section-heading">6.3. GPT<a class="anchor" aria-label="anchor" href="#gpt"></a>
</h2>
<hr class="half-width">
<p>Generative Pretrained Transformer (GPT) models, on the other hand,
use a decoder-only architecture. They excel at generating coherent and
contextually relevant text. Check the following table that summarizes
three different LLMs. The middle column misses some information about
GPT models. With the help of your teammates complete the table and
explain the differences in the end.</p>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-3"></a>
</h3>
<div class="callout-content">
<p>Write in the gray boxes with the correct explanations.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/c1bd86ec-2b48-4201-bc7f-db63d73b0df9" alt="image" class="figure"><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/e50f0111-2956-418d-8dba-612e1942ddfd" alt="image" class="figure"><a href="https://medium.com/@reyhaneh.esmailbeigi/bert-gpt-and-bart-a-short-comparison-5d6a57175fca" class="external-link">source</a></p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/c01d8b7f-b237-499b-b169-d40c785e30d0" alt="image" class="figure"><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/1153ca29-aafd-4de6-8013-525b21ded916" alt="image" class="figure"><a href="https://medium.com/@reyhaneh.esmailbeigi/bert-gpt-and-bart-a-short-comparison-5d6a57175fca" class="external-link">source</a></p>
</div>
</div>
</div>
</div>
<div id="discussion-1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: From what you learned above how can you explain the
differences between the three LLM types? Discuss in groups and share
your answers.</p>
<p>A: <em>We can see it as the processes of trying to understand a
conversation (BERT), versus trying to decide what to say next in the
conversation (GPT). BERT is like someone who listens to the entire
conversation before and after a word to really understand its
meaning.</em></p>
<p><em>For example, in the sentence “I ate an apple,” BERT would look at
both “I ate an” and “apple” to figure out what <strong>“an”</strong>
refers to. It’s trained by playing a game of <strong>‘guess the missing
word,’</strong> where some words are hidden <strong>(masked)</strong>
and it has to use the context to fill in the blanks.</em></p>
<p><em>GPT, on the other hand, is like a storyteller who only needs to
know what was said before to continue the tale. It would take “I ate an”
and <strong>predict that the next word</strong> might be “apple.” It
learns by <strong>reading a lot of text</strong> and practicing how to
predict the next word in a sentence.</em></p>
<p><em>Both are smart in their own ways, but they’re used for different
types of language tasks. BERT is great for understanding the
<strong>context of words</strong>, while GPT is excellent at
<strong>generating new text</strong> based on what it’s seen before. The
following schematics demonstrate their performing differences:</em></p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/114d8549-dede-4976-8e69-167cfe33879f" alt="image" class="figure"><a href="https://medium.com/@reyhaneh.esmailbeigi/bert-gpt-and-bart-a-short-comparison-5d6a57175fca" class="external-link">source</a></p>
</div>
</div>
</div>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>How LLMs can be Compared? What is HELM?</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" aria-labelledby="headingSpoiler2" data-bs-parent="#accordionSpoiler2">
<div class="accordion-body">
<p>Models are often benchmarked using standardized datasets and metrics.
The Holistic Evaluation of Language Models (HELM) by Stanford provides a
comprehensive framework for evaluating LLMs across multiple
dimensions.</p>
<p><img src="../fig/llms_9.png" class="figure"><a href="https://crfm.stanford.edu/helm/lite/latest/" class="external-link">source</a></p>
<p>GPT-4 models are outperforming other LLM models in terms of
accuracy.</p>
</div>
</div>
</div>
</div>
<div id="discussion-2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-2" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-2"></a>
</h3>
<div class="callout-content">
<p>What are some examples of LLMs, and how are they trained and used for
research tasks? Consider some of the main features and characteristics
of LLMs, such as transformer architecture, self-attention mechanism,
pre-training and fine-tuning, and embedding capabilities. How do these
features enable LLMs to perform various NLP tasks, such as text
classification, text generation, or question answering?</p>
</div>
</div>
</div>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-4"></a>
</h3>
<div class="callout-content">
<p>How can we compare different LLMs? Are there any benchmarks?</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">Show me the solution</h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<p>A: Comparing Performance (Benchmarking): 1. Performance can be
compared based on the model’s architecture, computational efficiency,
and suitability for specific tasks. 2. Benchmarks and leaderboards (such
as HELM) can provide insights into how different models perform on
standardized datasets. 3. Community feedback and use-case studies can
also inform the practical effectiveness of different LLMs.</p>
</div>
</div>
</div>
</div>
</section><section id="open-source-llms"><h2 class="section-heading">6.4. Open-Source LLMs:<a class="anchor" aria-label="anchor" href="#open-source-llms"></a>
</h2>
<hr class="half-width">
<p>It is very important for researchers to openly have access to capable
LLMs for their studies. Fortunately, some companies are supporting
open-source LLMs. The BLOOM model, developed by the BigScience Workshop
in collaboration with Hugging Face and other organizations, was released
on July 6, 2022. It offers a wide range of model sizes, from 1.1 billion
to 176 billion parameters, and is licensed under the open RAIL-M v1.
BLOOM is known for its instruct models, coding capabilities,
customization finetuning, and being open source. It is more openly
accessible and benefits from a large community and extensive
support.</p>
<p>On the other hand, the LLaMA model, developed by Meta AI, was
released on February 24, 2023. It is available in four sizes: 7 billion,
13 billion, 33 billion, and 65 billion parameters. The license for LLaMA
is restricted to noncommercial use, and access is primarily for
researchers. Despite its smaller size, LLaMA is parameter-efficient and
has outperformed GPT-3 on many benchmarks. However, its accessibility is
more gated compared to BLOOM, and community support is limited to
approved researchers.</p>
<p>Now let’s summarize what we learned here in the following table:</p>
<figure><img src="../fig/llms_10.png" class="figure mx-auto d-block"></figure><p>Hugging Face provides several different LLMs. Now we want to see how
we can use an open-source model. using the Hugging Face datasets library
and an open-source Large Language Model (LLM). We will go through the
process of setting up the environment, installing necessary libraries,
loading a dataset, and then using an LLM to process the data. We will
start with setting up the environment.</p>
<div id="heads-up" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="heads-up" class="callout-inner">
<h3 class="callout-title">Heads up<a class="anchor" aria-label="anchor" href="#heads-up"></a>
</h3>
<div class="callout-content">
<p>Before we begin, ensure that you have Python installed on your
system. Python 3.6 or later is recommended. You can download Python from
the official Python website.</p>
</div>
</div>
</div>
<p>Next, we will install the necessary libraries through the terminal or
command prompt:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>pip install datasets transformers</span></code></pre>
</div>
<p>We use the <strong>squad dataset</strong> here, which is a
question-answering dataset Question-answering is one of main goals of
utilizing LLMs for research projects. When you run this script, the
expected output should be the answer to the question based on the
provided context. Here is how to load it:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co"># Load the SQuAD dataset</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>squad_dataset <span class="op">=</span> load_dataset(<span class="st">'squad'</span>)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># Print the first example in the training set</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="bu">print</span>(squad_dataset[<span class="st">'train'</span>][<span class="dv">0</span>])</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a></span></code></pre>
</div>
<p>Now, we can load a pre-trained model from Hugging Face. For this
example, let’s use the bert-base-uncased model, which is different from
BLOOM and is suitable for question-answering tasks:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForQuestionAnswering, AutoTokenizer</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co"># Load the tokenizer and model</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>model <span class="op">=</span> AutoModelForQuestionAnswering.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span></code></pre>
</div>
<p>We need to define the question and context here:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What is the name of the university in Paris that was founded in 1257?"</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>context <span class="op">=</span> <span class="st">"The University of Paris, founded in 1257, is often referred to as the Sorbonne after the college created by Robert de Sorbon. It is one of the world's oldest universities."</span></span></code></pre>
</div>
<p>Recall to be able to feed data into the model, we should already
tokenize our data. Once we have our data tokenized, we can use the model
to make predictions. Here is how to tokenize the first example:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co"># Tokenize the first example</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>Inputs <span class="op">=</span> tokenizer(squad_dataset[<span class="st">'train'</span>][<span class="dv">0</span>][<span class="st">'question'</span>], squad_dataset[<span class="st">'train'</span>][<span class="dv">0</span>][<span class="st">'context'</span>], return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="co"># Get model predictions</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span></code></pre>
</div>
<p>Note that the model outputs are raw <strong>logits</strong>. We need
to convert these into an answer by selecting the tokens with the highest
start and end scores:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="co"># Find the tokens with the highest `start` and `end` scores</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>answer_start <span class="op">=</span> torch.argmax(outputs.start_logits)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>answer_end <span class="op">=</span> torch.argmax(outputs.end_logits) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a><span class="co"># Convert tokens to the answer string</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>answer <span class="op">=</span> tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens (inputs[<span class="st">'input_ids'</span>][<span class="dv">0</span>][answer_start:answer_end]))</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="bu">print</span>(answer)</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a></span></code></pre>
</div>
<p>This will print the answer to the question based on the context
provided in the dataset. In this case, the output would be:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>Output: the sorbonne</span></code></pre>
</div>
<p>This output indicates that the model has correctly identified “the
Sorbonne” as the name of the university in Paris founded in 1257, based
on the context given. Remember, the actual output may vary slightly
depending on the model version and the specific weights used at the time
of inference.</p>
<div id="discussion-3" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-3" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-3"></a>
</h3>
<div class="callout-content">
<p>Teamwork: What are the challenges and implications of LLMs, such as
scalability, generalization, and social impact? What does it mean when
an LLM hallucinates?</p>
</div>
</div>
</div>
<div id="challenge-5" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-5" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-5"></a>
</h3>
<div class="callout-content">
<p>Use the OpenAI library to access and use an open-source LLM for text
summarization. You can use the following code to load the OpenAI library
and the pre-trained model and tokenizer for text summarization:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>openai.api_key <span class="op">=</span> <span class="st">"sk-&lt;your_api_key&gt;"</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>text_summarizer <span class="op">=</span> openai.Completion.create(engine<span class="op">=</span><span class="st">"davinci"</span>, task<span class="op">=</span><span class="st">"summarize"</span>)</span></code></pre>
</div>
<p>Use the text_summarizer to summarize the following text.</p>
</div>
</div>
</div>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Text</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" aria-labelledby="headingSpoiler3" data-bs-parent="#accordionSpoiler3">
<div class="accordion-body">
<p>“Perovskite nanocrystals are a class of semiconductor nanocrystals,
which exhibit unique characteristics that separate them from traditional
quantum dots. Perovskite nanocrystals have an ABX3 composition where A =
cesium, methylammonium (MA), or formamidinium (FA); B = lead or tin; and
X = chloride, bromide, or iodide. Their unique qualities largely involve
their unusual band structure which renders these materials effectively
defect-tolerant or able to emit brightly without surface passivation.
This is in contrast to other quantum dots such as CdSe which must be
passivated with an epitaxially matched shell to be bright emitters. In
addition to this, lead-halide perovskite nanocrystals remain bright
emitters when the size of the nanocrystal imposes only weak quantum
confinement. This enables the production of nanocrystals that exhibit
narrow emission linewidths regardless of their polydispersity. The
combination of these attributes and their easy-to-perform synthesis has
resulted in numerous articles demonstrating the use of perovskite
nanocrystals as both classical and quantum light sources with
considerable commercial interest. Perovskite nanocrystals have been
applied to numerous other optoelectronic applications such as
light-emitting diodes, lasers, visible communication, scintillators,
solar cells, and photodetectors. The first report of perovskite
nanocrystals was published in 2014 by Protesescu et al., who synthesized
cesium lead halide nanocrystals using a hot-injection method. They
showed that the nanocrystals can emit brightly when excited by
ultraviolet or blue light, and their colors are tunable across the
entire visible spectrum by changing the halide from chloride (UV/blue)
to bromide (green) and iodide (red). They also demonstrated that the
nanocrystals can be incorporated into thin films and show high
photoluminescence quantum yields (PLQYs) of up to 90%. Since then, many
other synthetic methods have been developed to produce perovskite
nanocrystals with different shapes, sizes, compositions, and surface
ligands. Some of the common methods include ligand-assisted
reprecipitation, antisolvent precipitation, solvothermal synthesis,
microwave-assisted synthesis, and microfluidic synthesis. Perovskite
nanocrystals can be classified into different types based on their
structure, dimensionality, and composition. The most common type is the
three-dimensional (3D) perovskite nanocrystals, which have a cubic or
orthorhombic crystal structure and a band gap that depends on the halide
content. The 3D perovskite nanocrystals can be further divided into pure
halide perovskites (such as CsPbX3) and mixed halide perovskites (such
as CsPb(Br/I)3), which can exhibit color tuning, anion exchange, and
halide segregation phenomena. Another type is the two-dimensional (2D)
perovskite nanocrystals, which have a layered structure with organic
cations sandwiched between inorganic perovskite layers. The 2D
perovskite nanocrystals have a quantum well-like band structure and a
band gap that depends on the thickness of the perovskite layers. The 2D
perovskite nanocrystals can also be mixed with 3D perovskite
nanocrystals to form quasi-2D perovskite nanocrystals, which can improve
the stability and emission efficiency of the nanocrystals. A third type
is the metal-free perovskite nanocrystals, which replace the metal
cations (such as Pb or Sn) with other elements (such as Bi or Sb). The
metal-free perovskite nanocrystals have a lower toxicity and higher
stability than the metal-based perovskite nanocrystals, but they also
have a lower PLQY and a broader emission linewidth. The development of
perovskite nanocrystals in the past few years has been remarkable, with
significant advances in synthesis, characterization, and application.
However, there are still some challenges and opportunities for further
improvement. One of the major challenges is the stability of perovskite
nanocrystals, which are sensitive to moisture, oxygen, heat, light, and
electric fields. These factors can cause degradation, phase transition,
and non-radiative recombination of the nanocrystals, resulting in
reduced emission intensity and color stability. Several strategies have
been proposed to enhance the stability of perovskite nanocrystals, such
as surface passivation, encapsulation, doping, alloying, and embedding
in matrices. Another challenge is the toxicity of perovskite
nanocrystals, which are mainly composed of lead, a heavy metal that can
cause environmental and health hazards. Therefore, there is a need to
develop lead-free or low-lead perovskite nanocrystals that can maintain
the high performance and tunability of the lead-based ones. Some of the
promising candidates include tin-based, bismuth-based, and
antimony-based perovskite nanocrystals. A third challenge is the
scalability and integration of perovskite nanocrystals, which are
essential for practical applications. There is a need to develop
cost-effective and large-scale synthesis methods that can produce
high-quality and uniform perovskite nanocrystals. Moreover, there is a
need to develop efficient and reliable fabrication techniques that can
integrate perovskite nanocrystals into various devices and platforms. In
conclusion, perovskite nanocrystals are a fascinating class of
nanomaterials that have shown remarkable potential for various photonic
applications. They have unique properties such as defect tolerance, high
quantum yield, fast radiative decay, and narrow emission linewidth in
weak confinement, which make them ideal candidates for light emission
devices. They also have a wide color tunability from ultraviolet to
near-infrared regions, which makes them suitable for various
wavelength-dependent applications. However, there are still some
challenges that need to be overcome, such as stability, toxicity,
scalability, and integration. Therefore, further research and
development are needed to address these issues and to explore new
opportunities for perovskite nanocrystals in the field of
nanophotonics.</p>
</div>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6">Show me the solution</h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" aria-labelledby="headingSolution6" data-bs-parent="#accordionSolution6">
<div class="accordion-body">
<p>Print the summarized text.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>openai.api_key <span class="op">=</span> <span class="st">"sk-&lt;your_api_key&gt;"</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>text_summarizer <span class="op">=</span> openai.Completion.create(engine<span class="op">=</span><span class="st">"davinci"</span>, task<span class="op">=</span><span class="st">"summarize"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>text <span class="op">=</span> <span class="st">" Perovskite nanocrystals are a class of semiconductor …"</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>summary <span class="op">=</span> text_summarizer(text)[<span class="st">'choices'</span>][<span class="dv">0</span>][<span class="st">'text'</span>]</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a><span class="bu">print</span>(text)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="bu">print</span>(summary)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>output:</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="challenge-6" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-6" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-6"></a>
</h3>
<div class="callout-content">
<p>Use the huggingface library to access and use an open-source
domain-specific LLM for text classification. You can use the following
code to load the huggingface library and the pre-trained model and
tokenizer for text classification:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>text_classifier <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>...</span></code></pre>
</div>
<p>Use the text_classifier to classify the following text into one of
the categories: metals, ceramics, polymers, or composites. Print the
text and the predicted category and score.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>Text: <span class="st">"Polyethylene is a thermoplastic polymer that consists of long chains of ethylene monomers. It is one of the most common and widely used plastics in the world. It has many applications, such as packaging, bottles, containers, films, pipes, and cables. Polyethylene can be classified into different grades based on its density, molecular weight, branching, and crystallinity."</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution7" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution7" aria-expanded="false" aria-controls="collapseSolution7">
  <h4 class="accordion-header" id="headingSolution7">Show me the solution</h4>
</button>
<div id="collapseSolution7" class="accordion-collapse collapse" aria-labelledby="headingSolution7" data-bs-parent="#accordionSolution7">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>text_classifier <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Polyethylene is a thermoplastic polymer that consists of long chains of ethylene monomers. It is one of the most common and widely used plastics in the world. It has many applications, such as packaging, bottles, containers, films, pipes, and cables. Polyethylene can be classified into different grades based on its density, molecular weight, branching, and crystallinity."</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="bu">print</span>(text)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="bu">print</span>(text_classifier(text))</span></code></pre>
</div>
<pre><code><span></span>
<span><span class="va">output</span><span class="op">:</span> <span class="st">"Polyethylene is a thermoplastic polymer that consists of long chains of ethylene monomers. It is one of the most common and widely used plastics in the world. It has many applications, such as packaging, bottles, containers, films, pipes, and cables. Polyethylene can be classified into different grades based on its density, molecular weight, branching, and crystallinity."</span></span>
<span><span class="st">"[{'label': 'polymers', 'score': 0.9987659454345703}]"</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="challenge-7" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-7" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-7"></a>
</h3>
<div class="callout-content">
<p>Use the huggingface library to access and use an open-source LLM for
text generation. You can use the following code to load the huggingface
library and the pre-trained model and tokenizer for text generation:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>text_generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>)</span></code></pre>
</div>
<p>Use the text_generator to generate a paragraph of text based on the
following prompt: “The applications of nanomaterials in material science
are”. Print the prompt and the generated text.</p>
</div>
</div>
</div>
<div id="accordionSolution8" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution8" aria-expanded="false" aria-controls="collapseSolution8">
  <h4 class="accordion-header" id="headingSolution8">Show me the solution</h4>
</button>
<div id="collapseSolution8" class="accordion-collapse collapse" aria-labelledby="headingSolution8" data-bs-parent="#accordionSolution8">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>text_generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>)</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"The applications of nanomaterials in material science are"</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>generated_text <span class="op">=</span> text_generator(prompt)[<span class="dv">0</span>][<span class="st">'generated_text'</span>]</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="bu">print</span>(prompt)</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="bu">print</span>(generated_text)</span></code></pre>
</div>
<pre><code>
output: "The applications of nanomaterials in material science are
The applications of nanomaterials in material science are diverse and promising. Nanomaterials are materials that have at least one dimension in the nanometer range (1-100 nm). Nanomaterials can exhibit novel physical, chemical, and biological properties that are different from their bulk counterparts, such as high surface area, quantum confinement, enhanced reactivity, and tunable functionality. Nanomaterials can be used for various purposes in material science, such as improving the performance and functionality of existing materials, creating new materials with unique properties, and enabling new functionalities and devices. Some examples of nanomaterials applications in material science are:

- Nanocomposites: Nanomaterials can be incorporated into other materials, such as polymers, metals, ceramics, or biomaterials, to form nanocomposites that have improved mechanical, thermal, electrical, optical, or magnetic properties. For instance, carbon nanotubes can be used to reinforce polymer composites and increase their strength, stiffness, and conductivity.
- Nanocoatings: Nanomaterials can be used to coat the surface of other materials, such as metals, glass, or plastics, to provide them with enhanced protection, durability, or functionality. For example, titanium dioxide nanoparticles can be used to coat glass and make it self-cleaning, anti-fogging, and anti-bacterial.
- Nanosensors: Nanomaterials can be used to sense and measure various physical, chemical, or biological parameters, such as temperature, pressure, strain, pH, or biomolecules. For example, gold nanoparticles can be used to detect and quantify the presence of specific DNA sequences or proteins by changing their color or fluorescence.
- Nanomedicine: Nanomaterials can be used for various biomedical applications, such as drug delivery, imaging, diagnosis, or therapy. For example, magnetic nanoparticles can be used to deliver drugs to specific target sites in the body by using an external magnetic field, or to enhance the contrast of magnetic resonance imaging (MRI).
</code></pre>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>LLMs are based on the transformer architecture.</li>
<li>BERT and GPT have distinct approaches to processing language.</li>
<li>Open source LLMs provide transparency and customization for research
applications.</li>
<li>Benchmarking with HELM offers a holistic view of model
performance.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-07-domain-specific-llms"><p>Content from <a href="07-domain-specific-llms.html">Domain-Specific LLMs</a></p>
<hr>
<p>Last updated on 2024-05-12 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/07-domain-specific-llms.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can tune the LLMs to be domain-specific?</li>
<li>What are some available approaches to empower LLMs solve specific
research problems?</li>
<li>Which approach should I use for my research?</li>
<li>What are the challenges and trade-offs of domain-specific LLMs?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Be able to identify approaches by which LLMs can be tuned for
solving research problems.</li>
<li>Be able to use introductory approaches for creating domain-specific
LLMs.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="introduction-to-dsl-available-approaches"><h2 class="section-heading">7.1. Introduction to DSL (Available Approaches)<a class="anchor" aria-label="anchor" href="#introduction-to-dsl-available-approaches"></a>
</h2>
<hr class="half-width">
<p>To enhance the response quality of an LLM for solving specific
problems we need to use strategies by which we can tune the LLM.
Generally, there are four ways to enhance the performance of LLMs:</p>
<p><strong>1. Prompt Optimization:</strong></p>
<p>To elicit specific and accurate responses from LLMs by designing
prompts strategically.</p>
<ul>
<li>
<em>Zero-shot Prompting</em>: This is the simplest form of prompting
where the LLM is given a task or question without any context or
examples. It relies on the LLM’s pre-existing knowledge to generate a
response.</li>
</ul>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p>“What is the capital of France?” The LLM would respond with “Paris”
based on its internal knowledge.</p>
</div>
</div>
</div>
</div>
<ul>
<li>
<em>Few-shot Prompting</em>: In this technique, the LLM is provided
with a few examples to demonstrate the expected response format or
content.</li>
</ul>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" aria-labelledby="headingSpoiler2" data-bs-parent="#accordionSpoiler2">
<div class="accordion-body">
<p>To determine sentiment, you might provide examples like “I love sunny
days. (+1)” and “I hate traffic. (-1)” before asking the LLM to analyze
a new sentence.</p>
</div>
</div>
</div>
</div>
<p><strong>2. Retrieval Augmented Generation (RAG):</strong></p>
<p>To supplement the LLM’s generative capabilities with information
retrieved from external databases or documents.</p>
<ul>
<li>
<em>Retrieval</em>: The LLM queries a database to find relevant
information that can inform its response.</li>
</ul>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" aria-labelledby="headingSpoiler3" data-bs-parent="#accordionSpoiler3">
<div class="accordion-body">
<p>If asked about recent scientific discoveries, the LLM might retrieve
articles or papers on the topic.</p>
</div>
</div>
</div>
</div>
<ul>
<li>
<em>Generation</em>: After retrieving the information, the LLM
integrates it into a coherent response.</li>
</ul>
<div id="accordionSpoiler4" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler4" aria-expanded="false" aria-controls="collapseSpoiler4">
  <h3 class="accordion-header" id="headingSpoiler4">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler4" class="accordion-collapse collapse" aria-labelledby="headingSpoiler4" data-bs-parent="#accordionSpoiler4">
<div class="accordion-body">
<p>Using the retrieved scientific articles, the LLM could generate a
summary of the latest findings in a particular field.</p>
</div>
</div>
</div>
</div>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/b35c25f0-6176-490c-b726-662025f47075" alt="image" class="figure"><a href="https://www.maartengrootendorst.com/blog/improving-llms/" class="external-link">source</a></p>
<p><strong>3. Fine-Tuning:</strong></p>
<p>To adapt a general-purpose LLM to excel at a specific task or within
a particular domain.</p>
<ul>
<li>
<em>Language Modeling Task Fine-Tuning</em>: This involves training
the LLM on a large corpus of text to improve its ability to predict the
next word or phrase in a sentence.</li>
</ul>
<div id="accordionSpoiler5" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler5" aria-expanded="false" aria-controls="collapseSpoiler5">
  <h3 class="accordion-header" id="headingSpoiler5">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler5" class="accordion-collapse collapse" aria-labelledby="headingSpoiler5" data-bs-parent="#accordionSpoiler5">
<div class="accordion-body">
<p>An LLM fine-tuned on legal documents would become better at
generating text that resembles legal writing.</p>
</div>
</div>
</div>
</div>
<ul>
<li>
<em>Supervised Q&amp;A Fine-Tuning</em>: Here, the LLM is trained on
a dataset of question-answer pairs to enhance its performance on Q&amp;A
tasks.</li>
</ul>
<div id="accordionSpoiler6" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler6" aria-expanded="false" aria-controls="collapseSpoiler6">
  <h3 class="accordion-header" id="headingSpoiler6">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler6" class="accordion-collapse collapse" aria-labelledby="headingSpoiler6" data-bs-parent="#accordionSpoiler6">
<div class="accordion-body">
<p>An LLM fine-tuned with medical Q&amp;A pairs would provide more
accurate responses to health-related inquiries.</p>
</div>
</div>
</div>
</div>
<p><strong>4. Training from Scratch:</strong></p>
<p>Builds a model specifically for a domain, using relevant data from
the ground up.</p>
<div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Which approach do you think is more computation-intensive?
Which is more accurate? How are these qualities related? Evaluate the
trade-offs between fine-tuning and other approaches.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/c4f63e42-dcf9-4cfa-86fb-cf68357df229" alt="image" class="figure"><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/cffb0c09-6578-4206-94c3-ba90eca87515" alt="image" class="figure"><a href="https://medium.com/@pandey.vikesh/should-you-prompt-rag-tune-or-train-a-guide-to-choose-the-right-generative-ai-approach-5e264043bd7d" class="external-link">source</a></p>
</div>
</div>
</div>
</div>
<div id="discussion-1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: What is DSL and why are they useful for research tasks?
Think of some examples of NLP tasks that require domain-specific LLMs,
such as literature review, patent analysis, or material discovery. How
do domain-specific LLMs improve the performance and accuracy of these
tasks?</p>
<figure><img src="../fig/dsllms_2.png" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</section><section id="prompting"><h2 class="section-heading">7.2. Prompting<a class="anchor" aria-label="anchor" href="#prompting"></a>
</h2>
<hr class="half-width">
<p>For research applications where highly reliable answers are crucial,
Prompt Engineering combined with Retrieval-Augmented Generation (RAG) is
often the most suitable approach. This combination allows for
flexibility and high-quality outputs by leveraging both the generative
capabilities of LLMs and the precision of domain-specific data
sources:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>Install the Hugging Face libraries</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="op">!</span>pip install transformers datasets</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># Initialize the zero-shot classification pipeline</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"zero-shot-classification"</span>, model<span class="op">=</span><span class="st">"facebook/bart-large-mnli"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co"># Example research question</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What is the role of CRISPR-Cas9 in genome editing?"</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># Candidate topics to classify the question</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>topics <span class="op">=</span> [<span class="st">"Biology"</span>, <span class="st">"Technology"</span>, <span class="st">"Healthcare"</span>, <span class="st">"Genetics"</span>, <span class="st">"Ethics"</span>]</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co"># Perform zero-shot classification</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>result <span class="op">=</span> classifier(question, candidate_labels<span class="op">=</span>topics)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="co"># Output the results</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Classified under topics with the following scores:"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="cf">for</span> label, score <span class="kw">in</span> <span class="bu">zip</span>(result[<span class="st">'labels'</span>], result[<span class="st">'scores'</span>]):</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div id="accordionSpoiler7" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler7" aria-expanded="false" aria-controls="collapseSpoiler7">
  <h3 class="accordion-header" id="headingSpoiler7">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Be careful when fine-tuning a model</h3>
</button>
<div id="collapseSpoiler7" class="accordion-collapse collapse" aria-labelledby="headingSpoiler7" data-bs-parent="#accordionSpoiler7">
<div class="accordion-body">
<p>When fine-tuning a BERT model from Hugging Face, for instance, it is
essential to approach the process with precision and care.</p>
<ul>
<li><p>Begin by thoroughly understanding <strong>BERT’s
architecture</strong> and the specific task at hand to select the most
suitable model variant and hyperparameters.</p></li>
<li><p><strong>Prepare your dataset</strong> meticulously, ensuring it
is clean, well-represented, and split correctly to avoid <strong>data
leakage and overfitting</strong>.</p></li>
<li><p>Hyperparameter selection, such as learning rates and batch sizes,
should be made with consideration, and <strong>regularization</strong>
techniques like dropout should be employed to enhance the model’s
ability to generalize.</p></li>
<li><p><strong>Evaluate</strong> the model’s performance using
appropriate metrics and address any class imbalances with weighted loss
functions or similar strategies. Save checkpoints to preserve progress
and document every step of the fine-tuning process for transparency and
reproducibility.</p></li>
<li><p><strong>Ethical considerations</strong> are paramount; strive for
a model that is fair and unbiased. Ensure compliance with data
protection regulations, especially when handling sensitive
information.</p></li>
<li><p>Lastly, manage <strong>computational resources</strong> wisely
and engage with the Hugging Face community for additional support.
Fine-tuning is iterative, and success often comes through continuous
experimentation and learning.</p></li>
</ul>
</div>
</div>
</div>
</div>
<div id="challenge" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge"></a>
</h3>
<div class="callout-content">
<p>Check the following structure. Guess which optimization strategy is
represented in these architectures.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/cfec9a3a-e334-4a87-8318-5c7495e0f6b7" alt="image" class="figure"><a href="https://www.linkedin.com/pulse/fine-tuning-prompt-engineering-rag-lokesh--kjaie/" class="external-link">source</a></p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>RAG addresses the challenge of <strong>real-time</strong> data
fetching by merging the generative talents of these models with the
ability to consult a broad document corpus, enhancing their responses.
The potential for live-RAG in chatbots suggests a future where AI can
conduct on-the-spot searches, access up-to-date information, and rival
search engines in answering timely questions.</p>
</div>
</div>
</div>
</div>
<div id="discussion-2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-2" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-2"></a>
</h3>
<div class="callout-content">
<p>Teamwork: What are the challenges and trade-offs of domain-specific
LLMs, such as data availability, model size, and complexity?</p>
<p>Consider some of the factors that affect the quality and reliability
of domain-specific LLMs, such as the amount and quality of
domain-specific data, the computational resources and time required for
training or fine-tuning, and the generalization and robustness of the
model. How do these factors pose problems or difficulties for
domain-specific LLMs and how can we overcome them?</p>
</div>
</div>
</div>
<div id="discussion-3" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-3" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-3"></a>
</h3>
<div class="callout-content">
<p>What are some available approaches for creating domain-specific LLMs,
such as fine-tuning and knowledge distillation?</p>
<p>Consider some of the main steps and techniques for creating
domain-specific LLMs, such as selecting a general LLM, collecting and
preparing domain-specific data, training or fine-tuning the model, and
evaluating and deploying the model. How do these approaches differ from
each other and what are their advantages and disadvantages?</p>
</div>
</div>
</div>
<p>Now let’s try One-shot and Few-shot prompting examples and see how
they can help us to enhance the sensitivity of the LLM to our field of
study: One-shot prompting involves providing the model with a single
example to follow. It is like giving the model a hint about what you
expect. We will go through an example using Hugging Face’s transformers
library:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># Load a pre-trained model and tokenizer</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"gpt2"</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span>model_name)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co"># One-shot example</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Translate 'Hello, how are you?' to French:</span><span class="ch">\n</span><span class="st">Bonjour, comment ça va?</span><span class="ch">\n</span><span class="st">Translate 'I am learning new things every day' to French:"</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>result <span class="op">=</span> generator(prompt, max_length<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co"># Output the result</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="bu">print</span>(result[<span class="dv">0</span>][<span class="st">'generated_text'</span>])</span></code></pre>
</div>
<p>In this example, we provide the model with one translation example
and then ask it to translate a new sentence. The model uses the context
from the one-shot example to generate the translation.</p>
<p>But what if we have a Few-Shot Prompting? Few-shot prompting gives
the model several examples to learn from. This can improve the model’s
ability to understand and complete the task.</p>
<p>Here is how you can implement few-shot prompting:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># Load a pre-trained model and tokenizer</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"gpt2"</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span>model_name)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="co"># Few-shot examples</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"""</span><span class="ch">\</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="st">Q: What is the capital of France?</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a><span class="st">A: Paris.</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a><span class="st">Q: What is the largest mammal?</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a><span class="st">A: Blue whale.</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a><span class="st">Q: What is the human body's largest organ?</span></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a><span class="st">A: The skin.</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a><span class="st">Q: What is the currency of Japan?</span></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a><span class="st">A:"""</span></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>result <span class="op">=</span> generator(prompt, max_length<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a><span class="co"># Output the result</span></span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a><span class="bu">print</span>(result[<span class="dv">0</span>][<span class="st">'generated_text'</span>])</span></code></pre>
</div>
<p>In this few-shot example, we provide the model with three
question-answer pairs before posing a new question. The model uses the
pattern it learned from the examples to answer the new question.</p>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-1"></a>
</h3>
<div class="callout-content">
<p>To summarize this approach in a few steps, fill in the following
gaps: 1. Choose a Model: Select a <strong>—</strong> model from Hugging
Face that suits your task.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Load the Model: Use the <strong>—</strong> function to load the
model and tokenizer.</p></li>
<li><p>Craft Your Prompt: Write a <strong>—</strong> that includes one
or more examples, depending on whether you’re doing one-shot or few-shot
prompting.</p></li>
<li><p>Generate Text: Call the <strong>—</strong> with your prompt to
generate the <strong>—</strong>.</p></li>
<li><p>Review the Output: Check the generated text to see if the model
followed the <strong>—</strong> correctly.</p></li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li><p>Choose a Model: Select a <strong>pre-trained</strong> model from
Hugging Face that suits your task.</p></li>
<li><p>Load the Model: Use the <strong>pipeline</strong> function to
load the model and tokenizer.</p></li>
<li><p>Craft Your Prompt: Write a <strong>prompt</strong> that includes
one or more examples, depending on whether you’re doing one-shot or
few-shot prompting.</p></li>
<li><p>Generate Text: Call the <strong>generator</strong> with your
prompt to generate the <strong>output</strong>.</p></li>
<li><p>Review the Output: Check the generated text to see if the model
followed the <strong>examples</strong> correctly.</p></li>
<li>
</li>
</ol>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler8" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler8" aria-expanded="false" aria-controls="collapseSpoiler8">
  <h3 class="accordion-header" id="headingSpoiler8">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Prompting Quality</h3>
</button>
<div id="collapseSpoiler8" class="accordion-collapse collapse" aria-labelledby="headingSpoiler8" data-bs-parent="#accordionSpoiler8">
<div class="accordion-body">
<p>Remember, the quality of the output heavily depends on the quality
and relevance of the examples you provide. It’s also important to note
that larger models tend to perform better at these tasks due to their
greater capacity to understand and generalize from examples.</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Domain-specific LLMs are essential for tasks that require
specialized knowledge.</li>
<li>Prompt engineering, RAG, fine-tuning, and training from scratch are
viable approaches to create DSLs.</li>
<li>A mixed prompting-RAG approach is often preferred for its balance
between performance and resource efficiency.</li>
<li>Training from scratch offers the highest quality output but requires
significant resources.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-08-conclusion-final-project"><p>Content from <a href="08-conclusion-final-project.html">Wrap-up and Final Project</a></p>
<hr>
<p>Last updated on 2024-05-12 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/08-conclusion-final-project.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 11 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are the core concepts and techniques we’ve learned about NLP
and LLMs?</li>
<li>How can these techniques be applied to solve real-world
problems?</li>
<li>What are the future directions and opportunities in NLP?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>To be able to synthesize the key concepts from each episode.</li>
<li>To plan a path for further learning and exploration in NLP and
LLMs.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="takeaway-from-this-workshop"><h2 class="section-heading">8.1. Takeaway from This Workshop<a class="anchor" aria-label="anchor" href="#takeaway-from-this-workshop"></a>
</h2>
<hr class="half-width">
<p>We have covered a vast landscape of NLP, starting with the basics and
moving towards the intricacies of LLMs. Here is a brief recap to
illustrate our journey:</p>
<ul>
<li>
<strong>Text Preprocessing</strong>: Imagine cleaning a dataset of
tweets for sentiment analysis. We learned how to remove noise and
prepare the text for accurate classification.</li>
<li>
<strong>Text Analysis</strong>: Consider the task of extracting key
information from news articles. Techniques like Named Entity Recognition
helped us identify and categorize entities within the text.</li>
<li>
<strong>Word Embedding</strong>: We explored how words can be
converted into vectors, enabling us to capture semantic relationships,
as seen in the Word2Vec algorithm.</li>
<li>
<strong>Transformers and LLMs</strong>: We saw how transformers like
BERT and GPT can be fine-tuned for tasks such as summarizing medical
research papers and showcasing their power and flexibility.</li>
</ul>
<div id="quiz" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="quiz" class="callout-inner">
<h3 class="callout-title">Quiz<a class="anchor" aria-label="anchor" href="#quiz"></a>
</h3>
<div class="callout-content">
<p><strong>A)</strong> Stemming</p>
<p><strong>B)</strong> Word2Vec</p>
<p><strong>C)</strong> Text Preprocessing</p>
<p><strong>D)</strong> Part-of-Speech Tagging</p>
<p><strong>E)</strong> Stop-words Removal</p>
<p><strong>F)</strong> Transformers</p>
<p><strong>G)</strong> Bag of Words</p>
<p><strong>H)</strong> Tokenization</p>
<p><strong>I)</strong> BERT</p>
<p><strong>J)</strong> Lemmatization</p>
<p><strong>1. A statistical approach to modeling the meaning of words
based on their context.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>2. “A process of reducing words to their root form, enabling
the analysis of word frequency.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>3. An algorithm that uses neural networks to understand the
relationships and meanings in human language.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>4. A technique for identifying the parts of speech for each
word in a given sentence.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>5. A method for cleaning and preparing text data before
analysis.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>6. A library that provides tools for machine learning and
statistical modeling.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>7. A model that predicts the next word in a sentence based on
the words that come before it.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>8. A framework for building and training neural networks to
understand and generate human language.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>9. A technique that groups similar words together in vector
space.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>10. A method for removing commonly used words that carry
little meaning.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>A: 1</p>
</div>
</div>
</div>
</div>
<div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p><strong>Field of Interest</strong></p>
<p>Teamwork: Share insights on how NLP can be applied in your field of
interest.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p><strong><em>Environmental Science</em></strong></p>
<ul>
<li>NLP for Climate Change Research: How can NLP help in analyzing large
volumes of research papers on climate change to identify trends and gaps
in the literature?</li>
<li>Social Media Analysis for Environmental Campaigns: Discuss the use
of sentiment analysis to gauge public opinion on environmental
policies.</li>
<li>Automating Environmental Compliance: Share insights on how NLP can
streamline the process of checking compliance with environmental
regulations in corporate documents.</li>
</ul>
<p><strong><em>Education</em></strong></p>
<ul>
<li>Personalized Learning: Explore the potential of NLP in creating
personalized learning experiences by analyzing student feedback and
performance.</li>
<li>Content Summarization: Discuss the benefits of using NLP to
summarize educational content for quick revision.</li>
<li>Language Learning: Share thoughts on the role of NLP in developing
language learning applications that adapt to the learner’s proficiency
level.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="mini-project-using-an-llm" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="mini-project-using-an-llm" class="callout-inner">
<h3 class="callout-title">Mini-Project: Using an LLM<a class="anchor" aria-label="anchor" href="#mini-project-using-an-llm"></a>
</h3>
<div class="callout-content">
<p>Context Example: Environmental science and climate change Using
Hugging Face model distilbert-base-uncased and Few-Shot Prompting: To
improve the model’s performance in answering field-specific questions,
we will use few-shot prompting by providing examples of questions and
answers related to environmental topics.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co"># Initialize the question-answering pipeline with DistilBERT</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>qa_pipeline <span class="op">=</span> pipeline(<span class="st">'question-answering'</span>, model<span class="op">=</span><span class="st">'distilbert-base-uncased'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Few-shot prompting with examples</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>context <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="st">Question: What is the greenhouse effect?</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="st">Answer: The greenhouse effect is a natural process that warms the Earth's surface.</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="st">Question: How can we reduce carbon emissions?</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="st">Answer: We can reduce carbon emissions by using renewable energy sources, improving energy efficiency, and planting trees.</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="st">Question: What are the consequences of deforestation?</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="st">Answer: Deforestation can lead to loss of biodiversity, increased greenhouse gas emissions, and disruption of water cycles.</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="co"># User's field-specific question</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>user_question <span class="op">=</span> <span class="st">"What can individuals do to combat climate change?"</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="co"># Prepare the prompt for the model</span></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>prompt <span class="op">=</span> {</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>    <span class="st">'context'</span>: context,</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>    <span class="st">'question'</span>: user_question</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>}</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a><span class="co"># Get the answer from the model</span></span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>response <span class="op">=</span> qa_pipeline(prompt)</span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="bu">print</span>(response[<span class="st">'answer'</span>])</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>The model should provide a relevant answer based on the few-shot
examples provided. For instance, it might say: “Individuals can combat
climate change by reducing their carbon footprint, using less energy,
recycling, and supporting eco-friendly policies”.</p>
<p>In this challenge, we used the
<strong>distilbert-base-uncased</strong> model from Hugging Face’s
transformers library to create a question-answering system. Few-shot
prompting is employed to give the model context about environmental
topics, which helps it generate more accurate answers to user queries.
The <strong>qa_pipeline</strong> function is used to pass the prompt to
the model, which then processes the information and returns an answer to
the user’s question.</p>
<p>This mini-project showcases how LLMs can be fine-tuned to specific
fields of interest, providing valuable assistance in answering
domain-specific queries.</p>
</div>
</div>
</div>
</div>
</section><section id="further-resources"><h2 class="section-heading">8.2. Further Resources<a class="anchor" aria-label="anchor" href="#further-resources"></a>
</h2>
<hr class="half-width">
<p>For continued learning, here are detailed resources:</p>
<ul>
<li>
<em>Natural Language Processing Specialization (Coursera)</em>: A
series of courses that cover NLP foundations, algorithms, and how to
build NLP applications.</li>
<li>
<em>Stanford NLP Group</em>: Access to pioneering NLP research,
datasets, and tools like Stanford Parser and Stanford POS Tagger.</li>
<li>
<em>Hugging Face</em>: A platform for sharing and collaborating on
ML models, with a focus on democratizing NLP technologies.</li>
<li>
<em>Kaggle</em>: An online community for data scientists, offering
datasets, notebooks, and competitions to practice and improve your NLP
skills.</li>
</ul>
<p>Each resource is a gateway to further knowledge, community
engagement, and hands-on experience.</p>
</section><section id="feedback"><h2 class="section-heading">8.3. Feedback<a class="anchor" aria-label="anchor" href="#feedback"></a>
</h2>
<hr class="half-width">
<p>Please help us improve by answering the following survey
questions:</p>
<p><strong>1. How would you rate the overall quality of the
workshop?</strong></p>
<p>[ ] Excellent, [ ] Good, [ ] Average, [ ] Below Average, [ ] Poor</p>
<p><strong>2. Was the pace of the workshop appropriate?</strong></p>
<p>[ ] Too fast, [ ] Just right, [ ] Too slow</p>
<p><strong>3. How clear were the instructions and
explanations?</strong></p>
<p>[ ] Very clear, [ ] Clear, [ ] Somewhat clear, [ ] Not clear</p>
<p><strong>4. What was the most valuable part of the workshop for
you?</strong></p>
<p><strong>5. How can we improve the workshop for future
participants?</strong></p>
<p><em>Your feedback is crucial for us to evolve and enhance the
learning experience.</em></p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Various NLP techniques from preprocessing to advanced LLMs are
reviewed.</li>
<li>NLPs’ transformative potential provides real-world applications in
diverse fields.</li>
<li>Few-shot learning can enhance the performance of LLMs for specific
fields of research.</li>
<li>Valuable resources are highlighted for continued learning and
exploration in the field of NLP.</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/README.md" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/" class="external-link">Source</a></p>
				<p><a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:training@qcif.edu.au">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.4" class="external-link">sandpaper (0.16.4)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.5" class="external-link">pegboard (0.7.5)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.2" class="external-link">varnish (1.0.2)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/instructor/aio.html",
  "identifier": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/instructor/aio.html",
  "dateCreated": "2024-05-10",
  "dateModified": "2024-05-15",
  "datePublished": "2024-05-15"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

