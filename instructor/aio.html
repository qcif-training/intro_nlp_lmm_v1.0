<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Introduction to Natural Language Processing for Research: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="manifest" href="../site.webmanifest">
<link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="background-color: #FF4955; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #000">
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
            Pre-Alpha
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Introduction to Natural Language Processing for Research
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Introduction to Natural Language Processing for Research
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="../aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to Natural Language Processing for Research
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->
      
            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction to Natural Language Processing</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-text-preprocessing.html">2. Introduction to Text Preprocessing</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-text-analysis.html">3. Text Analysis</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-word-embedding.html">4. Word Embedding</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-transformers.html">5. Transformers for Natural Language Processing</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-llms.html">6. Large Language Models</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-domain-specific-llms.html">7. Domain-Specific LLMs</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="08-conclusion-final-project.html">8. Wrap-up and Final Project</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="../instructor/aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">
            
            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-introduction"><p>Content from <a href="01-introduction.html">Introduction to Natural Language Processing</a></p>
<hr>
<p>Last updated on 2024-05-10 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/01-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 10 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are some common research applications of NLP?</li>
<li>What are the basic concepts and terminology of NLP?</li>
<li>How can I use NLP in my research field?</li>
<li>How can I acquire data for NLP tasks?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Define natural language processing and its goals.</li>
<li>Identify main research applications and challenges of NLP.</li>
<li>Explain the basic concepts and terminology of NLP, such as tokens,
lemmas, and n-grams.</li>
<li>Use some popular datasets and libraries to acquire data for NLP
tasks.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="natural-language-processing-in-reseach"><h2 class="section-heading">Natural language processing in Reseach<a class="anchor" aria-label="anchor" href="#natural-language-processing-in-reseach"></a>
</h2>
<hr class="half-width">
<p>Natural Language Processing (NLP) is becoming a popular and robust
tool for a wide range of research projects. In this episode we embark on
a journey to explore the transformative power of NLP tools in the realm
of research.</p>
<p>It is tailored for researchers who are keen on harnessing the
capabilities of NLP to enhance and expedite their work. Whether you are
delving into text classification, extracting pivotal information,
discerning sentiments, summarizing extensive documents, translating
across languages, or developing sophisticated question-answering
systems, this session will lay the foundational knowledge you need to
leverage NLP effectively.</p>
<p>We will begin by delving into the Common Applications of NLP in
Research, showcasing how these tools are not just theoretical concepts
but practical instruments that drive forward today’s innovative research
projects. From analyzing public sentiment to extracting critical data
from a plethora of documents, NLP stands as a pillar in the modern
researcher’s toolkit.</p>
<p>Next, we’ll demystify the Basic Concepts and Terminology of NLP.
Understanding these fundamental terms is crucial, as they form the
building blocks of any NLP application. We’ll cover everything from the
basics of a corpus to the intricacies of transformers, ensuring you have
a solid grasp of the language used in NLP.</p>
<p>Finally, we’ll guide you through Data Acquisition: Dataset Libraries,
where you’ll learn about the treasure troves of data available at your
fingertips. We’ll compare different libraries and demonstrate how to
access and utilize these resources through hands-on examples.</p>
<p>By the end of this episode, you will not only understand the
significance of NLP in research but also be equipped with the knowledge
to start applying these tools to your own projects. Prepare to unlock
new potentials and streamline your research process with the power of
NLP!</p>
</section><section id="common-applications-of-nlp-in-research"><h2 class="section-heading">1.1. Common Applications of NLP in Research<a class="anchor" aria-label="anchor" href="#common-applications-of-nlp-in-research"></a>
</h2>
<hr class="half-width">
<p><strong>Sentiment Analysis</strong> is a powerful tool for
researchers, especially in fields like market research, political
science, and public health. It involves the computational identification
of opinions expressed in text, categorizing them as positive, negative,
or neutral. In market research for instance, sentiment analysis can be
applied to product reviews to gauge consumer satisfaction: a study could
analyze thousands of online reviews for a new smartphone model to
determine the overall public sentiment. This can help companies identify
areas of improvement or features that are well-received by
consumers.</p>
<p><strong>Information Extraction</strong> is crucial for quickly
gathering specific information from large datasets. It is used
extensively in legal research, medical research, and scientific studies
to extract entities and relationships from texts. In legal research for
example, information extraction can be used to sift through case law to
find precedents related to a particular legal issue. A researcher could
use NLP to extract instances of “negligence” from thousands of case
files, aiding in the preparation of legal arguments.</p>
<p><strong>Text Summarization</strong> helps researchers by providing
concise summaries of lengthy documents, such as research papers or
reports, allowing them to quickly understand the main points without
reading the entire text. In biomedical research, text summarization can
assist in literature reviews by providing summaries of research
articles. For example, a researcher could use an NLP model to summarize
articles on gene therapy, enabling them to quickly assimilate key
findings from a vast array of publications.</p>
<p><strong>Topic Modeling</strong> is used to uncover latent topics
within large volumes of text, which is particularly useful in fields
like sociology and history to identify trends and patterns in historical
documents or social media data. For example, in historical research,
topic modeling can reveal prevalent themes in primary source documents
from a particular era. A historian might use NLP to analyze newspapers
from the early 20th century to study public discourse around significant
events like World War I.</p>
<div class="section level3">
<h3 id="challenges-of-nlp">Challenges of NLP<a class="anchor" aria-label="anchor" href="#challenges-of-nlp"></a>
</h3>
<p>One of the significant challenges in NLP is dealing with the
ambiguity of language. Words or phrases can have multiple meanings, and
determining the correct one based on context can be difficult for NLP
systems. In a research paper discussing “bank erosion,” an NLP system
might confuse “bank” with a financial institution rather than the
geographical feature, leading to incorrect analysis.</p>
<p>This challenge leads to the fact that NLP systems often struggle with
contextual understanding which is crucial in text analysis tasks. This
can lead to misinterpretation of the meaning and sentiment of text. If a
research paper mentions “novel results,” an NLP system might interpret
“novel” as a literary work instead of “new” or “original,” which could
mislead the analysis of the paper’s contributions.</p>
</div>
<div class="section level3">
<h3 id="suggested-resources">Suggested Resources:<a class="anchor" aria-label="anchor" href="#suggested-resources"></a>
</h3>
<ul>
<li>Python’s Natural Language Toolkit (NLTK) for sentiment analysis</li>
<li>TextBlob, a library for processing textual data</li>
<li>Stanford NER for named entity recognition</li>
<li>spaCy, an open-source software library for advanced NLP</li>
<li>Sumy, a Python library for automatic summarization of text
documents</li>
<li>BERT-based models for extractive and abstractive summarization</li>
<li>Gensim for topic modeling and document similarity analysis</li>
<li>MALLET, a Java-based package for statistical natural language
processing</li>
</ul>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</section></section><section id="aio-02-text-preprocessing"><p>Content from <a href="02-text-preprocessing.html">Introduction to Text Preprocessing</a></p>
<hr>
<p>Last updated on 2024-05-10 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/02-text-preprocessing.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How much data do you need for Deep Learning?</li>
<li>Where can I find image data to train my model?</li>
<li>How do I plot image data in python?</li>
<li>How do I prepare image data for use in a convolutional neural
network (CNN)?</li>
<li>Know the difference between training, testing, and validation
datasets.</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Identify sources of image data.</li>
<li>Understand the properties of image data.</li>
<li>Write code to plot image data.</li>
<li>Prepare an image dataset to train a convolutional neural network
(CNN).</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="deep-learning-workflow"><h2 class="section-heading">Deep Learning Workflow<a class="anchor" aria-label="anchor" href="#deep-learning-workflow"></a>
</h2>
<hr class="half-width">
<p>Let’s start over from the beginning of our workflow.</p>
<div class="section level3">
<h3 id="step-1--formulate-outline-the-problem">Step 1. Formulate/ Outline the problem<a class="anchor" aria-label="anchor" href="#step-1--formulate-outline-the-problem"></a>
</h3>
<p>Firstly we must decide what it is we want our Deep Learning system to
do. This lesson is all about image classification and our aim is to put
an image into one of ten categories: airplane, automobile, bird, cat,
deer, dog, frog, horse, ship, or truck</p>
</div>
<div class="section level3">
<h3 id="step-2--identify-inputs-and-outputs">Step 2. Identify inputs and outputs<a class="anchor" aria-label="anchor" href="#step-2--identify-inputs-and-outputs"></a>
</h3>
<p>Next we identify the inputs and outputs of the neural network. In our
case, the data is images and the inputs could be the individual pixels
of the images.</p>
<p>We are performing a classification problem and we want to output one
category for each image.</p>
</div>
<div class="section level3">
<h3 id="step-3--prepare-data">Step 3. Prepare data<a class="anchor" aria-label="anchor" href="#step-3--prepare-data"></a>
</h3>
<p>Deep Learning requires extensive training using example data which
tells the network what output it should produce for a given input. In
this workshop, our network will be trained on a series of images and
told what they contain. Once the network is trained, it should be able
to take another image and correctly classify its contents.</p>
<p>Depending on your situation, you will prepare your own custom data
for training or use pre-existing data.</p>
<div id="challenge-how-much-data-do-you-need-for-deep-learning" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-how-much-data-do-you-need-for-deep-learning" class="callout-inner">
<h3 class="callout-title">CHALLENGE How much data do you need for Deep
Learning?<a class="anchor" aria-label="anchor" href="#challenge-how-much-data-do-you-need-for-deep-learning"></a>
</h3>
<div class="callout-content">
<p>The rise of Deep Learning is partially due to the increased
availability of very large datasets. But how much data do you actually
need to train a Deep Learning model?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Unfortunately, this question is not easy to answer. It depends, among
other things, on the complexity of the task (which you often do not know
beforehand), the quality of the available dataset and the complexity of
the network. For complex tasks with large neural networks, adding more
data often improves performance. However, this is also not a generic
truth: if the data you add is too similar to the data you already have,
it will not give much new information to the neural network.</p>
<p>In case you have too little data available to train a complex network
from scratch, it is sometimes possible to use a pretrained network that
was trained on a similar problem. Another trick is data augmentation,
where you expand the dataset with artificial data points that could be
real. An example of this is mirroring images when trying to classify
cats and dogs. An horizontally mirrored animal retains the label, but
exposes a different view.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="custom-image-data"><h2 class="section-heading">Custom image data<a class="anchor" aria-label="anchor" href="#custom-image-data"></a>
</h2>
<hr class="half-width">
<p>In some cases, you will create your own set of labelled images.</p>
<p>The steps to prepare your own custom image data include:</p>
<p><strong>Custom data i. Data collection and Labeling:</strong></p>
<p>For image classification the label applies to the entire image;
object detection requires bounding boxes around objects of interest, and
instance or semantic segmentation requires each pixel to be
labelled.</p>
<p>There are a number of open source software used to label your
dataset, including:</p>
<ul>
<li>(Visual Geometry Group) <a href="https://www.robots.ox.ac.uk/~vgg/software/via/" class="external-link">VGG Image
Annotator</a> (VIA)</li>
<li>
<a href="https://imagej.net/" class="external-link">ImageJ</a> can be extended with
plugins for annotation</li>
<li>
<a href="https://github.com/jsbroks/coco-annotator" class="external-link">COCO
Annotator</a> is designed specifically for creating annotations
compatible with Common Objects in Context (COCO) format</li>
</ul>
<p><strong>Custom data ii. Data preprocessing:</strong></p>
<p>This step involves various tasks to enhance the quality and
consistency of the data:</p>
<ul>
<li><p><strong>Resizing</strong>: Resize images to a consistent
resolution to ensure uniformity and reduce computational load.</p></li>
<li><p><strong>Augmentation</strong>: Apply random transformations
(e.g., rotations, flips, shifts) to create new variations of the same
image. This helps improve the model’s robustness and generalisation by
exposing it to more diverse data.</p></li>
<li><p><strong>Normalisation</strong>: Scale pixel values to a common
range, often between 0 and 1 or -1 and 1. Normalisation helps the model
converge faster during training.</p></li>
<li><p><strong>Label encoding</strong> is a technique used to represent
categorical data with numerical labels.</p></li>
<li><p><strong>Data Splitting</strong>: Split the data set into separate
parts to have one for training, one for evaluating the model’s
performance during training, and one reserved for the final evaluation
of the model’s performance.</p></li>
</ul>
<p>Before jumping into these specific preprocessing tasks, it’s
important to understand that images on a computer are stored as
numerical representations or simplified versions of the real world.
Therefore it’s essential to take some time to understand these numerical
abstractions.</p>
<div class="section level3">
<h3 id="pixels">Pixels<a class="anchor" aria-label="anchor" href="#pixels"></a>
</h3>
<p>Images on a computer are stored as rectangular arrays of hundreds,
thousands, or millions of discrete “picture elements,” otherwise known
as pixels. Each pixel can be thought of as a single square point of
coloured light.</p>
<p>For example, consider this image of a Jabiru, with a square area
designated by a red box:</p>
<figure><img src="../fig/02_Jabiru_TGS_marked.jpg" alt="Jabiru image that is 552 pixels wide and 573 pixels high. A red square around the neck region indicates the area to zoom in on." class="figure mx-auto d-block"></figure><p>Now, if we zoomed in close enough to the red box, the individual
pixels would stand out:</p>
<figure><img src="../fig/02_Jabiru_TGS_marked_zoom_enlarged.jpg" alt="zoomed in area of Jabiru where the individual pixels stand out" class="figure mx-auto d-block"></figure><p>Note each square in the enlarged image area (i.e. each pixel) is all
one colour, but each pixel can be a different colour from its
neighbours. Viewed from a distance, these pixels seem to blend together
to form the image.</p>
</div>
<div class="section level3">
<h3 id="working-with-pixels">Working with Pixels<a class="anchor" aria-label="anchor" href="#working-with-pixels"></a>
</h3>
<p>As noted, in practice, real world images will typically be made up of
a vast number of pixels, and each of these pixels will be one of
potentially millions of colours.</p>
<p>In python, an image can represented as a 2- or 3-dimensional array,
where each element corresponds to a pixel value in the image. In the
context of images, these arrays often have dimensions for height, width,
and colour channels (if applicable).</p>
<p>Let us start with the Jabiru image.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># load the required packages</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> keras.utils <span class="im">import</span> img_to_array</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras.utils <span class="im">import</span> load_img</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># specify the image path</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>new_img_path <span class="op">=</span> <span class="st">"../data/Jabiru_TGS.JPG"</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># read in the image with default arguments</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>new_img_pil <span class="op">=</span> load_img(new_img_path)</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="co"># check the image class and size</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Image class :'</span>, new_img_pil.__class__)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Image size'</span>, new_img_pil.size)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Image class : &lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt;
Image size (552, 573)</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="image-dimensions---resizing">Image Dimensions - Resizing<a class="anchor" aria-label="anchor" href="#image-dimensions---resizing"></a>
</h3>
<p>The new image has shape <code>(573, 552, 3)</code>, meaning it is
much larger in size, 573x552 pixels; a rectangle instead of a square;
and consists of three colour channels (RGB).</p>
<p>Recall from the introduction that our training data set consists of
50000 images of 32x32 pixels and three channels.</p>
<p>To reduce the computational load and ensure all of our images have a
uniform size, we need to choose an image resolution (or size in pixels)
and ensure all of the images we use are resized to that shape to be
consistent.</p>
<p>There are a couple of ways to do this in python but one way is to
specify the size you want using an argument to the
<code>load_img()</code> function from <code>keras.utils</code>.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># read in the image and specify the target size</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>new_img_pil_small <span class="op">=</span> load_img(new_img_path, target_size<span class="op">=</span>(<span class="dv">32</span>,<span class="dv">32</span>))</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># confirm the image class and size</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Resized image class :'</span>, new_img_pil_small.__class__)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Resized image size'</span>, new_img_pil_small.size) </span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Resized image class : &lt;class 'PIL.Image.Image'&gt;
Resized image size (32, 32)</code></pre>
</div>
<p>Of course, if there are a large number of images to preprocess you do
not want to copy and paste these steps for each image! Fortunately,
Keras has a solution: <a href="https://keras.io/api/data_loading/image/" class="external-link">tf.keras.utils.image_dataset_from_directory</a></p>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Python image libraries</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler1" aria-labelledby="headingSpoiler1">
<div class="accordion-body">
<p>Two of the most commonly used libraries for image representation and
manipulation are NumPy and Pillow (PIL). Additionally, when working with
deep learning frameworks like TensorFlow and PyTorch, images are often
represented as tensors within these frameworks.</p>
<ul>
<li>NumPy is a powerful library for numerical computing in Python. It
provides support for creating and manipulating arrays, which can be used
to represent images as multidimensional arrays.
<ul>
<li><code>import numpy as np</code></li>
</ul>
</li>
<li>The Pillow library provides functions to open, manipulate, and save
various image file formats. It represents images using its own Image
class.
<ul>
<li><code>from PIL import Image</code></li>
<li>
<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html" class="external-link">PIL
Image Module</a> documentation</li>
</ul>
</li>
<li>TensorFlow images are often represented as tensors that have
dimensions for batch size, height, width, and colour channels. This
framework provide tools to load, preprocess, and work with image data
seamlessly.
<ul>
<li><code>from tensorflow import keras</code></li>
<li>
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image" class="external-link">image
preprocessing</a> documentation</li>
<li>Note Keras image functions also use PIL</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="image-augmentation">Image augmentation<a class="anchor" aria-label="anchor" href="#image-augmentation"></a>
</h3>
<p>There are several ways to augment your data to increase the diversity
of the training data and improve model robustness.</p>
<ul>
<li>Geometric Transformations
<ul>
<li>rotation, scaling, zooming, cropping</li>
</ul>
</li>
<li>Flipping or Mirroring
<ul>
<li>some classes, like horse, have a different shape when facing left or
right and you want your model to recognize both</li>
</ul>
</li>
<li>Colour properties
<ul>
<li>brightness, contrast, or hue</li>
<li>these changes simulate variations in lighting conditions</li>
</ul>
</li>
</ul>
<p>We will not perform image augmentation in this lesson, but it is
important that you are aware of this type of data preparation because it
can make a big difference in your model’s ability to predict outside of
your training data.</p>
<p>Information about these operations are included in the Keras document
for <a href="https://keras.io/api/layers/preprocessing_layers/image_augmentation/" class="external-link">Image
augmentation layers</a>.</p>
</div>
<div class="section level3">
<h3 id="normalisation">Normalisation<a class="anchor" aria-label="anchor" href="#normalisation"></a>
</h3>
<p>Image RGB values are between 0 and 255. As input for neural networks,
it is better to have small input values. The process of converting the
RGB values to be between 0 and 1 is called
<strong>normalization</strong>.</p>
<p>Before we can normalize our image values we must convert the image to
an numpy array.</p>
<p>We introduced how to do this in <a href="01-introduction.html">Episode 01 Introduction to Deep Learning</a>
but what you may not have noticed is that the
<code>keras.datasets.cifar10.load_data</code> function did the
conversion for us whereas now we will do it ourselves.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># first convert the image into an array for normalization</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>new_img_arr <span class="op">=</span> img_to_array(new_img_pil_small)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># confirm the image class and shape</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Converted image class  :'</span>, new_img_arr.__class__)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Converted image shape'</span>, new_img_arr.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Converted image class  : &lt;class 'numpy.ndarray'&gt;
Converted image shape (32, 32, 3)</code></pre>
</div>
<p>Now that the image is an array, we can normalize the values. Let us
also investigate the image values before and after we normalize
them.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># inspect pixel values before and after normalisation</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="co"># extract the min, max, and mean pixel values BEFORE</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'BEFORE normalization'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Min pixel value '</span>, new_img_arr.<span class="bu">min</span>()) </span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Max pixel value '</span>, new_img_arr.<span class="bu">max</span>())</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Mean pixel value '</span>, new_img_arr.mean().<span class="bu">round</span>())</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="co"># normalize the RGB values to be between 0 and 1</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>new_img_arr_norm <span class="op">=</span> new_img_arr <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a><span class="co"># extract the min, max, and mean pixel values AFTER</span></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'AFTER normalization'</span>) </span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Min pixel value '</span>, new_img_arr_norm.<span class="bu">min</span>()) </span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Max pixel value '</span>, new_img_arr_norm.<span class="bu">max</span>())</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Mean pixel value '</span>, new_img_arr_norm.mean().<span class="bu">round</span>())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>BEFORE normalization
Min pixel value  0.0
Max pixel value  255.0
Mean pixel value  87.0
AFTER normalization
Min pixel value  0.0
Max pixel value  1.0
Mean pixel value  0.0</code></pre>
</div>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Why Normalize?</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler2" aria-labelledby="headingSpoiler2">
<div class="accordion-body">
<p>ChatGPT</p>
<p>Normalizing the RGB values to be between 0 and 1 is a common
pre-processing step in machine learning tasks, especially when dealing
with image data. This normalization has several benefits:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Numerical Stability</strong>: By scaling the RGB values
to a range between 0 and 1, you avoid potential numerical instability
issues that can arise when working with large values. Neural networks
and many other machine learning algorithms are sensitive to the scale of
input features, and normalizing helps to keep the values within a
manageable range.</p></li>
<li><p><strong>Faster Convergence</strong>: Normalizing the RGB values
often helps in faster convergence during the training process. Neural
networks and other optimization algorithms rely on gradient descent
techniques, and having inputs in a consistent range aids in smoother and
faster convergence.</p></li>
<li><p><strong>Equal Weightage for All Channels</strong>: In RGB images,
each channel (Red, Green, Blue) represents different colour intensities.
By normalizing to the range [0, 1], you ensure that each channel is
treated with equal weightage during training. This is important because
some machine learning algorithms could assign more importance to larger
values.</p></li>
<li><p><strong>Generalization</strong>: Normalization helps the model to
generalize better to unseen data. When the input features are in the
same range, the learned weights and biases can be more effectively
applied to new examples, making the model more robust.</p></li>
<li><p><strong>Compatibility</strong>: Many image-related libraries,
algorithms, and models expect pixel values to be in the range of [0, 1].
By normalizing the RGB values, you ensure compatibility and seamless
integration with these tools.</p></li>
</ol>
<p>The normalization process is typically done by dividing each RGB
value (ranging from 0 to 255) by 255, which scales the values to the
range [0, 1].</p>
<p>For example, if you have an RGB image with pixel values (100, 150,
200), after normalization, the pixel values would become (100/255,
150/255, 200/255) ≈ (0.39, 0.59, 0.78).</p>
<p>Remember that normalization is not always mandatory, and there could
be cases where other scaling techniques might be more suitable based on
the specific problem and data distribution. However, for most
image-related tasks in machine learning, normalizing RGB values to [0,
1] is a good starting point.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="one-hot-encoding">One-hot encoding<a class="anchor" aria-label="anchor" href="#one-hot-encoding"></a>
</h3>
<p>A neural network can only take numerical inputs and outputs, and
learns by calculating how “far away” the class predicted by the neural
network is from the true class. When the target (label) is categorical
data, or strings, it is very difficult to determine this “distance” or
error. Therefore we will transform this column into a more suitable
format. There are many ways to do this, however we will be using
<strong>one-hot encoding</strong>.</p>
<p>One-hot encoding is a technique to represent categorical data as
binary vectors, making it compatible with machine learning algorithms.
Each category becomes a separate column, and the presence or absence of
a category is indicated by 1s and 0s in the respective columns.</p>
<p>Let’s say you have a dataset with a “colour” column containing three
categories: yellow, orange, purple.</p>
<p>Table 1. Original Data.</p>
<table class="table">
<thead><tr class="header">
<th>colour</th>
<th align="right"></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>yellow</td>
<td align="right"><span class="emoji" data-emoji="yellow_square">🟨</span></td>
</tr>
<tr class="even">
<td>orange</td>
<td align="right"><span class="emoji" data-emoji="orange_square">🟧</span></td>
</tr>
<tr class="odd">
<td>purple</td>
<td align="right"><span class="emoji" data-emoji="purple_square">🟪</span></td>
</tr>
<tr class="even">
<td>yellow</td>
<td align="right"><span class="emoji" data-emoji="yellow_square">🟨</span></td>
</tr>
</tbody>
</table>
<p>Table 2. After One-Hot Encoding.</p>
<table class="table">
<thead><tr class="header">
<th>colour_yellow</th>
<th align="center">colour_orange</th>
<th align="right">colour_purple</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="center">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>0</td>
<td align="center">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>0</td>
<td align="center">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td>1</td>
<td align="center">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>The Keras function for one_hot encoding is called <a href="https://keras.io/api/utils/python_utils/#to_categorical-function" class="external-link">to_categorical</a>:</p>
<p><code>tf.keras.utils.to_categorical(y, num_classes=None, dtype="float32")</code></p>
<ul>
<li>
<code>y</code> is an array of class values to be converted into a
matrix (integers from 0 to num_classes - 1).</li>
<li>
<code>num_classes</code> is the total number of classes. If None,
this would be inferred as max(y) + 1.</li>
<li>
<code>dtype</code> is the data type expected by the input. Default:
‘float32’</li>
</ul>
</div>
<div class="section level3">
<h3 id="data-splitting">Data Splitting<a class="anchor" aria-label="anchor" href="#data-splitting"></a>
</h3>
<p>The typical practice in machine learning is to split your data into
two subsets: a <strong>training</strong> set and a <strong>test</strong>
set. This initial split separates the data you will use to train your
model from the data you will use to evaluate its performance.</p>
<p>After this initial split, you can choose to further split the
training set into a training set and a <strong>validation set</strong>.
This is often done when you are fine-tuning hyperparameters, selecting
the best model from a set of candidate models, or preventing
overfitting.</p>
<p>To split a dataset into training and test sets there is a very
convenient function from sklearn called <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" class="external-link">train_test_split</a>:</p>
<p><code>sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)</code></p>
<ul>
<li>The first two parameters are the dataset (X) and the corresponding
targets (y) (i.e. class labels).</li>
<li>Next is the named parameter <code>test_size</code>. This is the
fraction of the dataset used for testing and in this case
<code>0.2</code> means 20 per cent of the data will be used for
testing.</li>
<li>
<code>random_state</code> controls the shuffling of the dataset,
setting this value will reproduce the same results (assuming you give
the same integer) every time it is called.</li>
<li>
<code>shuffle</code> which can be either <code>True</code> or
<code>False</code>, it controls whether the order of the rows of the
dataset is shuffled before splitting. It defaults to
<code>True</code>.</li>
<li>
<code>stratify</code> is a more advanced parameter that controls how
the split is done. By setting it to <code>target</code> the train and
test sets the function will return will have roughly the same
proportions (with regards to the number of images of a certain class) as
the dataset.</li>
</ul>
</div>
</section><section id="pre-existing-image-data"><h2 class="section-heading">Pre-existing image data<a class="anchor" aria-label="anchor" href="#pre-existing-image-data"></a>
</h2>
<hr class="half-width">
<p>In other cases you will be able to download an image dataset that is
already labelled and can be used to classify a number of different
object like the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" class="external-link">CIFAR-10</a> dataset.
Other examples include:</p>
<ul>
<li>
<a href="https://en.wikipedia.org/wiki/MNIST_database" class="external-link">MNIST
database</a> - 60,000 training images of handwritten digits (0-9)</li>
<li>
<a href="https://www.image-net.org/" class="external-link">ImageNet</a> - 14 million
hand-annotated images indicating objects from more than 20,000
categories. ImageNet sponsors an <a href="https://www.image-net.org/challenges/LSVRC/#:~:text=The%20ImageNet%20Large%20Scale%20Visual,image%20classification%20at%20large%20scale." class="external-link">annual
software contest</a> where programs compete to achieve the highest
accuracy. When choosing a pretrained network, the winners of these sorts
of competitions are generally a good place to start.</li>
<li>
<a href="https://cocodataset.org/#home" class="external-link">MS COCO</a> - &gt;200,000
labelled images used for object detection, instance segmentation,
keypoint analysis, and captioning</li>
</ul>
<p>Where labelled data exists, in most cases the data provider or other
users will have created data-specific functions you can use to load the
data. We already did this in the introduction:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="co"># load the CIFAR-10 dataset included with the keras library</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>(train_images, train_labels), (test_images, test_labels) <span class="op">=</span> keras.datasets.cifar10.load_data()</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="co"># create a list of classnames associated with each CIFAR-10 label</span></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>class_names <span class="op">=</span> [<span class="st">'airplane'</span>, <span class="st">'automobile'</span>, <span class="st">'bird'</span>, <span class="st">'cat'</span>, <span class="st">'deer'</span>, <span class="st">'dog'</span>, <span class="st">'frog'</span>, <span class="st">'horse'</span>, <span class="st">'ship'</span>, <span class="st">'truck'</span>]</span></code></pre>
</div>
<p>In this instance the data is likely already prepared for use in a
CNN. However, it is always a good idea to first read any associated
documentation to find out what steps the data providers took to prepare
the images and second to take a closer at the images once loaded and
query their attributes.</p>
<p>In our case, we still want prepare the datset with these steps:</p>
<ul>
<li>normalise the image pixel values to be between 0 and 1</li>
<li>one-hot encode the training image labels</li>
<li>divide the data into <strong>training</strong>,
<strong>validation</strong>, and <strong>test</strong> subsets</li>
</ul>
<p>We performed these operations in <strong>Step 3. Prepare
data</strong> of the Introduction but let us create the function to
prepare the dataset again knowing what we know now.</p>
<div id="challenge-create-a-function-to-prepare-the-dataset" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-function-to-prepare-the-dataset" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a function to prepare the
dataset<a class="anchor" aria-label="anchor" href="#challenge-create-a-function-to-prepare-the-dataset"></a>
</h3>
<div class="callout-content">
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># create a function to prepare the dataset</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="kw">def</span> prepare_dataset(<span class="co">#blank#, #blank#):</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    </span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    <span class="co"># normalize the RGB values to be between 0 and 1</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>    <span class="co">#blank#</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>    <span class="co">#blank#</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>    </span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>    <span class="co"># one hot encode the training labels</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>    <span class="co">#blank#</span></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>    </span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>    <span class="co"># split the training data into training and validation set</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>    <span class="co">#blank#</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>    <span class="co">#blank#</span></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>    <span class="cf">return</span> <span class="co">#blank#</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="kw">def</span> prepare_dataset(train_images, train_labels):</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    </span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    <span class="co"># normalize the RGB values to be between 0 and 1</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>    train_images <span class="op">=</span> train_images <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    test_images <span class="op">=</span> train_labels <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>    </span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>    <span class="co"># one hot encode the training labels</span></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>    train_labels <span class="op">=</span> keras.utils.to_categorical(train_labels, <span class="bu">len</span>(class_names))</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>    </span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>    <span class="co"># split the training data into training and validation set</span></span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>    train_images, val_images, train_labels, val_labels <span class="op">=</span> train_test_split(</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>    train_images, train_labels, test_size <span class="op">=</span> <span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>    <span class="cf">return</span> train_images, val_images, train_labels, val_labels</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>Inspect the labels before and after one-hot encoding.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'train_labels before one hot encoding'</span>)</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(train_labels)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="co"># one-hot encode labels</span></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>train_labels <span class="op">=</span> keras.utils.to_categorical(train_labels, <span class="bu">len</span>(class_names))</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'train_labels after one hot encoding'</span>)</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a><span class="bu">print</span>(train_labels)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>train_labels before one hot encoding
[[6]
 [9]
 [9]
 ...
 [9]
 [1]
 [1]]

train_labels after one hot encoding
[[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 0. 0. 1.]
 ...
 [0. 0. 0. ... 0. 0. 1.]
 [0. 1. 0. ... 0. 0. 0.]
 [0. 1. 0. ... 0. 0. 0.]]</code></pre>
</div>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>WAIT I thought there were TEN classes!? Where is the rest of the
data?</p>
<p>The Spyder IDE uses the ‘…’ notation when it “hides” some of the data
for display purposes.</p>
<p>To view the entire array, go the Variable Explorer in the upper right
hand corner of your Spyder IDE and double click on the ‘train_labels’
object. This will open a new window that shows all of the columns.</p>
<figure><img src="../fig/02_spyder_onehot_train_labels_inFULL.png" alt="Screenshot of Spyder window displaying the entire train_labels array." class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
<div id="challenge-training-and-validation" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-training-and-validation" class="callout-inner">
<h3 class="callout-title">CHALLENGE Training and Validation<a class="anchor" aria-label="anchor" href="#challenge-training-and-validation"></a>
</h3>
<div class="callout-content">
<p>Inspect the training and validation sets we created.</p>
<p>How many samples does each set have and are the classes well
balanced?</p>
<p>Hint: Use <code>np.sum()</code> on the ’*_labels’ to find out if the
classes are well balanced.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>A. Training Set</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of training set images'</span>, train_images.shape[<span class="dv">0</span>])</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of images in each class:</span><span class="ch">\n</span><span class="st">'</span>, train_labels.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Number of training set images: 40000
Number of images in each class:
 [4027. 4021. 3970. 3977. 4067. 3985. 4004. 4006. 3983. 3960.]</code></pre>
</div>
<p>B. Validation Set (we can use the same code as the training set)</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of validation set images'</span>, val_images.shape[<span class="dv">0</span>])</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Nmber of images in each class:</span><span class="ch">\n</span><span class="st">'</span>, val_labels.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Number of validation set images: 10000
Nmber of images in each class:
 [ 973.  979. 1030. 1023.  933. 1015.  996.  994. 1017. 1040.]</code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Data Splitting Techniques</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler3" aria-labelledby="headingSpoiler3">
<div class="accordion-body">
<p>ChatGPT</p>
<p>Data is typically split into the training, validation, and test data
sets using a process called data splitting or data partitioning. There
are various methods to perform this split, and the choice of technique
depends on the specific problem, dataset size, and the nature of the
data. Here are some common approaches:</p>
<p><strong>Hold-Out Method:</strong></p>
<ul>
<li><p>In the hold-out method, the dataset is divided into two parts
initially: a training set and a test set.</p></li>
<li><p>The training set is used to train the model, and the test set is
kept completely separate to evaluate the model’s final
performance.</p></li>
<li><p>This method is straightforward and widely used when the dataset
is sufficiently large.</p></li>
</ul>
<p><strong>Train-Validation-Test Split:</strong></p>
<ul>
<li><p>The dataset is split into three parts: the training set, the
validation set, and the test set.</p></li>
<li><p>The training set is used to train the model, the validation set
is used to tune hyperparameters and prevent overfitting during training,
and the test set is used to assess the final model performance.</p></li>
<li><p>This method is commonly used when fine-tuning model
hyperparameters is necessary.</p></li>
</ul>
<p><strong>K-Fold Cross-Validation:</strong></p>
<ul>
<li><p>In k-fold cross-validation, the dataset is divided into k subsets
(folds) of roughly equal size.</p></li>
<li><p>The model is trained and evaluated k times, each time using a
different fold as the test set while the remaining k-1 folds are used as
the training set.</p></li>
<li><p>The final performance metric is calculated as the average of the
k evaluation results, providing a more robust estimate of model
performance.</p></li>
<li><p>This method is particularly useful when the dataset size is
limited, and it helps in better utilizing available data.</p></li>
</ul>
<p><strong>Stratified Sampling:</strong></p>
<ul>
<li><p>Stratified sampling is used when the dataset is imbalanced,
meaning some classes or categories are underrepresented.</p></li>
<li><p>The data is split in such a way that each subset (training,
validation, or test) maintains the same class distribution as the
original dataset.</p></li>
<li><p>This ensures all classes are well-represented in each subset,
which is important to avoid biased model evaluation.</p></li>
</ul>
<p>It’s important to note that the exact split ratios (e.g., 80-10-10 or
70-15-15) may vary depending on the problem, dataset size, and specific
requirements. Additionally, data splitting should be performed randomly
to avoid introducing any biases into the model training and evaluation
process.</p>
</div>
</div>
</div>
</div>
</section><section id="data-preprocessing-completed"><h2 class="section-heading">Data preprocessing completed!<a class="anchor" aria-label="anchor" href="#data-preprocessing-completed"></a>
</h2>
<hr class="half-width">
<p>We now have a function we can use throughout the lesson to preprocess
our data which means we are ready to learn how to build a CNN like we
used in the introduction.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Image datasets can be found online or created uniquely for your
research question.</li>
<li>Images consist of pixels arranged in a particular order.</li>
<li>Image data is usually preprocessed before use in a CNN for
efficiency, consistency, and robustness.</li>
<li>Input data generally consists of three sets: a training set used to
fit model parameters; a validation set used to evaluate the model fit on
training data; a test set used to evaluate the final model
performance.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-03-text-analysis"><p>Content from <a href="03-text-analysis.html">Text Analysis</a></p>
<hr>
<p>Last updated on 2024-05-12 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/03-text-analysis.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are text analysis methods?</li>
<li>How can I perform text analysis?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Define objectives associated with each one of the text analysis
techniques.</li>
<li>Implement named entity recognition, and topic modeling using Python
libraries and frameworks, such as NLTK, and Gensim.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="introduction-to-text-analysis"><h2 class="section-heading">3.1. Introduction to Text-Analysis<a class="anchor" aria-label="anchor" href="#introduction-to-text-analysis"></a>
</h2>
<hr class="half-width">
<p>In this episode, we will learn how to analyze text data for NLP
tasks. We will explore some common techniques and methods for text
analysis, such as named entity recognition, topic modeling, and text
summarization. We will use some popular libraries and frameworks, such
as spaCy, NLTK, and Gensim, to implement these techniques and
methods.</p>
<div id="discussion" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: What are some of the goals of text analysis for NLP tasks
in your research field (e.g. material science)? Think of some examples
of NLP tasks that require text analysis, such as literature review,
patent analysis, or material discovery. How does text analysis help to
achieve these goals?</p>
</div>
</div>
</div>
<div id="discussion-1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Name some of the common techniques in text analysis and
associating libraries. Briefly explain how they differ from each other
in terms of their objectives and required libraries.</p>
</div>
</div>
</div>
</section><section id="named-entity-recognition"><h2 class="section-heading">3.1. Named Entity Recognition<a class="anchor" aria-label="anchor" href="#named-entity-recognition"></a>
</h2>
<hr class="half-width">
<p>Named Entity Recognition is a process of identifying and classifying
key elements in text into predefined categories. The categories could be
names of persons, organizations, locations, expressions of times,
quantities, monetary values, percentages, etc. Next, let’s discuss how
it works.</p>
<div id="discussion-2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-2" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-2"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Discuss what tasks can be done with NER.</p>
<p>A: <em>NER can help with 1) categorizing resumes, 2) categorizing
customer feedback, 3) categorizing research papers, etc.</em></p>
</div>
</div>
</div>
<p>Using a text example from Wikipedia can help us to see how NER works.
Note that the spaCy library is a common framework here as well. Thus,
first, we make sure that the library is installed and imported:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>pip install spacy</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span></code></pre>
</div>
<p>Create an NLP model (<em>nlp</em>) and download the small English
model from spaCy that is suitable for general tasks.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.download(<span class="st">"en_core_web_sm"</span>)</span></code></pre>
</div>
<p>Create a variable to store your text and then apply the model to
process your text (text from <a href="https://en.wikipedia.org/wiki/Australian_Securities_Exchange" class="external-link">Wikipedia</a>):</p>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Text</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p>text = “Australian Shares Exchange Ltd (ASX) is an Australian public
company that operates Australia’s primary shares exchange, the
Australian Shares Exchange (sometimes referred to outside of Australia
as, or confused within Australia as, The Sydney Stock Exchange, a
separate entity). The ASX was formed on 1 April 1987, through
incorporation under legislation of the Australian Parliament as an
amalgamation of the six state securities exchanges, and merged with the
Sydney Futures Exchange in 2006. Today, ASX has an average daily
turnover of A$4.685 billion and a market capitalization of around A$1.6
trillion, making it one of the world’s top 20 listed exchange groups,
and the largest in the southern hemisphere. ASX Clear is the clearing
house for all shares, structured products, warrants and, ASX Equity
Derivatives.”</p>
</div>
</div>
</div>
</div>
<p>Use for loop to print all the named entities in the document:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>For ent <span class="kw">in</span> doc.ents</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>	Print(ent.text, ent.label_)</span></code></pre>
</div>
<p>The results will be:</p>
<pre><code>
output:

Australian Shares Exchange Ltd ORG
ASX ORG
Australian NORP
Australia GPE
the Australian Shares Exchange ORG
Australia GPE
Australia GPE
The Sydney Stock Exchange ORG
ASX ORG
1 April 1987 DATE
the Australian Parliament ORG
six CARDINAL
the Sydney Futures Exchange ORG
2006 DATE
Today DATE
ASX ORG
A$4.685 billion MONEY
around A$1.6 trillion MONEY
20 CARDINAL
</code></pre>
<div id="challenge" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge"></a>
</h3>
<div class="callout-content">
<p>Q: How can you interpret the labels in the output?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>A: You can use the following code to get information about each one
of the labels. For example, from we want to know what GPE represents
here. We can use <em>explain()</em> to get the required information:
spacy.explain(‘GPE’)</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>spacy.explain(‘GPE’)</span></code></pre>
</div>
<pre><code>Output: ‘Countries, cities, states’
</code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What Else Might We Use A Spoiler For?</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" aria-labelledby="headingSpoiler2" data-bs-parent="#accordionSpoiler2">
<div class="accordion-body">

</div>
</div>
</div>
</div>
<div id="chemistry-joke" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="chemistry-joke" class="callout-inner">
<h3 class="callout-title">Chemistry Joke<a class="anchor" aria-label="anchor" href="#chemistry-joke"></a>
</h3>
<div class="callout-content">
<p>Q: If you aren’t part of the solution, then what are you?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>A: part of the precipitate</p>
</div>
</div>
</div>
</div>
</section><section id="neural-networks"><h2 class="section-heading">Neural Networks<a class="anchor" aria-label="anchor" href="#neural-networks"></a>
</h2>
<hr class="half-width">
<p>A <strong>neural network</strong> is an artificial intelligence
technique loosely based on the way neurons in the brain work.</p>
<div class="section level3">
<h3 id="a-single-nueron">A single nueron<a class="anchor" aria-label="anchor" href="#a-single-nueron"></a>
</h3>
<p>A neural network consists of connected computational units called
<strong>neurons</strong>. Each neuron will:</p>
<ul>
<li>Take one or more inputs (<span class="math inline">\(x_1, x_2,
...\)</span>), e.g., input data expressed as floating point
numbers.</li>
<li>Conduct three main operations most of the time:
<ul>
<li>Calculate the weighted sum of the inputs where ($w_1, w_2, … $)
indicate weights</li>
<li>Add an extra constant weight (i.e. a bias term) to this weighted
sum</li>
<li>Apply a non-linear function to the output so far (using a predefined
activation function such as the ReLU function)</li>
</ul>
</li>
<li>Return one output value, again a floating point number.</li>
</ul>
<p>One example equation to calculate the output for a neuron is: <span class="math inline">\(output=ReLU(∑i(xi∗wi)+bias)\)</span></p>
<figure><img src="../fig/03_neuron.png" alt="diagram of a single neuron taking multiple inputs and their associated weights in and then applying an activation function to predict a single output" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="combining-multiple-neurons-into-a-network">Combining multiple neurons into a network<a class="anchor" aria-label="anchor" href="#combining-multiple-neurons-into-a-network"></a>
</h3>
<p>Multiple neurons can be joined together by connecting the output of
one to the input of another. These connections are associated with
weights that determine the ‘strength’ of the connection, and the weights
are adjusted during training. In this way, the combination of neurons
and connections describe a computational graph, an example can be seen
in the image below.</p>
<p>In most neural networks neurons are aggregated into layers. Signals
travel from the input layer to the output layer, possibly through one or
more intermediate layers called hidden layers. The image below
illustrates an example of a neural network with three layers, each
circle is a neuron, each line is an edge and the arrows indicate the
direction data moves in.</p>
<figure><img src="../fig/03_neural_net.png" alt="diagram of a neural with four neurons taking multiple inputs and their weights and predicting multiple outputs" class="figure mx-auto d-block"><div class="figcaption">The image above is by Glosser.ca, <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">CC BY-SA 3.0</a>,
via Wikimedia Commons, <a href="https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg" class="external-link">original
source</a>
</div>
</figure><p>Neural networks aren’t a new technique, they have been around since
the late 1940s. But until around 2010 neural networks tended to be quite
small, consisting of only 10s or perhaps 100s of neurons. This limited
them to only solving quite basic problems. Around 2010 improvements in
computing power and the algorithms for training the networks made much
larger and more powerful networks practical. These are known as deep
neural networks or Deep Learning.</p>
</div>
</section><section id="convolutional-neural-networks"><h2 class="section-heading">Convolutional Neural Networks<a class="anchor" aria-label="anchor" href="#convolutional-neural-networks"></a>
</h2>
<hr class="half-width">
<p>A convolutional neural network (CNN) is a type of artificial neural
network (ANN) most commonly applied to analyze visual imagery. They are
designed to recognize the spatial structure of images when extracting
features.</p>
<div class="section level3">
<h3 id="step-4--build-an-architecture-from-scratch-or-choose-a-pretrained-model">Step 4. Build an architecture from scratch or choose a pretrained
model<a class="anchor" aria-label="anchor" href="#step-4--build-an-architecture-from-scratch-or-choose-a-pretrained-model"></a>
</h3>
<p>Let us explore how to build a neural network from scratch. Although
this sounds like a daunting task, with Keras it is surprisingly
straightforward. With Keras you compose a neural network by creating
layers and linking them together.</p>
</div>
<div class="section level3">
<h3 id="parts-of-a-neural-network">Parts of a neural network<a class="anchor" aria-label="anchor" href="#parts-of-a-neural-network"></a>
</h3>
<p>There are three main components of a neural network:</p>
<ul>
<li>CNN Part 1. Input Layer</li>
<li>CNN Part 2. Hidden Layers</li>
<li>CNN Part 3. Output Layer</li>
</ul>
<p>The output from each layer becomes the input to the next layer.</p>
<div class="section level4">
<h4 id="cnn-part-1--input-layer">CNN Part 1. Input Layer<a class="anchor" aria-label="anchor" href="#cnn-part-1--input-layer"></a>
</h4>
<p>The Input in Keras gets special treatment when images are used. Keras
automatically calculates the number of inputs and outputs a specific
layer needs and therefore how many edges need to be created. This means
we must let Keras know how big our input is going to be. We do this by
instantiating a <code>keras.Input</code> class and pass it a tuple to
indicate the dimensionality of the input data.</p>
<p>The input layer is created with the <code>tf.keras.Input</code>
function and its first parameter is the expected shape of the input:</p>
<pre><code><span><span class="fu">keras.Input</span><span class="op">(</span>shape<span class="op">=</span><span class="va">None</span>, batch_size<span class="op">=</span><span class="va">None</span>, dtype<span class="op">=</span><span class="va">None</span>, sparse<span class="op">=</span><span class="va">None</span>, batch_shape<span class="op">=</span><span class="va">None</span>, name<span class="op">=</span><span class="va">None</span>, tensor<span class="op">=</span><span class="va">None</span><span class="op">)</span></span></code></pre>
<p>In our case, the shape of an image is defined by its pixel dimensions
and number of channels:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># recall the shape of the images in our dataset</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="bu">print</span>(train_images.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(40000, 32, 32, 3) # number of images, image width in pixels, image height in pixels, number of channels (RGB)</code></pre>
</div>
<div id="challenge-create-the-input-layer-for-our-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-the-input-layer-for-our-network" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create the input layer for our
network<a class="anchor" aria-label="anchor" href="#challenge-create-the-input-layer-for-our-network"></a>
</h3>
<div class="callout-content">
<p>Hint 1: Specify shape argument only and use defaults for the rest.
Hint 2: The shape of our input dataset includes the total number of
images. We want to take a slice of the shape for a single individual
image to use an input.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>    <span class="co"># CNN Part 1</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    <span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>   inputs_intro <span class="op">=</span> keras.Input(<span class="co">#blank#)</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>    # CNN Part 1
    # Input layer of 32x32 images with three channels (RGB)
   inputs_intro = keras.Input(shape=train_images.shape[1:])</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="cnn-part-2--hidden-layers">CNN Part 2. Hidden Layers<a class="anchor" aria-label="anchor" href="#cnn-part-2--hidden-layers"></a>
</h4>
<p>The next component consists of the so-called hidden layers of the
network. The reason they are referred to as hidden is because the true
values of their nodes are unknown.</p>
<p>In a CNN, the hidden layers typically consist of convolutional,
pooling, reshaping (e.g., Flatten), and dense layers.</p>
<p>Check out the <a href="https://keras.io/api/layers/" class="external-link">Layers API</a>
section of the Keras documentation for each layer type and its
parameters.</p>
<div class="section level5">
<h5 id="convolutional-layers">
<strong>Convolutional Layers</strong><a class="anchor" aria-label="anchor" href="#convolutional-layers"></a>
</h5>
<p>A <strong>convolutional</strong> layer is a fundamental building
block in a CNN designed for processing structured grid data, such as
images. It applies convolution operations to input data using learnable
filters or kernels, extracting local patterns and features (e.g. edges,
corners). These filters enable the network to capture hierarchical
representations of visual information, allowing for effective feature
learning.</p>
<p>To find the particular features of an image, CNNs make use of a
concept from image processing that precedes Deep Learning.</p>
<p>A <strong>convolution matrix</strong>, or <strong>kernel</strong>, is
a matrix transformation that we ‘slide’ over the image to calculate
features at each position of the image. For each pixel, we calculate the
matrix product between the kernel and the pixel with its surroundings.
Here is one example of a 3x3 kernel used to detect edges:</p>
<pre><code>[[-1, -1, -1],
 [0,   0,  0]
 [1,   1,  1]]</code></pre>
<p>This kernel will give a high value to a pixel if it is on a
horizontal border between dark and light areas.</p>
<p>In the following image, the effect of such a kernel on the values of
a single-channel image stands out. The red cell in the output matrix is
the result of multiplying and summing the values of the red square in
the input, and the kernel. Applying this kernel to a real image
demonstrates it does indeed detect horizontal edges.</p>
<figure><img src="../fig/03_conv_matrix.png" alt="6x5 input matrix representing a single colour channel image being multipled by a 3x3 kernel to produce a 4x4 output matrix to detect horizonal edges in an image " class="figure mx-auto d-block"></figure><figure><img src="../fig/03_conv_image.png" alt="single colour channel image of a cat multiplied by a 3x3 kernel to produce an image of a cat where the edges  stand out" class="figure mx-auto d-block"></figure><p>There are several types of convolutional layers available in Keras
depending on your application. We use the two-dimensional layer
typically used for images:</p>
<pre><code>keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding="valid", activation=None, **kwargs)</code></pre>
<p>We want to create a Conv2D layer with 16 filters, a 3x3 kernel size,
and the ‘relu’ activation function.</p>
<div id="challenge-create-a-2d-convolutional-layer-for-our-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-2d-convolutional-layer-for-our-network" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a 2D convolutional layer for
our network<a class="anchor" aria-label="anchor" href="#challenge-create-a-2d-convolutional-layer-for-our-network"></a>
</h3>
<div class="callout-content">
<p>Hint 1: The input to each layer is the output of the previous
layer.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>    <span class="co"># CNN Part 2</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>    <span class="co"># Convolutional layer with 16 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>    x_intro <span class="op">=</span> keras.layers.Conv2D(filters<span class="op">=</span><span class="co">#blank#, kernel_size=#blank#, activation=#blank#)(#blank#)</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>    # CNN Part 2
    # Convolutional layer with 16 filters, 3x3 kernel size, and ReLU activation
    x_intro = keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(inputs_intro)</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>The instantiation here has three parameters and a seemingly strange
combination of parentheses, so let us break it down.</p>
<ul>
<li>The first parameter is the number of filters in this layer. This is
one of the hyperparameters of our system and should be chosen carefully.
<ul>
<li>Good practice is to start with a relatively small number of filters
in the first layer to prevent overfitting.</li>
<li>Choosing a number of filters as a power of two (e.g., 32, 64, 128)
is common.</li>
</ul>
</li>
<li>The second parameter is the kernel size which we already discussed.
Smaller kernels are often used to capture fine-grained features and
odd-sized filters are preferred because they have a centre pixel which
helps maintain spatial symmetry during convolutions.</li>
<li>The third parameter is the activation function to use.
<ul>
<li>Here we choose <strong>relu</strong> which is one of the most
commonly used in deep neural networks that is proven to work well.</li>
<li>We will discuss activation functions later in <strong>Step 9. Tune
hyperparameters</strong> but to satisfy your curiosity,
<code>ReLU</code> stands for Rectified Linear Unit (ReLU).</li>
</ul>
</li>
<li>Next is an extra set of parenthenses with inputs in them that means
after an instance of the Conv2D layer is created, it can be called as if
it was a function. This tells the Conv2D layer to connect the layer
passed as a parameter, in this case the inputs.</li>
<li>Finally, we store a reference so we can pass it to the next
layer.</li>
</ul>
<div id="playing-with-convolutions" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="playing-with-convolutions" class="callout-inner">
<h3 class="callout-title">Playing with convolutions<a class="anchor" aria-label="anchor" href="#playing-with-convolutions"></a>
</h3>
<div class="callout-content">
<p>Convolutions applied to images can be hard to grasp at first.
Fortunately, there are resources out there that enable users to
interactively play around with images and convolutions:</p>
<ul>
<li><p><a href="https://setosa.io/ev/image-kernels/" class="external-link">Image kernels
explained</a> illustrates how different convolutions can achieve certain
effects on an image, like sharpening and blurring.</p></li>
<li><p>The <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" class="external-link">convolutional
neural network cheat sheet</a> provides animated examples of the
different components of convolutional neural nets.</p></li>
</ul>
</div>
</div>
</div>
<div id="challenge-border-pixels" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-border-pixels" class="callout-inner">
<h3 class="callout-title">CHALLENGE Border pixels<a class="anchor" aria-label="anchor" href="#challenge-border-pixels"></a>
</h3>
<div class="callout-content">
<p>What do you think happens to the border pixels when applying a
convolution?</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">Show me the solution</h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<p>There are different ways of dealing with border pixels.</p>
<ul>
<li>You can ignore them, which means your output image is slightly
smaller then your input.</li>
<li>It is also possible to ‘pad’ the borders, e.g., with the same value
or with zeros, so that the convolution can also be applied to the border
pixels. In that case, the output image will have the same size as the
input image.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level5">
<h5 id="pooling-layers">
<strong>Pooling Layers</strong><a class="anchor" aria-label="anchor" href="#pooling-layers"></a>
</h5>
<p>The convolutional layers are often intertwined with
<strong>Pooling</strong> layers. As opposed to the convolutional layer
used in feature extraction, the pooling layer alters the dimensions of
the image and reduces it by a scaling factor effectively decreasing the
resolution of your picture.</p>
<p>The rationale behind this is that higher layers of the network should
focus on higher-level features of the image. By introducing a pooling
layer, the subsequent convolutional layer has a broader ‘view’ on the
original image.</p>
<p>Similar to convolutional layers, Keras offers several pooling layers
and one used for images (2D spatial data):</p>
<pre><code>keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid", data_format=None, name=None, **kwargs)</code></pre>
<p>We want to create a pooling layer with input window sized 2,2.</p>
<div id="challenge-create-a-pooling-layer-for-our-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-pooling-layer-for-our-network" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a Pooling layer for our
network<a class="anchor" aria-label="anchor" href="#challenge-create-a-pooling-layer-for-our-network"></a>
</h3>
<div class="callout-content">
<p>Hint 1: The input to each layer is the output of the previous
layer.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>    <span class="co"># Pooling layer with input window sized 2,2</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>    x_intro <span class="op">=</span> keras.layers.MaxPooling2D(<span class="co">#blank#)(#blank#)</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6">Show me the solution</h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" aria-labelledby="headingSolution6" data-bs-parent="#accordionSolution6">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>    # Pooling layer with input window sized 2,2
    x_intro = keras.layers.MaxPooling2D(pool_size=(2, 2))(x_intro)</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>The instantiation here has a single parameter, pool_size.</p>
<p>The function downsamples the input along its spatial dimensions
(height and width) by taking the <strong>maximum</strong> value over an
input window (of size defined by pool_size) for each channel of the
input. By taking the maximum instead of the average, the most prominent
features in the window are emphasized.</p>
<p>A 2x2 pooling size reduces the width and height of the input by a
factor of 2. Empirically, a 2x2 pooling size has been found to work well
in various for image classification tasks and also strikes a balance
between down-sampling for computational efficiency and retaining
important spatial information.</p>
</div>
<div class="section level5">
<h5 id="dense-layers">
<strong>Dense layers</strong><a class="anchor" aria-label="anchor" href="#dense-layers"></a>
</h5>
<p>A <strong>dense</strong> layer has a number of neurons, which is a
parameter you choose when you create the layer. When connecting the
layer to its input and output layers every neuron in the dense layer
gets an edge (i.e. connection) to <strong>all</strong> of the input
neurons and <strong>all</strong> of the output neurons.</p>
<figure><img src="../fig/03-neural_network_sketch_dense.png" alt="diagram of a neural network with multiple inputs feeding into to two seperate dense layers with connections between all the inputs and outputs" class="figure mx-auto d-block"></figure><p>This layer is called fully connected, because all input neurons are
taken into account by each output neuron. It aggregates global
information about the features learned in previous layers to make a
decision about the class of the input.</p>
<p>In Keras, a densely-connected layer is defined:</p>
<pre><code>keras.layers.Dense(units, activation=None, **kwargs)</code></pre>
<p>Units in this case refer to the number of neurons.</p>
<p>The choice of how many neurons to specify is often determined through
experimentation and can impact the performance of our CNN. Too few
neurons may not capture complex patterns in the data but too many
neurons may lead to overfitting.</p>
<p>We will choose 64 for our dense layer and ‘relu’ activation.</p>
<div id="challenge-create-a-dense-layer-for-our-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-dense-layer-for-our-network" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a Dense layer for our
network<a class="anchor" aria-label="anchor" href="#challenge-create-a-dense-layer-for-our-network"></a>
</h3>
<div class="callout-content">
<p>Hint 1: The input to each layer is the output of the previous
layer.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>    <span class="co"># Dense layer with 64 neurons and ReLU activation</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>    x_intro <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="co">#hidden#, activation=#hidden#)(#hidden#)</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution7" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution7" aria-expanded="false" aria-controls="collapseSolution7">
  <h4 class="accordion-header" id="headingSolution7">Show me the solution</h4>
</button>
<div id="collapseSolution7" class="accordion-collapse collapse" aria-labelledby="headingSolution7" data-bs-parent="#accordionSolution7">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span>    <span class="co"># Dense layer with 64 neurons and ReLU activation</span></span>
<span>    <span class="va">x_intro</span> <span class="op">=</span> <span class="fu">keras.layers.Dense</span><span class="op">(</span><span class="fl">64</span>, activation<span class="op">=</span><span class="st">'relu'</span><span class="op">)</span><span class="op">(</span><span class="va">x_intro</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level5">
<h5 id="reshaping-layers-flatten">
<strong>Reshaping Layers: Flatten</strong><a class="anchor" aria-label="anchor" href="#reshaping-layers-flatten"></a>
</h5>
<p>The next type of hidden layer used in our introductory model is a
type of reshaping layer defined in Keras by the
<code>tf.keras.layers.Flatten</code> class. It is necessary when
transitioning from convolutional and pooling layers to fully connected
layers.</p>
<pre><code>keras.layers.Flatten(data_format=None, **kwargs)</code></pre>
<p>The <strong>Flatten</strong> layer converts the output of the
previous layer into a single one-dimensional vector that can be used as
input for a dense layer.</p>
<div id="challenge-create-a-flatten-layer-for-our-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-flatten-layer-for-our-network" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a Flatten layer for our
network<a class="anchor" aria-label="anchor" href="#challenge-create-a-flatten-layer-for-our-network"></a>
</h3>
<div class="callout-content">
<p>Hint 1: The input to each layer is the output of the previous
layer.</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>    <span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>    x_intro <span class="op">=</span> keras.layers.Flatten()(<span class="op">=</span><span class="co">#hidden#)</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution8" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution8" aria-expanded="false" aria-controls="collapseSolution8">
  <h4 class="accordion-header" id="headingSolution8">Show me the solution</h4>
</button>
<div id="collapseSolution8" class="accordion-collapse collapse" aria-labelledby="headingSolution8" data-bs-parent="#accordionSolution8">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span>    <span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span>    <span class="va">x_intro</span> <span class="op">=</span> <span class="fu">keras.layers.Flatten</span><span class="op">(</span><span class="op">)</span><span class="op">(</span><span class="va">x_intro</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What does Flatten mean exactly?</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" aria-labelledby="headingSpoiler3" data-bs-parent="#accordionSpoiler3">
<div class="accordion-body">
<p>A flatten layer function is typically used to transform the
two-dimensional arrays (matrices) generated by the convolutional and
pooling layers into a one-dimensional array. This is necessary when
transitioning from the convolutional/pooling layers to the fully
connected layers, which require one-dimensional input.</p>
<p>During the convolutional and pooling operations, a neural network
extracts features from the input images, resulting in multiple feature
maps, each represented by a matrix. These feature maps capture different
aspects of the input image, such as edges, textures, or patterns.
However, to feed these features into a fully connected layer for
classification or regression tasks, they must be a single vector.</p>
<p>The flatten layer takes each element from the feature maps and
arranges them into a single long vector, concatenating them along a
single dimension. This transformation preserves the spatial
relationships between the features in the original image while providing
a suitable format for the fully connected layers to process.</p>
</div>
</div>
</div>
</div>
<div id="is-one-layer-of-each-type-enough" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="is-one-layer-of-each-type-enough" class="callout-inner">
<h3 class="callout-title">Is one layer of each type enough?<a class="anchor" aria-label="anchor" href="#is-one-layer-of-each-type-enough"></a>
</h3>
<div class="callout-content">
<p>Not for complex data!</p>
<p>A typical architecture for image classification is likely to include
at least one convolutional layer, one pooling layer, one or more dense
layers, and possibly a flatten layer.</p>
<p>Convolutional and Pooling layers are often used together in multipe
sets to capture a wider range of features and learn more complex
representations of the input data. Using this technique, the network can
learn a hierarchical representation of features, where simple features
detected in early layers are combined to form more complex features in
deeper layers.</p>
<p>There isn’t a strict rule of thumb for the number of sets of
convolutional and pooling layers to start with, however, there are some
guidelines.</p>
<p>We are starting with a relatively small and simple architecture
because we are limited in time and computational resources. A simple CNN
with one or two sets of convolutional and pooling layers can still
achieve decent results for many tasks but for your network you will
experiment with different architectures.</p>
</div>
</div>
</div>
<div id="challenge-using-the-four-layers-above-create-a-hidden-layer-architecture-that-contains" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-using-the-four-layers-above-create-a-hidden-layer-architecture-that-contains" class="callout-inner">
<h3 class="callout-title">CHALLENGE Using the four layers above, create
a hidden layer architecture that contains:<a class="anchor" aria-label="anchor" href="#challenge-using-the-four-layers-above-create-a-hidden-layer-architecture-that-contains"></a>
</h3>
<div class="callout-content">
<ul>
<li>2 sets of Conv2D and Pooling layers, with 16 and 32 filters
respectively</li>
<li>1 Flatten layer</li>
<li>1 Dense layer with</li>
</ul>
<p>Hint 1: The input to each layer is the output of the previous
layer.</p>
</div>
</div>
</div>
<div id="accordionSolution9" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution9" aria-expanded="false" aria-controls="collapseSolution9">
  <h4 class="accordion-header" id="headingSolution9">Show me the solution</h4>
</button>
<div id="collapseSolution9" class="accordion-collapse collapse" aria-labelledby="headingSolution9" data-bs-parent="#accordionSolution9">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>    # CNN Part 2
    # Convolutional layer with 16 filters, 3x3 kernel size, and ReLU activation
    x_intro = keras.layers.Conv2D(16, (3, 3), activation='relu')(inputs_intro)
    # Pooling layer with input window sized 2,2
    x_intro = keras.layers.MaxPooling2D((2, 2))(x_intro)
    # Second Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation
    x_intro = keras.layers.Conv2D(32, (3, 3), activation='relu')(x_intro)
    # Second Pooling layer with input window sized 2,2
    x_intro = keras.layers.MaxPooling2D((2, 2))(x_intro)
    # Flatten layer to convert 2D feature maps into a 1D vector
    x_intro = keras.layers.Flatten()(x_intro)
    # Dense layer with 64 neurons and ReLU activation
    x_intro = keras.layers.Dense(64, activation='relu')(x_intro)</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="cnn-part-3--output-layer">CNN Part 3. Output Layer<a class="anchor" aria-label="anchor" href="#cnn-part-3--output-layer"></a>
</h4>
<p>Recall for the outputs we asked ourselves what we want to identify
from the data. If we are performing a classification problem, then
typically we have one output for each potential class.</p>
<p>In traditional CNN architectures, a dense layer is typically used as
the final layer for classification. This dense layer receives the
flattened feature maps from the preceding convolutional and pooling
layers and outputs the final class probabilities or regression
values.</p>
<p>For multiclass data, the <code>softmax</code> activation is used
instead of <code>relu</code> because it helps the computer give each
option (class) a likelihood score, and the scores add up to 100 per
cent. This way, it’s easier to pick the one the computer thinks is most
probable.</p>
<div id="challenge-create-an-output-layer-for-our-network-using-a-dense-layer" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-an-output-layer-for-our-network-using-a-dense-layer" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create an Output layer for our
network using a Dense layer<a class="anchor" aria-label="anchor" href="#challenge-create-an-output-layer-for-our-network-using-a-dense-layer"></a>
</h3>
<div class="callout-content">
<p>Hint 1: The input to each layer is the output of the previous layer.
Hint 2: The units (neurons) should be the same as number of classes as
our dataset. Hint 3: Use softmax activation.</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>    <span class="co"># CNN Part 3</span></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>    <span class="co"># Output layer with 10 units (one for each class) and softmax activation</span></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>    outputs_intro <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="co">#hidden#, activation=#hidden#)(#hidden#)</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution10" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution10" aria-expanded="false" aria-controls="collapseSolution10">
  <h4 class="accordion-header" id="headingSolution10">Show me the solution</h4>
</button>
<div id="collapseSolution10" class="accordion-collapse collapse" aria-labelledby="headingSolution10" data-bs-parent="#accordionSolution10">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span>    <span class="co"># CNN Part 3</span></span>
<span>    <span class="co"># Output layer with 10 units (one for each class) and softmax activation</span></span>
<span>    <span class="va">outputs_intro</span> <span class="op">=</span> <span class="fu">keras.layers.Dense</span><span class="op">(</span><span class="fl">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span><span class="op">)</span><span class="op">(</span><span class="va">x_intro</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section><section id="putting-it-all-together"><h2 class="section-heading">Putting it all together<a class="anchor" aria-label="anchor" href="#putting-it-all-together"></a>
</h2>
<hr class="half-width">
<div id="challenge-create-a-function-that-defines-a-cnn-using-the-input-hidden-and-output-layers-in-previous-challenges." class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-function-that-defines-a-cnn-using-the-input-hidden-and-output-layers-in-previous-challenges." class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a function that defines a CNN
using the input, hidden, and output layers in previous challenges.<a class="anchor" aria-label="anchor" href="#challenge-create-a-function-that-defines-a-cnn-using-the-input-hidden-and-output-layers-in-previous-challenges."></a>
</h3>
<div class="callout-content">
<p>Hint 1: The input to each layer is the output of the previous layer.
Hint 2: The units (neurons) should be the same as number of classes as
our dataset. Hint 3: Use softmax activation.</p>
</div>
</div>
</div>
<div id="accordionSolution11" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution11" aria-expanded="false" aria-controls="collapseSolution11">
  <h4 class="accordion-header" id="headingSolution11">Show me the solution</h4>
</button>
<div id="collapseSolution11" class="accordion-collapse collapse" aria-labelledby="headingSolution11" data-bs-parent="#accordionSolution11">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span>    <span class="co"># CNN Part 3</span></span>
<span>    <span class="co"># Output layer with 10 units (one for each class) and softmax activation</span></span>
<span>    <span class="va">outputs_intro</span> <span class="op">=</span> <span class="fu">keras.layers.Dense</span><span class="op">(</span><span class="fl">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span><span class="op">)</span><span class="op">(</span><span class="va">x_intro</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="kw">def</span> create_model_intro():</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a>    </span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>    <span class="co"># CNN Part 1</span></span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>    <span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>    inputs_intro <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a>    </span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a>    <span class="co"># CNN Part 2</span></span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a>    <span class="co"># Convolutional layer with 16 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a>    x_intro <span class="op">=</span> keras.layers.Conv2D(<span class="dv">16</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_intro)</span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a>    <span class="co"># Pooling layer with input window sized 2,2</span></span>
<span id="cb29-11"><a href="#cb29-11" tabindex="-1"></a>    x_intro <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_intro)</span>
<span id="cb29-12"><a href="#cb29-12" tabindex="-1"></a>    <span class="co"># Second Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb29-13"><a href="#cb29-13" tabindex="-1"></a>    x_intro <span class="op">=</span> keras.layers.Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_intro)</span>
<span id="cb29-14"><a href="#cb29-14" tabindex="-1"></a>    <span class="co"># Second Pooling layer with input window sized 2,2</span></span>
<span id="cb29-15"><a href="#cb29-15" tabindex="-1"></a>    x_intro <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_intro)</span>
<span id="cb29-16"><a href="#cb29-16" tabindex="-1"></a>    <span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb29-17"><a href="#cb29-17" tabindex="-1"></a>    x_intro <span class="op">=</span> keras.layers.Flatten()(x_intro)</span>
<span id="cb29-18"><a href="#cb29-18" tabindex="-1"></a>    <span class="co"># Dense layer with 64 neurons and ReLU activation</span></span>
<span id="cb29-19"><a href="#cb29-19" tabindex="-1"></a>    x_intro <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x_intro)</span>
<span id="cb29-20"><a href="#cb29-20" tabindex="-1"></a>    </span>
<span id="cb29-21"><a href="#cb29-21" tabindex="-1"></a>    <span class="co"># CNN Part 3</span></span>
<span id="cb29-22"><a href="#cb29-22" tabindex="-1"></a>    <span class="co"># Output layer with 10 units (one for each class) and softmax activation</span></span>
<span id="cb29-23"><a href="#cb29-23" tabindex="-1"></a>    outputs_intro <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x_intro)</span>
<span id="cb29-24"><a href="#cb29-24" tabindex="-1"></a>    </span>
<span id="cb29-25"><a href="#cb29-25" tabindex="-1"></a>    <span class="co"># create the model</span></span>
<span id="cb29-26"><a href="#cb29-26" tabindex="-1"></a>    model_intro <span class="op">=</span> keras.Model(inputs <span class="op">=</span> inputs_intro, </span>
<span id="cb29-27"><a href="#cb29-27" tabindex="-1"></a>                              outputs <span class="op">=</span> outputs_intro, </span>
<span id="cb29-28"><a href="#cb29-28" tabindex="-1"></a>                              name <span class="op">=</span> <span class="st">"cifar_model_intro"</span>)</span>
<span id="cb29-29"><a href="#cb29-29" tabindex="-1"></a>    </span>
<span id="cb29-30"><a href="#cb29-30" tabindex="-1"></a>    <span class="cf">return</span> model_intro</span></code></pre>
</div>
<p>Use the function you created to create the introduction model and
view a summary of it’s structure.</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="co"># create the introduction model</span></span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a>model_intro <span class="op">=</span> create_model_intro()</span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a><span class="co"># view model summary</span></span>
<span id="cb30-5"><a href="#cb30-5" tabindex="-1"></a>model_intro.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "cifar_model_intro"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 32, 32, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 30, 30, 16)        448       
                                                                 
 max_pooling2d (MaxPooling2  (None, 15, 15, 16)        0         
 D)                                                              
                                                                 
 conv2d_1 (Conv2D)           (None, 13, 13, 32)        4640      
                                                                 
 max_pooling2d_1 (MaxPoolin  (None, 6, 6, 32)          0         
 g2D)                                                            
                                                                 
 flatten (Flatten)           (None, 1152)              0         
                                                                 
 dense (Dense)               (None, 64)                73792     
                                                                 
 dense_1 (Dense)             (None, 10)                650       
                                                                 
=================================================================
Total params: 79530 (310.66 KB)
Trainable params: 79530 (310.66 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
<div id="how-to-choose-an-architecture" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="how-to-choose-an-architecture" class="callout-inner">
<h3 class="callout-title">How to choose an architecture?<a class="anchor" aria-label="anchor" href="#how-to-choose-an-architecture"></a>
</h3>
<div class="callout-content">
<p>Even for this neural network, we had to make a choice on the number
of hidden neurons. Other choices to be made are the number of layers and
type of layers. You might wonder how you should make these architectural
choices. Unfortunately, there are no clear rules to follow here, and it
often boils down to a lot of trial and error. However, it is recommended
to explore what others have done with similar datasets and problems.
Another best practice is to start with a relatively simple architecture.
Once running start to add layers and tweak the network to test if
performance increases.</p>
</div>
</div>
</div>
</section><section id="we-have-a-model-now-what"><h2 class="section-heading">We have a model now what?<a class="anchor" aria-label="anchor" href="#we-have-a-model-now-what"></a>
</h2>
<hr class="half-width">
<p>This CNN should be able to run with the CIFAR-10 dataset and provide
reasonable results for basic classification tasks. However, do keep in
mind this model is relatively simple, and its performance may not be as
high as more complex architectures. The reason it’s called deep learning
is because in most cases, the more layers we have, i.e. the deeper and
more sophisticated CNN architecture we use, the better the
performance.</p>
<p>How can we tell? We can inspect a couple metrics produced during the
training process to detect whether our model is underfitting or
overfitting. To do that, we continue with the next steps in our Deep
Learning workflow, <strong>Step 5. Choose a loss function and
optimizer</strong> and <strong>Step 6. Train model</strong>.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Artificial neural networks (ANN) are a machine learning technique
based on a model inspired by groups of neurons in the brain.</li>
<li>Convolution neural networks (CNN) are a type of ANN designed for
image classification and object detection.</li>
<li>The number of filters corresponds to the number of distinct features
the layer is learning to recognise whereas the kernel size determines
the level of features being captured.</li>
<li>A CNN can consist of many types of layers including convolutional,
pooling, flatten, and dense (fully connected) layers</li>
<li>Convolutional layers are responsible for learning features from the
input data.</li>
<li>Pooling layers are often used to reduce the spatial dimensions of
the data.</li>
<li>The flatten layer is used to convert the multi-dimensional output of
the convolutional and pooling layers into a flat vector.</li>
<li>Dense layers are responsible for combining features learned by the
previous layers to perform the final classification.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-04-word-embedding"><p>Content from <a href="04-word-embedding.html">Word Embedding</a></p>
<hr>
<p>Last updated on 2024-05-10 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/04-word-embedding.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 16 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a vector space in the context of NLP?</li>
<li>How can I visualize vector space in a 2D model?</li>
<li>How can I use embeddings and how do embeddings capture the meaning
of words?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Be able to explain vector space and how it is related to text
analysis.</li>
<li>Identify the tools required for text embeddings.</li>
<li>To explore the Word2Vec algorithm and its advantages over
traditional models.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/c96878f9-4c4a-49d7-9ec8-346d75663b76" alt="image" class="figure"><a href="https://carpentries-incubator.github.io/python-text-analysis/06-lsa/index.html" class="external-link">source</a></p>
<section id="introduction-to-vector-space-embeddings"><h2 class="section-heading">4.1. Introduction to Vector Space &amp; Embeddings:<a class="anchor" aria-label="anchor" href="#introduction-to-vector-space-embeddings"></a>
</h2>
<hr class="half-width">
<p>We have discussed how tokenization works and how it is important in
text analysis, however, this is not the whole story of preprocessing.
For conducting robust and reliable text analysis with NLP models,
vectorization and embedding are required after tokenization. To
understand this concept, we first talk about vector space.</p>
<p>Vector space models represent text data as vectors, which can be used
in various machine learning algorithms. Embeddings are dense vectors
that capture the semantic meanings of words based on their context.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/e22fea16-a593-4c75-988b-0ba81588f98b" alt="embedding_2" class="figure"><a href="https://www.deeplearning.ai/resources/natural-language-processing/" class="external-link">source</a></p>
<div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Discuss how tokenization affects the representation of text
in vector space models. Consider the impact of ignoring common words
(stop words) and the importance of word order.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p><img src="../fig/embedding_3.png" class="figure"><a href="https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037" class="external-link">source</a></p>
<p>A: Ignoring stop words might lead to loss of some contextual
information but can also reduce noise. Preserving word order can be
crucial for understanding the meaning, especially in languages with
flexible syntax.</p>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Tell me MORE!</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler1" aria-labelledby="headingSpoiler1">
<div class="accordion-body">
<p>Tokenization is a fundamental step in the processing of text for
vector space models. It involves breaking down a string of text into
individual units, or “tokens,” which typically represent words or
phrases. Here’s how tokenization impacts the representation of text in
vector space models:</p>
<ul>
<li>
<em>Granularity</em>: Tokenization determines the granularity of
text representation. Finer granularity (e.g., splitting on punctuation)
can capture more nuances but may increase the dimensionality of the
vector space.</li>
<li>
<em>Dimensionality</em>: Each unique token becomes a dimension in
the vector space. The choice of tokenization can significantly affect
the number of dimensions, with potential implications for computational
efficiency and the “curse of dimensionality.”</li>
<li>
<em>Semantic Meaning</em>: Proper tokenization ensures that
semantically significant units are captured as tokens, which is crucial
for the model to understand the meaning of the text.</li>
</ul>
<p>Ignoring common words, or “stop words,” can also have a significant
impact:</p>
<p><em>Noise Reduction</em>: Stop words are often filtered out to reduce
noise since they usually don’t carry important meaning and are highly
frequent (e.g., “the,” “is,” “at”).</p>
<p><em>Focus on Content Words</em>: By removing stop words, the model
can focus on content words that carry the core semantic meaning,
potentially improving the performance of tasks like information
retrieval or topic modeling.</p>
<p><em>Computational Efficiency</em>: Ignoring stop words reduces the
dimensionality of the vector space, which can make computations more
efficient.</p>
<p>The importance of word order is another critical aspect:</p>
<p><em>Contextual Meaning</em>: Word order is essential for capturing
the syntactic structure and meaning of a sentence. Traditional
bag-of-words models ignore word order, which can lead to a loss of
contextual meaning.</p>
<p><em>Phrase Identification</em>: Preserving word order allows for the
identification of multi-word expressions and phrases that have distinct
meanings from their constituent words.</p>
<p><em>Word Embeddings</em>: Advanced models like word embeddings (e.g.,
Word2Vec) and contextual embeddings (e.g., BERT) can capture word order
to some extent, leading to a more nuanced understanding of text
semantics.</p>
<p>In summary, tokenization, the treatment of stop words, and the
consideration of word order are all crucial factors that influence how
text is represented in vector space models, affecting both the quality
of the representation and the performance of downstream tasks.</p>
</div>
</div>
</div>
</div>
<div id="tokenization-vs.-vectorization-vs.-embedding" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="tokenization-vs.-vectorization-vs.-embedding" class="callout-inner">
<h3 class="callout-title">Tokenization Vs. Vectorization Vs.
Embedding<a class="anchor" aria-label="anchor" href="#tokenization-vs.-vectorization-vs.-embedding"></a>
</h3>
<div class="callout-content">
<p>Initially, <strong>tokenization</strong> breaks down text into
discrete elements, or tokens, which can include words, phrases, symbols,
and even punctuation, each represented by a unique numerical identifier.
These tokens are then mapped to <strong>vectors</strong> of real numbers
within an n-dimensional space, a process that is part of
<strong>embedding</strong>. During model training, these vectors are
adjusted to reflect the semantic similarities between tokens,
positioning those with similar meanings closer together in the embedding
space. This allows the model to grasp the nuances of language and
transforms raw text into a format that machine learning algorithms can
interpret, paving the way for advanced text analysis and
understanding.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/5a669f97-1bce-4466-ba6b-2b5e792124f0" alt="image" class="figure"><a href="https://geoffrey-geofe.medium.com/tokenization-vs-embedding-understanding-the-differences-and-their-importance-in-nlp-b62718b5964a" class="external-link">source</a></p>
</div>
</div>
</div>
</section><section id="bag-of-words-tf-idf"><h2 class="section-heading">4.2. Bag of Words &amp; TF-IDF:<a class="anchor" aria-label="anchor" href="#bag-of-words-tf-idf"></a>
</h2>
<hr class="half-width">
<p>Feature extraction in machine learning involves creating numerical
features that describe a document’s relationship to its corpus.
Traditional methods like Bag-of-Words and TF-IDF count words or n-grams,
with the latter assigning weights based on a word’s importance,
calculated by Term Frequency (TF) and Inverse Document Frequency (IDF).
TF measures a word’s importance within a document, while IDF assesses
its rarity across the corpus.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/2b3b5f37-667b-4fe8-bc31-e6798b6e2b61" alt="image" class="figure"><a href="https://www.deeplearning.ai/resources/natural-language-processing/" class="external-link">source</a></p>
<p>The product of TF and IDF gives the TF-IDF score, which balances a
word’s frequency in a document against its commonness in the corpus.
This approach helps to highlight significant words while diminishing the
impact of commonly used words like “the” or “a.”</p>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div>Instructor Note</h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" data-bs-parent="#accordionInstructor1" aria-labelledby="headingInstructor1">
<div class="accordion-body">
<ul>
<li>
<strong>BoW</strong> “encodes the total number of times a document
uses each word in the associated corpus through the
CounterVectorizer.”</li>
<li>
<strong>TF-IDF</strong> “creates features for each document based on
how often each word shows up in a document versus the entire
corpus.</li>
<li><a href="https://www.deeplearning.ai/resources/natural-language-processing/" class="external-link">source</a></li>
</ul>
</div>
</div>
</div>
</div>
<div id="discussion-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Discuss how each method represents the importance of words
and the potential impact on sentiment analysis.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>A: To compare the Bag of Words (BoW) and Term Frequency-Inverse
Document Frequency (TF-IDF) methods in representing text data and their
implications for sentiment analysis.</p>
<p>Data Collection: Gather a corpus of product reviews. For this
activity, let’s assume we have a list of reviews stored in a variable
called reviews. Clean the text data by removing punctuation, converting
to lowercase, and possibly removing stop words. Use a vectorizer to
convert the reviews into a BoW representation.</p>
<p>Discuss how BoW represents the frequency of words without considering
the context or rarity across documents. Use a vectorizer to convert the
same reviews into a TF-IDF representation. Discuss how TF-IDF represents
the importance of words by considering both the term frequency and how
unique the word is across all documents.</p>
</div>
</div>
</div>
</div>
<div id="teamwork" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="teamwork" class="callout-inner">
<h3 class="callout-title">Teamwork<a class="anchor" aria-label="anchor" href="#teamwork"></a>
</h3>
<div class="callout-content">
<p>Sentiment Analysis Implications:</p>
<p>Analyze a corpus of product reviews using both BoW and TF-IDF.
Consider how the lack of context in BoW might affect sentiment analysis.
Evaluate whether TF-IDF’s emphasis on unique words improves the model’s
ability to understand sentiment.</p>
<p>Share Findings: Groups should present their findings, highlighting
the strengths and weaknesses of each method.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer, TfidfVectorizer</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># Sample corpus of product reviews</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>reviews <span class="op">=</span> [</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="st">"Great product, really loved it!"</span>,</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="st">"Bad quality, totally disappointed."</span>,</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="st">"Decent product for the price."</span>,</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="st">"Excellent quality, will buy again!"</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>]</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># Initialize the CountVectorizer for BoW</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>bow_vectorizer <span class="op">=</span> CountVectorizer(stop_words<span class="op">=</span><span class="st">'english'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co"># Fit and transform the reviews</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>bow_matrix <span class="op">=</span> bow_vectorizer.fit_transform(reviews)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="co"># Display the BoW matrix</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bag of Words Matrix:"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="bu">print</span>(bow_matrix.toarray())</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="co"># Initialize the TfidfVectorizer for TF-IDF</span></span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>tfidf_vectorizer <span class="op">=</span> TfidfVectorizer(stop_words<span class="op">=</span><span class="st">'english'</span>)</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a><span class="co"># Fit and transform the reviews</span></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>tfidf_matrix <span class="op">=</span> tfidf_vectorizer.fit_transform(reviews)</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="co"># Display the TF-IDF matrix</span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">TF-IDF Matrix:"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a><span class="bu">print</span>(tfidf_matrix.toarray())</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>The BoW matrix shows the frequency of each word in the reviews,
disregarding context and word importance. The TF-IDF matrix shows the
weighted importance of words, giving less weight to common words and
more to unique ones.</p>
<p>In sentiment analysis, BoW might misinterpret sentiments due to
ignoring context, while TF-IDF might capture nuances better by
emphasizing words that are significant in a particular review.</p>
<p>By comparing BoW and TF-IDF, participants can gain insights into how
each method processes text data and their potential impact on NLP tasks
like sentiment analysis. This activity encourages critical thinking
about feature representation in machine learning models.</p>
</section><section id="word2vec-algorithm"><h2 class="section-heading">4.3. Word2Vec Algorithm:<a class="anchor" aria-label="anchor" href="#word2vec-algorithm"></a>
</h2>
<hr class="half-width">
<p>More advanced techniques like Word2Vec and GLoVE, as well as feature
learning during neural network training, have also been developed to
improve feature extraction.</p>
<p>Word2Vec uses neural networks to learn word associations from large
text corpora. It has two architectures: Skip-Gram and Continuous
Bag-of-Words (CBOW).</p>
<p>After training, it discards the final layer and outputs word
embeddings that capture context. These embeddings capture the context of
words, making similar contexts yield similar embeddings. Post-data
preprocessing, these numerical features can be used in various NLP
models for tasks like classification or named entity recognition.</p>
<p>Now let’s see how this framework can be used in practice. First
import required libraries: Start by importing necessary libraries like
gensim for Word2Vec and nltk for tokenization. Next, prepare the data:
Tokenize your text data into words.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># Sample text</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Tokenization splits text into words. Embeddings capture semantic meaning."</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># Tokenize the text</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>tokens <span class="op">=</span> word_tokenize(text.lower())</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a></span></code></pre>
</div>
<p>Now train the model: Use the Word2Vec class from gensim to train your
model on the tokenized sentences.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># Sample text</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Tokenization splits text into words. Embeddings capture semantic meaning."</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co"># Tokenize the text</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>tokens <span class="op">=</span> word_tokenize(text.lower())</span></code></pre>
</div>
<p>Retrieve Vectors: After training, use the model to get vectors for
words of interest.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="co"># Display the vector</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="bu">print</span>(vector_embeddings)</span></code></pre>
</div>
<p>The code tokenizes the sample text, trains a Word2Vec model, and
retrieves the vector for the word ‘embeddings’.</p>
<p>The resulting vector is a 50-dimensional representation of
‘embeddings’, capturing its context within the sample text. This vector
can then be used in various NLP tasks to represent the semantic meaning
of the word ‘embeddings’.</p>
<p>By understanding the roles of tokenization and embedding, we can
better prepare text data for complex NLP tasks and build models that
more accurately interpret human language.</p>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What is GLoVE?</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler2" aria-labelledby="headingSpoiler2">
<div class="accordion-body">
<p>Global Vectors for Word Representation (GLoVE)</p>
<p>GLoVE is a model for learning word embeddings, which are
representations of words in the form of high-dimensional vectors. Unlike
Word2Vec, which uses a neural network to learn word embeddings from
local context information, GLoVE is designed to capture both global
statistics and local context. Here’s how GLoVE stands out:</p>
<ul>
<li>Matrix Factorization: GLoVE uses matrix factorization on a word
co-occurrence matrix that reflects how often each word appears in the
context of every other word within a large corpus.</li>
<li>Global Word-Word Co-Occurrence: It focuses on word-to-word
co-occurrence globally across the entire corpus, rather than just within
a local context window as in Word2Vec.</li>
<li>Weighting Function: GLoVE employs a weighting function that helps to
address the disparity in word co-occurrence frequencies, giving less
weight to rare and frequent co-occurrences.</li>
</ul>
<p>The main difference between GLoVE and Word2Vec is that GLoVE is built
on the idea that word meanings can be derived from their co-occurrence
probabilities with other words, and hence it incorporates global corpus
statistics, whereas Word2Vec relies more on local context information.
This allows GLoVE to effectively capture both the semantic and syntactic
relationships between words, making it powerful for various natural
language processing tasks.</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Tokenization is crucial for converting text into a format usable by
machine learning models.</li>
<li>BoW and TF-IDF are fundamental techniques for feature extraction in
NLP.</li>
<li>Word2Vec and GloVE generate embeddings that encapsulate word
meanings based on context and co-occurrence, respectively.</li>
<li>Understanding these concepts is essential for building effective NLP
models that can interpret and process human language.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-05-transformers"><p>Content from <a href="05-transformers.html">Transformers for Natural Language Processing</a></p>
<hr>
<p>Last updated on 2024-05-12 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/05-transformers.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do Transformers work?</li>
<li>How can I use Transformers for text analysis?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>To be able to describe Transformers’ architecture.</li>
<li>To be able to implement sentiment analysis, and text summarization
using transformers.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Transformers have revolutionized the field of NLP since their
introduction by the Google team in 2017. Unlike previous models that
processed text sequentially, Transformers use an attention mechanism to
process all words at once, allowing them to capture context more
effectively. This parallel processing capability enables Transformers to
handle long-range dependencies and understand the nuances of language
better than their predecessors. For now, try to recognize the building
blocks of the general structure of a transformer</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/7ec8bb01-aa7e-4378-af45-ae28fbe8916b" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><section id="introduction-to-artificial-neural-networks"><h2 class="section-heading">5.1. Introduction to Artificial Neural Networks<a class="anchor" aria-label="anchor" href="#introduction-to-artificial-neural-networks"></a>
</h2>
<hr class="half-width">
<p>To understand how Transformers work we also need to learn about
artificial neural networks (ANNs). Imagine a neural network as a team of
workers in a factory. Each worker (neuron) has a specific task
(processing information), and they pass their work along to the next
person in line until the final product (output) is created.</p>
<p>Just like a well-organized assembly line, a neural network processes
information in stages, with each neuron contributing to the final
result.</p>
<div id="activity" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Take a look at the architecture of a simple ANN below.
Identify the underlying layers and components of this ANN and add the
correct name label to each one.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/445b963e-caf1-451d-8edc-e686f8950ae5" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/523924b5-055b-4125-8bce-aa2be2b38ca8" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What is Multilayer Perceptron then?</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler1" aria-labelledby="headingSpoiler1">
<div class="accordion-body">
<p>In the context of machine learning, a multilayer perceptron (MLP) is
indeed a fully connected multi-layer neural network and is a classic
example of a feedforward artificial neural network (ANN). It typically
includes an input layer, one or more hidden layers, and an output layer.
When an MLP has more than one hidden layer, it can be considered a deep
ANN, part of a broader category known as deep learning.</p>
</div>
</div>
</div>
</div>
<div id="summation-and-activation-function" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="summation-and-activation-function" class="callout-inner">
<h3 class="callout-title">Summation and Activation Function<a class="anchor" aria-label="anchor" href="#summation-and-activation-function"></a>
</h3>
<div class="callout-content">
<p>If we zoom into a neuron in the hidden layer, we can see the
mathematical operations (weights summation and activation function). An
input is transformed at each hidden layer node through a process that
multiplies the input (x_i) by learned weights (w_i), adds a bias (b),
and then applies an activation function to determine the node’s output.
This output is either passed on to the next layer or contributes to the
final output of the network. Essentially, each node performs a small
calculation that, when combined with the operations of other nodes,
allows the network to process complex patterns and data.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/d1006506-ac54-43bc-b6e9-5181ca98be36" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What happens next? How to optimize an ANN?</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler2" aria-labelledby="headingSpoiler2">
<div class="accordion-body">
<p>Backpropagation is an algorithmic cornerstone in the training of
ANNs, serving as a method for optimizing weights and biases through
gradient descent. Conceptually, it is akin to an iterative refinement
process where the network’s output error is propagated backward, layer
by layer, using the chain rule of calculus. This backward flow of error
information allows for the computation of gradients, which inform the
magnitude and direction of adjustments to be made to the network’s
parameters. The objective is to iteratively reduce the differences
between the predicted output and the actual target values. This
systematic adjustment of parameters, guided by error gradients,
incrementally leads to a more accurate ANN model.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/5ae4d845-c3d2-42df-87f0-e928be9ba64b" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
</div>
<div id="challenge" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge"></a>
</h3>
<div class="callout-content">
<p>Teamwork: When we talk about ANNs, we also consider their parameters.
But what are the parameters? Draw a small neural network with 3
following layers: x1</p>
<ul>
<li>Input Layer: 3 neurons</li>
<li>Hidden Layer: 4 neurons</li>
<li>Output Layer: 1 neurons</li>
</ul>
<ol style="list-style-type: decimal">
<li>Connect each neuron in the input layer to every neuron in the hidden
layer (next layer). How many connections (weights) do we have?</li>
<li>Now, add a bias for each neuron in the hidden layer. How many biases
do we have?</li>
<li>Repeat the process for the hidden layer to the output layer.</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/144a8f7d-c1ac-4b57-8688-b5280cabd59c" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><ul>
<li>(3 { neurons} x 4 { neurons} + 4{ biases}) = 16</li>
<li>(4 { neurons} x 1 { neurons} + 1{ biases}) = 5</li>
<li>Total parameters for this network: (16 + 5 = 21)</li>
</ul>
</div>
</div>
</div>
</div>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-1"></a>
</h3>
<div class="callout-content">
<p>Q: Add another hidden layer with 4 neurons to the previous ANN and
calculate the number of parameters.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/39820eb0-7959-4ed1-bdfc-69131ab1a834" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>We would add: - (4 * 4) weights from the first to the second hidden
layer - (4) biases for the new hidden layer - (4 * 1) weights from the
second hidden layer to the output layer (we already counted the biases
for the output layer)</p>
<p>That’s an additional (16 + 4 = 20) parameters, bringing our total to
(21 + 20 = 41) parameters.</p>
</div>
</div>
</div>
</div>
</section><section id="transformers"><h2 class="section-heading">5.2. Transformers<a class="anchor" aria-label="anchor" href="#transformers"></a>
</h2>
<hr class="half-width">
<p>As mentioned in the introduction, Most of the recent NLP models are
built based on Transformers. Building on our understanding of ANNs,
let’s explore the architecture of transformers. Transformers consist of
several key components that work together to process and generate
data.</p>
<div id="activity-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity-1" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: We go back to the first figure of this episode. In the
simplified schematic below, write the function of each component in the
allocated textbox:</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/e94c677a-53c5-4da3-87ac-e45960429986" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<p>A: <img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/25823a7c-6059-4c83-b592-a273fa59b9ec" alt="image" class="figure"></p>
<p>Briefly, we can say:</p>
<ul>
<li>Encoder: Processes input text into contextualized representations,
enabling the understanding of the context within the input sequence. It
is like the ‘listener’ in a conversation, taking in information and
understanding it.</li>
<li>Decoder: Generates output sequences by translating the
contextualized representations from the encoder into coherent text,
often using mechanisms like masked multi-head attention and
encoder-decoder attention to maintain sequence order and coherence. This
acts as the ‘speaker’ in the conversation, generating the output based
on the information processed by the encoder.</li>
<li>Positional Encoding: Adds unique information to each word embedding,
indicating the word’s position in the sequence, which is essential for
the model to maintain the order of words and understand their relative
positions within a sentence</li>
<li>Input Embedding: The input text is converted into vectors that the
model can understand. Think of it as translating words into a secret
code that the transformer can read.</li>
<li>Output Embedding: Similar to input embedding, but for the output
text. It translates the transformer’s secret code back into words we can
understand.</li>
<li>Softmax Output: Applies the softmax function to the final layer’s
outputs to convert them into a probability distribution, which helps in
tasks like classification and sequence generation by selecting the most
likely next word or class. It is like choosing the best response in a
conversation from many options.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="attention-mechanism" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="attention-mechanism" class="callout-inner">
<h3 class="callout-title">Attention Mechanism<a class="anchor" aria-label="anchor" href="#attention-mechanism"></a>
</h3>
<div class="callout-content">
<p>So far, we have learned what the architecture of a transformer block
looks like. However, for simplicity, many parts of this architecture
have not been considered.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/559580aa-f2c9-4ec0-b87d-7e1438839431" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>In the following section, we will show the underlying components of a
transformer.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/2b325dc1-20d8-4bac-91b8-030067ee8097" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>For more details see <a href="https://arxiv.org/abs/1706.03762" class="external-link">source</a>.</p>
<p>Attention mechanisms in transformers, allow LLMs to focus on
different parts of the input text to understand context and
relationships between words. The concept of ‘attention’ in encoders and
decoders is akin to the selective focus of ‘fast reading,’ where one
zeroes in on crucial information and disregards the irrelevant. This
mechanism adapts to the context of a query, emphasizing different words
or tokens based on the query’s intent. For instance, in the sentence
“Sarah went to a restaurant to meet her friend that night,” the words
highlighted would vary depending on whether the question is about the
action (What?), location (Where?), individuals involved (Who?), or time
(When?).</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/f28a1a04-40f7-4ad3-a846-4f0d4c4ddee4" alt="image" class="figure"><a href="https://medium.com/@hunter-j-phillips/multi-head-attention-7924371d477a" class="external-link">source</a></p>
<p>In transformer models, this selective focus is achieved through
‘queries,’ ‘keys,’ and ‘values,’ all represented as vectors. A query
vector seeks out the closest key vectors, which are encoded
representations of values. The relationship between words, like ‘where’
and ‘restaurant,’ is determined by their frequency of co-occurrence in
sentences, allowing the model to assign greater attention to
‘restaurant’ when the query pertains to a location. This dynamic
adjustment of focus enables transformers to process language with a
nuanced understanding of context and relevance.</p>
</div>
</div>
</div>
<div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Have you heard of any other applications of the
Transformers rather than in NLPs? Explain why transformers can be useful
for other AI applications. Share your thoughts and findings with other
groups.</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">Show me the solution</h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" data-bs-parent="#accordionSolution5" aria-labelledby="headingSolution5">
<div class="accordion-body">
<p>A: Transformers, initially popular in NLP, have found applications
beyond text analysis. They excel in computer vision, speech recognition,
and even genomics. Their versatility extends to music generation and
recommendation systems. Transformers’ innovative architecture allows
them to adapt to diverse tasks, revolutionizing AI applications.</p>
</div>
</div>
</div>
</div>
<div id="transformers-in-text-translation" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="transformers-in-text-translation" class="callout-inner">
<h3 class="callout-title">Transformers in Text Translation<a class="anchor" aria-label="anchor" href="#transformers-in-text-translation"></a>
</h3>
<div class="callout-content">
<p>Imagine you want to translate the sentence “What time is it?” from
English to German using a transformer. The input embedding layer
converts each English word into a vector. The six layers of encoders
process these vectors, understanding the context of the sentence. The
six layers of decoders then start generating the German translation, one
word at a time.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/09e7ae0f-bfc1-4d7c-be5f-443b97c05bd3" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p>For each word, the Softmax output predicts the most likely next word
in German. The output embedding layer converts these predictions back
into readable German words. By the end, you get the German translation
of <strong>“What time is it?”</strong> as <strong>“Wie spät ist
es?”</strong></p>
</div>
</div>
</div>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>What are other sequential learning models?</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler3" aria-labelledby="headingSpoiler3">
<div class="accordion-body">
<p>Transformers are essential for NLP tasks because they overcome the
limitations of earlier models like recurrent neural networks (RNNs) and
long short-term memory models (LSTMs), which struggled with long
sequences and were computationally intensive respectively. Transformers,
in contrast to the sequential input processing of RNNs, handle entire
sequences simultaneously. This parallel processing capability enables
data scientists to employ GPUs to train large language models (LLMs)
based on transformers, which markedly decreases the duration of
training.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/a0a429e4-c87e-4529-9151-781dd566c800" alt="rnn-transf-nlp" class="figure"><a href="https://thegradient.pub/transformers-are-graph-neural-networks/" class="external-link">source</a></p>
</div>
</div>
</div>
</div>
</section><section id="semantic-analysis"><h2 class="section-heading">5.3. Semantic Analysis<a class="anchor" aria-label="anchor" href="#semantic-analysis"></a>
</h2>
<hr class="half-width">
<p>Sentiment analysis is a powerful tool in NLP that helps determine the
emotional tone behind the text. It is used to understand opinions,
sentiments, emotions, and attitudes from various entities and classify
them according to their polarity.</p>
<div id="activity-2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="activity-2" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-2"></a>
</h3>
<div class="callout-content">
<p>Teamwork: How do you categorize the following text in terms of
positive and negative sounding? Select an Emoji.</p>
<p><em>“A research team has unveiled a novel ligand exchange technique
that enables the synthesis of organic cation-based perovskite quantum
dots (PQDs), ensuring exceptional stability while suppressing internal
defects in the photoactive layer of solar cells.”</em> <a href="https://www.sciencedaily.com/releases/2024/02/240221160400.htm" class="external-link">source</a></p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/897d0938-d84f-4189-afac-b8f244e16b46" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<p>Computer models can do this job for us! Let’s see how it works
through a step-by-step example: First, install the required libraries
and pipelines:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>pip install transformers</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code></pre>
</div>
<p>Now, initialize the sentiment analysis pipeline and analyze the
sentiment of a sample text:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>sentiment_pipeline <span class="op">=</span> pipeline(<span class="st">'sentiment-analysis'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>text <span class="op">=</span> <span class="st">" A research team has unveiled a novel ligand exchange technique that enables the synthesis of organic cation-based perovskite quantum dots (PQDs), ensuring exceptional stability while suppressing internal defects in the photoactive layer of solar cells."</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>sentiment <span class="op">=</span> sentiment_pipeline(text)</span></code></pre>
</div>
<p>After the analysis is completed, you can print out the results:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sentiment: </span><span class="sc">{</span>sentiment[<span class="dv">0</span>][<span class="st">'label'</span>]<span class="sc">}</span><span class="ss">, Confidence: </span><span class="sc">{</span>sentiment[<span class="dv">0</span>][<span class="st">'score'</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>Output: Sentiment: POSITIVE, Confidence: <span class="fl">1.00</span></span></code></pre>
</div>
<p>In this example, the sentiment analysis pipeline from the Hugging
Face library is used to analyze the sentiment of a research paper
abstract. The model predicts the sentiment as positive, negative, or
neutral, along with a confidence score. This can be particularly useful
for gauging the reception of research papers in a field.</p>
<div id="activity-3" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="activity-3" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-3"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Fill in the blanks to complete the sentiment analysis
process: Install the __________ library for sentiment analysis. Use the
__________ function to create a sentiment analysis pipeline. The
sentiment analysis model will output a __________ and a __________
score.</p>
</div>
</div>
</div>
<div id="vadrer" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="vadrer" class="callout-inner">
<h3 class="callout-title">VADRER<a class="anchor" aria-label="anchor" href="#vadrer"></a>
</h3>
<div class="callout-content">
<p>Valence Aware Dictionary and sEntiment Reasoner (VADER) is a lexicon
and rule-based sentiment analysis tool that is particularly attuned to
sentiments expressed in social media. VADER analyzes the sentiment of
the text and returns a dictionary with scores for negative, neutral,
positive, and a compound score that aggregates them. It is useful for
quick sentiment analysis, especially on social media texts. Let’s how we
can use this framework.</p>
<p>First, we need to import the SentimentIntensityAnalyzer module from
VADER library:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="im">from</span> vaderSentiment.vaderSentiment <span class="im">import</span> SentimentIntensityAnalyzer</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co"># Initialize VADER sentiment intensity analyzer:</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>analyzer <span class="op">=</span> SentimentIntensityAnalyzer()</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co"># We use the same sample text:</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>text <span class="op">=</span> <span class="st">" A research team has unveiled a novel ligand exchange technique that enables the synthesis of organic cation-based perovskite quantum dots (PQDs), ensuring exceptional stability while suppressing internal defects in the photoactive layer of solar cells."</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="co"># Now we can analyze sentiment:</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>vader_sentiment <span class="op">=</span> analyzer.polarity_scores(text)</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a><span class="co"># Print the sentiment:</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sentiment: </span><span class="sc">{</span>vader_sentiment<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>Output: Sentiment: {<span class="st">'neg'</span>: <span class="fl">0.069</span>, <span class="st">'neu'</span>: <span class="fl">0.818</span>, <span class="st">'pos'</span>: <span class="fl">0.113</span>, <span class="st">'compound'</span>: <span class="fl">0.1779</span>}</span></code></pre>
</div>
</div>
</div>
</div>
<div id="discussion-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Which framework do you think could be more helpful for
research applications? Elaborate your opinion. Share your thoughts with
other team members.</p>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6">Show me the solution</h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" data-bs-parent="#accordionSolution6" aria-labelledby="headingSolution6">
<div class="accordion-body">
<p>A: Transformers use deep learning models that can understand context
and nuances of language, making them suitable for complex and lengthy
texts. They can be particularly useful for sentiment analysis of
research papers, as they can understand the complex language and context
often found in academic writing. This allows for a more nuanced
understanding of the sentiment conveyed in the papers. VADER, on the
other hand, is a rule-based model that excels in analyzing short texts
with clear sentiment expressions, often found in social media.</p>
</div>
</div>
</div>
</div>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-2"></a>
</h3>
<div class="callout-content">
<p>Use the transformers library to perform sentiment analysis on the
following text:</p>
<p><em>“Perovskite nanocrystals have emerged as a promising class of
materials for next-generation optoelectronic devices due to their unique
properties. Their crystal structure allows for tunable bandgaps, which
are the energy differences between occupied and unoccupied electronic
states. This tunability enables the creation of materials that can
absorb and emit light across a wide range of the electromagnetic
spectrum, making them suitable for applications like solar cells,
light-emitting diodes (LEDs), and lasers.”</em></p>
<p>Print the original text and the sentiment score and label. You can
use the following code to load the transformers library and the
pre-trained model and tokenizer for sentiment analysis:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>sentiment_analysis <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution7" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution7" aria-expanded="false" aria-controls="collapseSolution7">
  <h4 class="accordion-header" id="headingSolution7">Show me the solution</h4>
</button>
<div id="collapseSolution7" class="accordion-collapse collapse" data-bs-parent="#accordionSolution7" aria-labelledby="headingSolution7">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>sentiment_analysis <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"This book is amazing. It is well-written, engaging, and informative. I learned a lot from reading it and I highly recommend it to anyone interested in natural language processing."</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="bu">print</span>(text)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="bu">print</span>(sentiment_analysis(text))</span></code></pre>
</div>
<p>Output:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>output: <span class="st">"Perovskite nanocrystals have emerged as a promising class of materials for next-generation optoelectronic devices due to their unique properties. Their crystal structure allows for tunable bandgaps, which are the energy differences between occupied and unoccupied electronic states. This tunability enables the creation of materials that can absorb and emit light across a wide range of the electromagnetic spectrum, making them suitable for applications like solar cells, light-emitting diodes (LEDs), and lasers."</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>[{<span class="st">'label'</span>: <span class="st">'POSITIVE'</span>, <span class="st">'score'</span>: <span class="fl">0.9998656511306763</span>}]</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-3"></a>
</h3>
<div class="callout-content">
<p>Comparing Transformer with VADER on a large size text. Use the
Huggingface library database.</p>
</div>
</div>
</div>
<div id="accordionSolution8" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution8" aria-expanded="false" aria-controls="collapseSolution8">
  <h4 class="accordion-header" id="headingSolution8">Show me the solution</h4>
</button>
<div id="collapseSolution8" class="accordion-collapse collapse" data-bs-parent="#accordionSolution8" aria-labelledby="headingSolution8">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</section><section id="text-summarization"><h2 class="section-heading">5.4. Text Summarization<a class="anchor" aria-label="anchor" href="#text-summarization"></a>
</h2>
<hr class="half-width">
<p>Text summarization is the process of distilling the most important
information from a source (or sources) to produce an abbreviated version
for a particular user and task. It can be broadly classified into two
types: extractive and abstractive summarization.</p>
<div id="discussion-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion-2" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-2"></a>
</h3>
<div class="callout-content">
<p>How extractive and abstractive summarization methods are different?
Connect the following text boxes to the correct category. Share your
results with other group members.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/1630ffcf-90c8-49df-9abe-98a739fd58ef" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<div id="accordionSolution9" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution9" aria-expanded="false" aria-controls="collapseSolution9">
  <h4 class="accordion-header" id="headingSolution9">Show me the solution</h4>
</button>
<div id="collapseSolution9" class="accordion-collapse collapse" data-bs-parent="#accordionSolution9" aria-labelledby="headingSolution9">
<div class="accordion-body">
<p>A: <img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/166c2025-14b0-4f1b-aee5-3f46d4f3c8b4" alt="image" class="figure"></p>
</div>
</div>
</div>
</div>
<p>Now, let’s see how to use the Hugging Face Transformers library to
perform abstractive summarization. First, from the transformers import
pipeline:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="co"># Initialize the summarization pipeline</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span></code></pre>
</div>
<p>Input a sample text from an article from <a href="https://www.sciencedaily.com/releases/2024/02/240221160400.htm#:~:text=Summary%3A,photoactive%20layer%20of%20solar%20cells." class="external-link">source</a>:</p>
<div id="accordionSpoiler4" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler4" aria-expanded="false" aria-controls="collapseSpoiler4">
  <h3 class="accordion-header" id="headingSpoiler4">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Input Text</h3>
</button>
<div id="collapseSpoiler4" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler4" aria-labelledby="headingSpoiler4">
<div class="accordion-body">
<p>text = “A groundbreaking research breakthrough in solar energy has
propelled the development of the world’s most efficient quantum dot (QD)
solar cell, marking a significant leap towards the commercialization of
next-generation solar cells. This cutting-edge QD solution and device
have demonstrated exceptional performance, retaining their efficiency
even after long-term storage. Led by Professor Sung-Yeon Jang from the
School of Energy and Chemical Engineering at UNIST, a team of
researchers has unveiled a novel ligand exchange technique. This
innovative approach enables the synthesis of organic cation-based
perovskite quantum dots (PQDs), ensuring exceptional stability while
suppressing internal defects in the photoactive layer of solar cells.
Our developed technology has achieved an impressive 18.1% efficiency in
QD solar cells,” stated Professor Jang. This remarkable achievement
represents the highest efficiency among quantum dot solar cells
recognized by the National Renewable Energy Laboratory (NREL) in the
United States. The increasing interest in related fields is evident, as
last year, three scientists who discovered and developed QDs, as
advanced nanotechnology products, were awarded the Nobel Prize in
Chemistry. QDs are semiconducting nanocrystals with typical dimensions
ranging from several to tens of nanometers, capable of controlling
photoelectric properties based on their particle size. PQDs, in
particular, have garnered significant attention from researchers due to
their outstanding photoelectric properties. Furthermore, their
manufacturing process involves simple spraying or application to a
solvent, eliminating the need for the growth process on substrates. This
streamlined approach allows for high-quality production in various
manufacturing environments. However, the practical use of QDs as solar
cells necessitates a technology that reduces the distance between QDs
through ligand exchange, a process that binds a large molecule, such as
a ligand receptor, to the surface of a QD. Organic PQDs face notable
challenges, including defects in their crystals and surfaces during the
substitution process. As a result, inorganic PQDs with limited
efficiency of up to 16% have been predominantly utilized as materials
for solar cells. In this study, the research team employed an alkyl
ammonium iodide-based ligand exchange strategy, effectively substituting
ligands for organic PQDs with excellent solar utilization. This
breakthrough enables the creation of a photoactive layer of QDs for
solar cells with high substitution efficiency and controlled defects.
Consequently, the efficiency of organic PQDs, previously limited to 13%
using existing ligand substitution technology, has been significantly
improved to 18.1%. Moreover, these solar cells demonstrate exceptional
stability, maintaining their performance even after long-term storage
for over two years. The newly-developed organic PQD solar cells exhibit
both high efficiency and stability simultaneously. Previous research on
QD solar cells predominantly employed inorganic PQDs,” remarked Sang-Hak
Lee, the first author of the study. Through this study, we have
demonstrated the potential by addressing the challenges associated with
organic PQDs, which have proven difficult to utilize. This study
presents a new direction for the ligand exchange method in organic PQDs,
serving as a catalyst to revolutionize the field of QD solar cell
material research in the future,” commented Professor Jang. The findings
of this study, co-authored by Dr. Javid Aqoma Khoiruddin and Sang-Hak
Lee, have been published online in Nature Energy on January 27, 2024.
The research was made possible through the support of the ‘Basic
Research Laboratory (BRL)’ and ‘Mid-Career Researcher Program,’ as well
as the ‘Nano·Material Technology Development Program,’ funded by the
National Research Foundation of Korea (NRF) under the Ministry of
Science and ICT (MSIT). It has also received support through the ’Global
Basic Research Lab Project.”</p>
</div>
</div>
</div>
</div>
<p>Now we can perform summarization and print the results:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>summary <span class="op">=</span> summarizer(text, max_length<span class="op">=</span><span class="dv">130</span>, min_length<span class="op">=</span><span class="dv">30</span>, do_sample<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="co"># Print the summary:</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Summary:"</span>, summary[<span class="dv">0</span>][<span class="st">'summary_text'</span>])</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>Output: </span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span></code></pre>
</div>
<div id="sumy-for-summarization" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="sumy-for-summarization" class="callout-inner">
<h3 class="callout-title">Sumy for summarization<a class="anchor" aria-label="anchor" href="#sumy-for-summarization"></a>
</h3>
<div class="callout-content">
<p>Sumy is a Python library for extractive summarization. It uses
algorithms like LSA to rank sentences based on their importance and
creates a summary by selecting the top-ranked sentences. We can see how
it works in practice: We start with importing the PlaintextParser and
LsaSummarizer modules:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="im">from</span> sumy.parsers.plaintext <span class="im">import</span> PlaintextParser</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="im">from</span> sumy.nlp.tokenizers <span class="im">import</span> Tokenizer</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="im">from</span> sumy.summarizers.lsa <span class="im">import</span> LsaSummarizer</span></code></pre>
</div>
<p>To create a parser we use the same text sample from an article from
<a href="https://www.sciencedaily.com/releases/2024/02/240221160400.htm#:~:text=Summary%3A,photoactive%20layer%20of%20solar%20cells." class="external-link">source</a>:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>parser <span class="op">=</span> PlaintextParser.from_string(text, Tokenizer(<span class="st">"english"</span>))</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="co"># Next, we initialize the LSA summarize:</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>summarizer <span class="op">=</span> LsaSummarizer()</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="co"># Summarize the text and print the results</span></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>summary <span class="op">=</span> summarizer(parser.document, <span class="dv">5</span>)</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a><span class="cf">for</span> sentence <span class="kw">in</span> summary:</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>    <span class="bu">print</span>(sentence)</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>Output:</span></code></pre>
</div>
<p>Sumy extracts key sentences from the original text, which can be
quicker but may lack the cohesiveness of an abstractive summary. On the
other hand, Transformer is suitable for generating a new summary that
captures the text’s essence in a coherent and often more readable
form.</p>
</div>
</div>
</div>
<div id="activity-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity-4" class="callout-inner">
<h3 class="callout-title">Activity<a class="anchor" aria-label="anchor" href="#activity-4"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Which framework could be more useful for text
summarizations in your field of research? Explain why?</p>
</div>
</div>
</div>
<div id="accordionSolution10" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution10" aria-expanded="false" aria-controls="collapseSolution10">
  <h4 class="accordion-header" id="headingSolution10">Show me the solution</h4>
</button>
<div id="collapseSolution10" class="accordion-collapse collapse" data-bs-parent="#accordionSolution10" aria-labelledby="headingSolution10">
<div class="accordion-body">
<p>A: Transformers are particularly useful for summarizing research
papers and documents where understanding the context and generating a
coherent summary is crucial. They can produce summaries that are not
only concise but also maintain the narrative flow, making them more
readable. Sumy, while quicker and less resource-intensive, is best
suited for scenarios where extracting key information without the need
for narrative flow is acceptable.</p>
</div>
</div>
</div>
</div>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-4"></a>
</h3>
<div class="callout-content">
<p>Use the transformers library to perform text summarization on the
following text [generated by Copilot]:</p>
</div>
</div>
</div>
<div id="accordionSpoiler5" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler5" aria-expanded="false" aria-controls="collapseSpoiler5">
  <h3 class="accordion-header" id="headingSpoiler5">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Input Text</h3>
</button>
<div id="collapseSpoiler5" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler5" aria-labelledby="headingSpoiler5">
<div class="accordion-body">
<p>text: “Perovskite nanocrystals are a class of semiconductor
nanocrystals that have attracted a lot of attention in recent years due
to their unique optical and electronic properties. Perovskite
nanocrystals have an ABX3 composition, where A is a monovalent cation
(such as cesium, methylammonium, or formamidinium), B is a divalent
metal (such as lead or tin), and X is a halide (such as chloride,
bromide, or iodide). Perovskite nanocrystals can emit brightly across
the entire visible spectrum, with tunable colors depending on their
composition and size. They also have high quantum yields, fast radiative
decay rates, and narrow emission line widths, making them ideal
candidates for various optoelectronic applications. The first report of
perovskite nanocrystals was published in 2014 by Protesescu et al., who
synthesized cesium lead halide nanocrystals using a hot-injection
method. They demonstrated that the nanocrystals had cubic or
orthorhombic crystal structures, depending on the halide ratio, and that
they exhibited strong photoluminescence with quantum yields up to 90%.
They also showed that the emission wavelength could be tuned from 410 nm
to 700 nm by changing the halide composition or the nanocrystal size.
Since then, many other groups have developed various synthetic methods
and strategies to control the shape, size, composition, and surface
chemistry of perovskite nanocrystals. One of the remarkable features of
perovskite nanocrystals is their defect tolerance, which means that they
can maintain high luminescence even with a high density of surface or
bulk defects. This is in contrast to other semiconductor nanocrystals,
such as CdSe, which require surface passivation to prevent non-radiative
recombination and quenching of the emission. The defect tolerance of
perovskite nanocrystals is attributed to their electronic band
structure, which has a large density of states near the band edges and a
small effective mass of the charge carriers. These factors reduce the
formation energy and the localization of defects and enhance the
radiative recombination rate of the excitons. Another interesting aspect
of perovskite nanocrystals is their weak quantum confinement, which
means that their emission properties are not strongly affected by their
size. This is because the exciton binding energy of perovskite
nanocrystals is much larger than the quantum confinement energy, and
thus the excitons are localized within a few unit cells regardless of
the nanocrystal size. As a result, perovskite nanocrystals can exhibit
narrow emission line widths even with a large size distribution, which
simplifies the synthesis and purification processes. Moreover,
perovskite nanocrystals can show dual emission from both the band edge
and the surface states, which can be exploited for color tuning and
white light generation. Perovskite nanocrystals have been applied to a
wide range of photonic devices, such as light-emitting diodes, lasers,
solar cells, photodetectors, and scintillators. Perovskite nanocrystals
can offer high brightness, color purity, and stability as light
emitters, and can be integrated with various substrates and
architectures. Perovskite nanocrystals can also act as efficient light
absorbers and charge transporters and can be coupled with other
materials to enhance the performance and functionality of the devices.
Perovskite nanocrystals have shown promising results in terms of
efficiency, stability, and versatility in these applications. However,
perovskite nanocrystals also face some challenges and limitations, such
as the toxicity of lead, the instability under ambient conditions, the
hysteresis and degradation under electrical or optical stress, and the
reproducibility and scalability of the synthesis and fabrication
methods. These issues need to be addressed and overcome to realize the
full potential of perovskite nanocrystals in practical devices.
Therefore, further research and development are needed to improve the
material quality, stability, and compatibility of perovskite
nanocrystals, and to explore new compositions, structures, and
functionalities of these fascinating nanomaterials.”</p>
</div>
</div>
</div>
</div>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge<em>(continued)</em><a class="anchor" aria-label="anchor" href="#challenge-4"></a>
</h3>
<div class="callout-content">
<p>Print the summarized text.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>...</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution11" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution11" aria-expanded="false" aria-controls="collapseSolution11">
  <h4 class="accordion-header" id="headingSolution11">Show me the solution</h4>
</button>
<div id="collapseSolution11" class="accordion-collapse collapse" data-bs-parent="#accordionSolution11" aria-labelledby="headingSolution11">
<div class="accordion-body">
<p>A: You can use the following code to load the transformers library
and the pre-trained model and tokenizer for text summarization:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>text <span class="op">=</span> <span class="st">" Perovskite nanocrystals are a class of semiconductor nanocrystals that have attracted…</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a><span class="er">Output:</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Transformers revolutionized NLP by processing words in parallel
through an attention mechanism, capturing context more effectively than
sequential models</li>
<li>The summation and activation function within a neuron transform
inputs through weighted sums and biases, followed by an activation
function to produce an output.</li>
<li>Transformers consist of encoders, decoders, positional encoding,
input/output embedding, and softmax output, working together to process
and generate data.</li>
<li>Transformers are not limited to NLP and can be applied to other AI
applications due to their ability to handle complex data patterns.</li>
<li>Sentiment analysis and text summarization are practical applications
of transformers in NLP, enabling the analysis of emotional tone and the
creation of concise summaries from large texts.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-06-llms"><p>Content from <a href="06-llms.html">Large Language Models</a></p>
<hr>
<p>Last updated on 2024-05-12 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/06-llms.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are the main features of large language models?</li>
<li>How is BERT different from GPT models?</li>
<li>How can I use open-source LLMs, such as LLM examples in huggingface,
for research tasks?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Be able to explain the structure of large language models and their
main components</li>
<li>Identify differences between BERT and GPT.</li>
<li>Be able to use open-source LLMs, such as huggingface, for text
summarization, classification, and generation.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="introduction-to-llms"><h2 class="section-heading">6.1. Introduction to LLMs<a class="anchor" aria-label="anchor" href="#introduction-to-llms"></a>
</h2>
<hr class="half-width">
<p>Large Language Models (LLMs) have become a cornerstone of modern
natural language processing (NLP). Since the introduction of the
transformer architecture in 2017, LLMs have leveraged this design to
achieve remarkable language understanding and generation capabilities.
In the previous episode, we discussed the transformer architecture,
which is integral to all LLMs, utilizing its encoder and decoder
components to process language.</p>
<p>LLMs have several key features.</p>
<figure><img src="../fig/llms_1.png" class="figure mx-auto d-block"></figure><div id="challenge" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge"></a>
</h3>
<div class="callout-content">
<p>Fill in the above feature placeholders. Discuss what are these key
components. Explain the key features in detail and compare your thoughts
with the other group members:</p>
<ol style="list-style-type: decimal">
<li><p>Transformer Architecture: A neural network design that uses
self-attention mechanisms to weigh the influence of different parts of
the input data.</p></li>
<li><p>Pre-training: involves teaching LLMs to anticipate words in
sentences, using either bi-directional or uni-directional approaches,
(based on the LLM type), without the need for understanding or
experience.</p></li>
<li><hr></li>
<li><hr></li>
<li><hr></li>
<li><hr></li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<figure><img src="../fig/llms_4.png" class="figure mx-auto d-block"></figure><ol style="list-style-type: decimal">
<li><p>Transformer Architecture: A neural network design that uses
self-attention mechanisms to weigh the influence of different parts of
the input data.</p></li>
<li><p>Pre-training: involves teaching LLMs to anticipate words in
sentences, using either bi-directional or uni-directional approaches,
(based on the LLM type), without the need for understanding or
experience.</p></li>
<li><p>Word/Token Embedding: The process of converting words or phrases
into numerical form (vectors) that computers can understand.</p></li>
</ol>
<p><strong>RECALL</strong> embedding?</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/06a4c996-cfe4-414e-b91c-9048c1006fc6" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/6e7a5d15-94a4-4522-910f-3ad67dd2ee69" alt="image" class="figure"><a href="https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/" class="external-link">source</a></p>
<ol start="4" style="list-style-type: decimal">
<li><p>Context Window: The range of words the model considers for
predicting the next word or understanding the current word within a
sentence.</p></li>
<li><p>Parameters: The aspects of the model that are learned from
training data and determine the model’s behavior.</p></li>
<li><p>Transfer Learning: The process LLMs use to apply their prior
knowledge to new tasks.</p></li>
</ol>
<p>Thus, the completed graph will be:</p>
</div>
</div>
</div>
</div>
<p>We can categorize LLMs based on the transformer architecture. Let’s
have another look into the transformer architecture, this time we
categorize them based on the two main components: <strong>Encoder and
Decoder</strong>. LLMs can be designed to handle different tasks based
on their underlying transformer blocks and whether they have
encoder-only, decoder-only, or encoder-decoder layers.</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/db7f7677-67eb-475d-bb3e-3b703b2fbb0f" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure><div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-1"></a>
</h3>
<div class="callout-content">
<p>How do you think we should connect each one of the following
transformers to the correct color?</p>
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/7b5bd803-c075-4b66-9d8e-b989172f50d8" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<figure><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/9d4afc99-d8f4-4a17-9af9-5fab4fac5dfd" alt="image" class="figure mx-auto d-block"><div class="figcaption">image</div>
</figure>
</div>
</div>
</div>
</div>
<p>• Encoders are used for understanding tasks like sentence
classification.</p>
<p>• Decoders excel in generative tasks like text generation.</p>
<p>• The combination of encoders and decoders in transformers allows
them to be versatile and perform a variety of tasks, from translation to
summarization, depending on the specific requirements of the task at
hand.</p>
<div id="encoder-vs.-decoder-andor-bert-vs.-gpt" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="encoder-vs.-decoder-andor-bert-vs.-gpt" class="callout-inner">
<h3 class="callout-title">Encoder Vs. Decoder and/or BERT Vs. GPT<a class="anchor" aria-label="anchor" href="#encoder-vs.-decoder-andor-bert-vs.-gpt"></a>
</h3>
<div class="callout-content">
<p>We will see models like BERT use encoders for bidirectional
understanding, and models like GPT use decoders for generating coherent
text, making them suitable for chatbots or virtual assistants.</p>
</div>
</div>
</div>
<div id="discussion" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Think of some examples of traditional NLP models, such as
n-gram models, hidden Markov models, LSTMs, and RNNs. How do large
language models differ from them in terms of architecture, data, and
performance?</p>
<p>A: Traditional NLP models, such as n-gram models, hidden Markov
models (HMMs), Long Short-Term Memory Networks (LSTMs), and Recurrent
Neural Networks (RNNs), differ significantly from the recent LLMs.
N-gram models predict the next item in a sequence based on the previous
‘n-1’ items without any deep understanding of context. HMMs are
statistical models that output probabilities of sequences and are often
used for tasks like part-of-speech tagging. LSTMs and RNNs are types of
neural networks that can process sequences of data and are capable of
learning order dependence in sequence prediction.</p>
<p>Compared to these traditional models, LLMs have several key
differences: - <strong>Architecture</strong>: Novel LLMs use transformer
architectures, which are more advanced than the simple recurrent units
of RNNs or the gated units of LSTMs. Transformers use self-attention to
weigh the influence of different parts of the input data, which is more
effective for understanding context. - <strong>Data</strong>: Novel LLMs
are trained on massive datasets, often sourced from the internet, which
allows them to learn a wide variety of language patterns, common
knowledge, and even reasoning abilities. Traditional models typically
use smaller, more curated datasets. - <strong>Performance</strong>:
Novel LLMs generally outperform traditional models in a wide range of
language tasks due to their ability to understand and generate
human-like text. They can capture subtleties and complexities of
language that simpler models cannot, leading to more accurate and
coherent outputs.</p>
</div>
</div>
</div>
</section><section id="bert"><h2 class="section-heading">6.2. BERT<a class="anchor" aria-label="anchor" href="#bert"></a>
</h2>
<hr class="half-width">
<p>Bidirectional Encoder Representations from Transformers (BERT) is an
LLM that uses an encoder-only architecture from transformers. It is
designed to understand the context of a word based on all of its
surroundings (bidirectional context). Let’s guess the missing words in
the text below to comprehend the workings of BERT:</p>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-2"></a>
</h3>
<div class="callout-content">
<p>Complete the following paragraph:</p>
<p>“BERT is a revolutionary language model that uses an ______ (encoder)
to process words in a sentence. Unlike traditional models, it predicts
words based on the ______ rather than in sequence. Its training involves
______, where words are intentionally hidden, or ______, and the model
learns to predict them. This results in rich ______ that capture the
nuanced meanings of words.”</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>“BERT is a revolutionary language model that uses an
<strong>encoder</strong> to process words in a sentence. Unlike
traditional models, it predicts words based on the
<strong>context</strong> rather than in sequence. Its training involves
<strong>self-supervised learning</strong>, where words are intentionally
hidden, or <strong>‘masked’</strong>, and the model learns to predict
them. This results in rich <strong>embeddings</strong> that capture the
nuanced meanings of words.”</p>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Heads-up: MLM &amp; NSP</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p>Pre-training of language models involves a process where models like
BERT and GPT learn to predict words in sentences without specific task
training. This is achieved through methods like the Masked Language
Model (MLM) for bi-directional models, which predict masked words using
surrounding context. MLM in BERT predicts missing words in a sentence by
masking them during training.</p>
<p>For Next Sentence Prediction (NSP) BERT learns to predict if two
sentences logically follow each other.</p>
</div>
</div>
</div>
</div>
</section><section id="gpt"><h2 class="section-heading">6.3. GPT<a class="anchor" aria-label="anchor" href="#gpt"></a>
</h2>
<hr class="half-width">
<p>Generative Pretrained Transformer (GPT) models, on the other hand,
use a decoder-only architecture. They excel at generating coherent and
contextually relevant text. Check the following table that summarizes
three different LLMs. The middle column misses some information about
GPT models. With the help of your teammates complete the table and
explain the differences in the end.</p>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-3"></a>
</h3>
<div class="callout-content">
<p>Write in the gray boxes with the correct explanations.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/c1bd86ec-2b48-4201-bc7f-db63d73b0df9" alt="image" class="figure"><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/e50f0111-2956-418d-8dba-612e1942ddfd" alt="image" class="figure"><a href="https://medium.com/@reyhaneh.esmailbeigi/bert-gpt-and-bart-a-short-comparison-5d6a57175fca" class="external-link">source</a></p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/c01d8b7f-b237-499b-b169-d40c785e30d0" alt="image" class="figure"><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/1153ca29-aafd-4de6-8013-525b21ded916" alt="image" class="figure"><a href="https://medium.com/@reyhaneh.esmailbeigi/bert-gpt-and-bart-a-short-comparison-5d6a57175fca" class="external-link">source</a></p>
</div>
</div>
</div>
</div>
<div id="discussion-1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: From what you learned above how can you explain the
differences between the three LLM types? Discuss in groups and share
your answers.</p>
<p>A: <em>We can see it as the processes of trying to understand a
conversation (BERT), versus trying to decide what to say next in the
conversation (GPT). BERT is like someone who listens to the entire
conversation before and after a word to really understand its
meaning.</em></p>
<p><em>For example, in the sentence “I ate an apple,” BERT would look at
both “I ate an” and “apple” to figure out what <strong>“an”</strong>
refers to. It’s trained by playing a game of <strong>‘guess the missing
word,’</strong> where some words are hidden <strong>(masked)</strong>
and it has to use the context to fill in the blanks.</em></p>
<p><em>GPT, on the other hand, is like a storyteller who only needs to
know what was said before to continue the tale. It would take “I ate an”
and <strong>predict that the next word</strong> might be “apple.” It
learns by <strong>reading a lot of text</strong> and practicing how to
predict the next word in a sentence.</em></p>
<p><em>Both are smart in their own ways, but they’re used for different
types of language tasks. BERT is great for understanding the
<strong>context of words</strong>, while GPT is excellent at
<strong>generating new text</strong> based on what it’s seen before. The
following schematics demonstrate their performing differences:</em></p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/114d8549-dede-4976-8e69-167cfe33879f" alt="image" class="figure"><a href="https://medium.com/@reyhaneh.esmailbeigi/bert-gpt-and-bart-a-short-comparison-5d6a57175fca" class="external-link">source</a></p>
</div>
</div>
</div>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>How LLMs can be Compared? What is HELM?</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" aria-labelledby="headingSpoiler2" data-bs-parent="#accordionSpoiler2">
<div class="accordion-body">
<p>Models are often benchmarked using standardized datasets and metrics.
The Holistic Evaluation of Language Models (HELM) by Stanford provides a
comprehensive framework for evaluating LLMs across multiple
dimensions.</p>
<p><img src="../fig/llms_9.png" class="figure"><a href="https://crfm.stanford.edu/helm/lite/latest/" class="external-link">source</a></p>
<p>GPT-4 models are outperforming other LLM models in terms of
accuracy.</p>
</div>
</div>
</div>
</div>
<div id="discussion-2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-2" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-2"></a>
</h3>
<div class="callout-content">
<p>What are some examples of LLMs, and how are they trained and used for
research tasks? Consider some of the main features and characteristics
of LLMs, such as transformer architecture, self-attention mechanism,
pre-training and fine-tuning, and embedding capabilities. How do these
features enable LLMs to perform various NLP tasks, such as text
classification, text generation, or question answering?</p>
</div>
</div>
</div>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-4"></a>
</h3>
<div class="callout-content">
<p>How can we compare different LLMs? Are there any benchmarks?</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">Show me the solution</h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<p>A: Comparing Performance (Benchmarking): 1. Performance can be
compared based on the model’s architecture, computational efficiency,
and suitability for specific tasks. 2. Benchmarks and leaderboards (such
as HELM) can provide insights into how different models perform on
standardized datasets. 3. Community feedback and use-case studies can
also inform the practical effectiveness of different LLMs.</p>
</div>
</div>
</div>
</div>
</section><section id="open-source-llms"><h2 class="section-heading">6.4. Open-Source LLMs:<a class="anchor" aria-label="anchor" href="#open-source-llms"></a>
</h2>
<hr class="half-width">
<p>It is very important for researchers to openly have access to capable
LLMs for their studies. Fortunately, some companies are supporting
open-source LLMs. The BLOOM model, developed by the BigScience Workshop
in collaboration with Hugging Face and other organizations, was released
on July 6, 2022. It offers a wide range of model sizes, from 1.1 billion
to 176 billion parameters, and is licensed under the open RAIL-M v1.
BLOOM is known for its instruct models, coding capabilities,
customization finetuning, and being open source. It is more openly
accessible and benefits from a large community and extensive
support.</p>
<p>On the other hand, the LLaMA model, developed by Meta AI, was
released on February 24, 2023. It is available in four sizes: 7 billion,
13 billion, 33 billion, and 65 billion parameters. The license for LLaMA
is restricted to noncommercial use, and access is primarily for
researchers. Despite its smaller size, LLaMA is parameter-efficient and
has outperformed GPT-3 on many benchmarks. However, its accessibility is
more gated compared to BLOOM, and community support is limited to
approved researchers.</p>
<p>Now let’s summarize what we learned here in the following table:</p>
<figure><img src="../fig/llms_10.png" class="figure mx-auto d-block"></figure><p>Hugging Face provides several different LLMs. Now we want to see how
we can use an open-source model. using the Hugging Face datasets library
and an open-source Large Language Model (LLM). We will go through the
process of setting up the environment, installing necessary libraries,
loading a dataset, and then using an LLM to process the data. We will
start with setting up the environment.</p>
<div id="heads-up" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="heads-up" class="callout-inner">
<h3 class="callout-title">Heads up<a class="anchor" aria-label="anchor" href="#heads-up"></a>
</h3>
<div class="callout-content">
<p>Before we begin, ensure that you have Python installed on your
system. Python 3.6 or later is recommended. You can download Python from
the official Python website.</p>
</div>
</div>
</div>
<p>Next, we will install the necessary libraries through the terminal or
command prompt:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>pip install datasets transformers</span></code></pre>
</div>
<p>We use the <strong>squad dataset</strong> here, which is a
question-answering dataset Question-answering is one of main goals of
utilizing LLMs for research projects. When you run this script, the
expected output should be the answer to the question based on the
provided context. Here is how to load it:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co"># Load the SQuAD dataset</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>squad_dataset <span class="op">=</span> load_dataset(<span class="st">'squad'</span>)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># Print the first example in the training set</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="bu">print</span>(squad_dataset[<span class="st">'train'</span>][<span class="dv">0</span>])</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a></span></code></pre>
</div>
<p>Now, we can load a pre-trained model from Hugging Face. For this
example, let’s use the bert-base-uncased model, which is different from
BLOOM and is suitable for question-answering tasks:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForQuestionAnswering, AutoTokenizer</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co"># Load the tokenizer and model</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>model <span class="op">=</span> AutoModelForQuestionAnswering.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span></code></pre>
</div>
<p>We need to define the question and context here:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What is the name of the university in Paris that was founded in 1257?"</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>context <span class="op">=</span> <span class="st">"The University of Paris, founded in 1257, is often referred to as the Sorbonne after the college created by Robert de Sorbon. It is one of the world's oldest universities."</span></span></code></pre>
</div>
<p>Recall to be able to feed data into the model, we should already
tokenize our data. Once we have our data tokenized, we can use the model
to make predictions. Here is how to tokenize the first example:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co"># Tokenize the first example</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>Inputs <span class="op">=</span> tokenizer(squad_dataset[<span class="st">'train'</span>][<span class="dv">0</span>][<span class="st">'question'</span>], squad_dataset[<span class="st">'train'</span>][<span class="dv">0</span>][<span class="st">'context'</span>], return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="co"># Get model predictions</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span></code></pre>
</div>
<p>Note that the model outputs are raw <strong>logits</strong>. We need
to convert these into an answer by selecting the tokens with the highest
start and end scores:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="co"># Find the tokens with the highest `start` and `end` scores</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>answer_start <span class="op">=</span> torch.argmax(outputs.start_logits)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>answer_end <span class="op">=</span> torch.argmax(outputs.end_logits) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a><span class="co"># Convert tokens to the answer string</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>answer <span class="op">=</span> tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens (inputs[<span class="st">'input_ids'</span>][<span class="dv">0</span>][answer_start:answer_end]))</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="bu">print</span>(answer)</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a></span></code></pre>
</div>
<p>This will print the answer to the question based on the context
provided in the dataset. In this case, the output would be:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>Output: the sorbonne</span></code></pre>
</div>
<p>This output indicates that the model has correctly identified “the
Sorbonne” as the name of the university in Paris founded in 1257, based
on the context given. Remember, the actual output may vary slightly
depending on the model version and the specific weights used at the time
of inference.</p>
<div id="discussion-3" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-3" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-3"></a>
</h3>
<div class="callout-content">
<p>Teamwork: What are the challenges and implications of LLMs, such as
scalability, generalization, and social impact? What does it mean when
an LLM hallucinates?</p>
</div>
</div>
</div>
<div id="challenge-5" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-5" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-5"></a>
</h3>
<div class="callout-content">
<p>Use the OpenAI library to access and use an open-source LLM for text
summarization. You can use the following code to load the OpenAI library
and the pre-trained model and tokenizer for text summarization:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>openai.api_key <span class="op">=</span> <span class="st">"sk-&lt;your_api_key&gt;"</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>text_summarizer <span class="op">=</span> openai.Completion.create(engine<span class="op">=</span><span class="st">"davinci"</span>, task<span class="op">=</span><span class="st">"summarize"</span>)</span></code></pre>
</div>
<p>Use the text_summarizer to summarize the following text.</p>
</div>
</div>
</div>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Text</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" aria-labelledby="headingSpoiler3" data-bs-parent="#accordionSpoiler3">
<div class="accordion-body">
<p>“Perovskite nanocrystals are a class of semiconductor nanocrystals,
which exhibit unique characteristics that separate them from traditional
quantum dots. Perovskite nanocrystals have an ABX3 composition where A =
cesium, methylammonium (MA), or formamidinium (FA); B = lead or tin; and
X = chloride, bromide, or iodide. Their unique qualities largely involve
their unusual band structure which renders these materials effectively
defect-tolerant or able to emit brightly without surface passivation.
This is in contrast to other quantum dots such as CdSe which must be
passivated with an epitaxially matched shell to be bright emitters. In
addition to this, lead-halide perovskite nanocrystals remain bright
emitters when the size of the nanocrystal imposes only weak quantum
confinement. This enables the production of nanocrystals that exhibit
narrow emission linewidths regardless of their polydispersity. The
combination of these attributes and their easy-to-perform synthesis has
resulted in numerous articles demonstrating the use of perovskite
nanocrystals as both classical and quantum light sources with
considerable commercial interest. Perovskite nanocrystals have been
applied to numerous other optoelectronic applications such as
light-emitting diodes, lasers, visible communication, scintillators,
solar cells, and photodetectors. The first report of perovskite
nanocrystals was published in 2014 by Protesescu et al., who synthesized
cesium lead halide nanocrystals using a hot-injection method. They
showed that the nanocrystals can emit brightly when excited by
ultraviolet or blue light, and their colors are tunable across the
entire visible spectrum by changing the halide from chloride (UV/blue)
to bromide (green) and iodide (red). They also demonstrated that the
nanocrystals can be incorporated into thin films and show high
photoluminescence quantum yields (PLQYs) of up to 90%. Since then, many
other synthetic methods have been developed to produce perovskite
nanocrystals with different shapes, sizes, compositions, and surface
ligands. Some of the common methods include ligand-assisted
reprecipitation, antisolvent precipitation, solvothermal synthesis,
microwave-assisted synthesis, and microfluidic synthesis. Perovskite
nanocrystals can be classified into different types based on their
structure, dimensionality, and composition. The most common type is the
three-dimensional (3D) perovskite nanocrystals, which have a cubic or
orthorhombic crystal structure and a band gap that depends on the halide
content. The 3D perovskite nanocrystals can be further divided into pure
halide perovskites (such as CsPbX3) and mixed halide perovskites (such
as CsPb(Br/I)3), which can exhibit color tuning, anion exchange, and
halide segregation phenomena. Another type is the two-dimensional (2D)
perovskite nanocrystals, which have a layered structure with organic
cations sandwiched between inorganic perovskite layers. The 2D
perovskite nanocrystals have a quantum well-like band structure and a
band gap that depends on the thickness of the perovskite layers. The 2D
perovskite nanocrystals can also be mixed with 3D perovskite
nanocrystals to form quasi-2D perovskite nanocrystals, which can improve
the stability and emission efficiency of the nanocrystals. A third type
is the metal-free perovskite nanocrystals, which replace the metal
cations (such as Pb or Sn) with other elements (such as Bi or Sb). The
metal-free perovskite nanocrystals have a lower toxicity and higher
stability than the metal-based perovskite nanocrystals, but they also
have a lower PLQY and a broader emission linewidth. The development of
perovskite nanocrystals in the past few years has been remarkable, with
significant advances in synthesis, characterization, and application.
However, there are still some challenges and opportunities for further
improvement. One of the major challenges is the stability of perovskite
nanocrystals, which are sensitive to moisture, oxygen, heat, light, and
electric fields. These factors can cause degradation, phase transition,
and non-radiative recombination of the nanocrystals, resulting in
reduced emission intensity and color stability. Several strategies have
been proposed to enhance the stability of perovskite nanocrystals, such
as surface passivation, encapsulation, doping, alloying, and embedding
in matrices. Another challenge is the toxicity of perovskite
nanocrystals, which are mainly composed of lead, a heavy metal that can
cause environmental and health hazards. Therefore, there is a need to
develop lead-free or low-lead perovskite nanocrystals that can maintain
the high performance and tunability of the lead-based ones. Some of the
promising candidates include tin-based, bismuth-based, and
antimony-based perovskite nanocrystals. A third challenge is the
scalability and integration of perovskite nanocrystals, which are
essential for practical applications. There is a need to develop
cost-effective and large-scale synthesis methods that can produce
high-quality and uniform perovskite nanocrystals. Moreover, there is a
need to develop efficient and reliable fabrication techniques that can
integrate perovskite nanocrystals into various devices and platforms. In
conclusion, perovskite nanocrystals are a fascinating class of
nanomaterials that have shown remarkable potential for various photonic
applications. They have unique properties such as defect tolerance, high
quantum yield, fast radiative decay, and narrow emission linewidth in
weak confinement, which make them ideal candidates for light emission
devices. They also have a wide color tunability from ultraviolet to
near-infrared regions, which makes them suitable for various
wavelength-dependent applications. However, there are still some
challenges that need to be overcome, such as stability, toxicity,
scalability, and integration. Therefore, further research and
development are needed to address these issues and to explore new
opportunities for perovskite nanocrystals in the field of
nanophotonics.</p>
</div>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6">Show me the solution</h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" aria-labelledby="headingSolution6" data-bs-parent="#accordionSolution6">
<div class="accordion-body">
<p>Print the summarized text.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>openai.api_key <span class="op">=</span> <span class="st">"sk-&lt;your_api_key&gt;"</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>text_summarizer <span class="op">=</span> openai.Completion.create(engine<span class="op">=</span><span class="st">"davinci"</span>, task<span class="op">=</span><span class="st">"summarize"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>text <span class="op">=</span> <span class="st">" Perovskite nanocrystals are a class of semiconductor …"</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>summary <span class="op">=</span> text_summarizer(text)[<span class="st">'choices'</span>][<span class="dv">0</span>][<span class="st">'text'</span>]</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a><span class="bu">print</span>(text)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="bu">print</span>(summary)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>output:</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="challenge-6" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-6" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-6"></a>
</h3>
<div class="callout-content">
<p>Use the huggingface library to access and use an open-source
domain-specific LLM for text classification. You can use the following
code to load the huggingface library and the pre-trained model and
tokenizer for text classification:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>text_classifier <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>...</span></code></pre>
</div>
<p>Use the text_classifier to classify the following text into one of
the categories: metals, ceramics, polymers, or composites. Print the
text and the predicted category and score.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>Text: <span class="st">"Polyethylene is a thermoplastic polymer that consists of long chains of ethylene monomers. It is one of the most common and widely used plastics in the world. It has many applications, such as packaging, bottles, containers, films, pipes, and cables. Polyethylene can be classified into different grades based on its density, molecular weight, branching, and crystallinity."</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution7" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution7" aria-expanded="false" aria-controls="collapseSolution7">
  <h4 class="accordion-header" id="headingSolution7">Show me the solution</h4>
</button>
<div id="collapseSolution7" class="accordion-collapse collapse" aria-labelledby="headingSolution7" data-bs-parent="#accordionSolution7">
<div class="accordion-body">
<p>A:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>text_classifier <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Polyethylene is a thermoplastic polymer that consists of long chains of ethylene monomers. It is one of the most common and widely used plastics in the world. It has many applications, such as packaging, bottles, containers, films, pipes, and cables. Polyethylene can be classified into different grades based on its density, molecular weight, branching, and crystallinity."</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="bu">print</span>(text)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="bu">print</span>(text_classifier(text))</span></code></pre>
</div>
<pre><code><span></span>
<span><span class="va">output</span><span class="op">:</span> <span class="st">"Polyethylene is a thermoplastic polymer that consists of long chains of ethylene monomers. It is one of the most common and widely used plastics in the world. It has many applications, such as packaging, bottles, containers, films, pipes, and cables. Polyethylene can be classified into different grades based on its density, molecular weight, branching, and crystallinity."</span></span>
<span><span class="st">"[{'label': 'polymers', 'score': 0.9987659454345703}]"</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="challenge-7" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-7" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-7"></a>
</h3>
<div class="callout-content">
<p>Use the huggingface library to access and use an open-source LLM for
text generation. You can use the following code to load the huggingface
library and the pre-trained model and tokenizer for text generation:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>text_generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>)</span></code></pre>
</div>
<p>Use the text_generator to generate a paragraph of text based on the
following prompt: “The applications of nanomaterials in material science
are”. Print the prompt and the generated text.</p>
</div>
</div>
</div>
<div id="accordionSolution8" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution8" aria-expanded="false" aria-controls="collapseSolution8">
  <h4 class="accordion-header" id="headingSolution8">Show me the solution</h4>
</button>
<div id="collapseSolution8" class="accordion-collapse collapse" aria-labelledby="headingSolution8" data-bs-parent="#accordionSolution8">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>text_generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>)</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"The applications of nanomaterials in material science are"</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>generated_text <span class="op">=</span> text_generator(prompt)[<span class="dv">0</span>][<span class="st">'generated_text'</span>]</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="bu">print</span>(prompt)</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="bu">print</span>(generated_text)</span></code></pre>
</div>
<pre><code>
output: "The applications of nanomaterials in material science are
The applications of nanomaterials in material science are diverse and promising. Nanomaterials are materials that have at least one dimension in the nanometer range (1-100 nm). Nanomaterials can exhibit novel physical, chemical, and biological properties that are different from their bulk counterparts, such as high surface area, quantum confinement, enhanced reactivity, and tunable functionality. Nanomaterials can be used for various purposes in material science, such as improving the performance and functionality of existing materials, creating new materials with unique properties, and enabling new functionalities and devices. Some examples of nanomaterials applications in material science are:

- Nanocomposites: Nanomaterials can be incorporated into other materials, such as polymers, metals, ceramics, or biomaterials, to form nanocomposites that have improved mechanical, thermal, electrical, optical, or magnetic properties. For instance, carbon nanotubes can be used to reinforce polymer composites and increase their strength, stiffness, and conductivity.
- Nanocoatings: Nanomaterials can be used to coat the surface of other materials, such as metals, glass, or plastics, to provide them with enhanced protection, durability, or functionality. For example, titanium dioxide nanoparticles can be used to coat glass and make it self-cleaning, anti-fogging, and anti-bacterial.
- Nanosensors: Nanomaterials can be used to sense and measure various physical, chemical, or biological parameters, such as temperature, pressure, strain, pH, or biomolecules. For example, gold nanoparticles can be used to detect and quantify the presence of specific DNA sequences or proteins by changing their color or fluorescence.
- Nanomedicine: Nanomaterials can be used for various biomedical applications, such as drug delivery, imaging, diagnosis, or therapy. For example, magnetic nanoparticles can be used to deliver drugs to specific target sites in the body by using an external magnetic field, or to enhance the contrast of magnetic resonance imaging (MRI).
</code></pre>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>LLMs are based on the transformer architecture.</li>
<li>BERT and GPT have distinct approaches to processing language.</li>
<li>Open source LLMs provide transparency and customization for research
applications.</li>
<li>Benchmarking with HELM offers a holistic view of model
performance.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-07-domain-specific-llms"><p>Content from <a href="07-domain-specific-llms.html">Domain-Specific LLMs</a></p>
<hr>
<p>Last updated on 2024-05-12 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/07-domain-specific-llms.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can tune the LLMs to be domain-specific?</li>
<li>What are some available approaches to empower LLMs solve specific
research problems?</li>
<li>Which approach should I use for my research?</li>
<li>What are the challenges and trade-offs of domain-specific LLMs?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Be able to identify approaches by which LLMs can be tuned for
solving research problems.</li>
<li>Be able to use introductory approaches for creating domain-specific
LLMs.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="introduction-to-dsl-available-approaches"><h2 class="section-heading">7.1. Introduction to DSL (Available Approaches)<a class="anchor" aria-label="anchor" href="#introduction-to-dsl-available-approaches"></a>
</h2>
<hr class="half-width">
<p>To enhance the response quality of an LLM for solving specific
problems we need to use strategies by which we can tune the LLM.
Generally, there are four ways to enhance the performance of LLMs:</p>
<p><strong>1. Prompt Optimization:</strong></p>
<p>To elicit specific and accurate responses from LLMs by designing
prompts strategically.</p>
<ul>
<li>
<em>Zero-shot Prompting</em>: This is the simplest form of prompting
where the LLM is given a task or question without any context or
examples. It relies on the LLM’s pre-existing knowledge to generate a
response.</li>
</ul>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p>“What is the capital of France?” The LLM would respond with “Paris”
based on its internal knowledge.</p>
</div>
</div>
</div>
</div>
<ul>
<li>
<em>Few-shot Prompting</em>: In this technique, the LLM is provided
with a few examples to demonstrate the expected response format or
content.</li>
</ul>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" aria-labelledby="headingSpoiler2" data-bs-parent="#accordionSpoiler2">
<div class="accordion-body">
<p>To determine sentiment, you might provide examples like “I love sunny
days. (+1)” and “I hate traffic. (-1)” before asking the LLM to analyze
a new sentence.</p>
</div>
</div>
</div>
</div>
<p><strong>2. Retrieval Augmented Generation (RAG):</strong></p>
<p>To supplement the LLM’s generative capabilities with information
retrieved from external databases or documents.</p>
<ul>
<li>
<em>Retrieval</em>: The LLM queries a database to find relevant
information that can inform its response.</li>
</ul>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" aria-labelledby="headingSpoiler3" data-bs-parent="#accordionSpoiler3">
<div class="accordion-body">
<p>If asked about recent scientific discoveries, the LLM might retrieve
articles or papers on the topic.</p>
</div>
</div>
</div>
</div>
<ul>
<li>
<em>Generation</em>: After retrieving the information, the LLM
integrates it into a coherent response.</li>
</ul>
<div id="accordionSpoiler4" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler4" aria-expanded="false" aria-controls="collapseSpoiler4">
  <h3 class="accordion-header" id="headingSpoiler4">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler4" class="accordion-collapse collapse" aria-labelledby="headingSpoiler4" data-bs-parent="#accordionSpoiler4">
<div class="accordion-body">
<p>Using the retrieved scientific articles, the LLM could generate a
summary of the latest findings in a particular field.</p>
</div>
</div>
</div>
</div>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/b35c25f0-6176-490c-b726-662025f47075" alt="image" class="figure"><a href="https://www.maartengrootendorst.com/blog/improving-llms/" class="external-link">source</a></p>
<p><strong>3. Fine-Tuning:</strong></p>
<p>To adapt a general-purpose LLM to excel at a specific task or within
a particular domain.</p>
<ul>
<li>
<em>Language Modeling Task Fine-Tuning</em>: This involves training
the LLM on a large corpus of text to improve its ability to predict the
next word or phrase in a sentence.</li>
</ul>
<div id="accordionSpoiler5" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler5" aria-expanded="false" aria-controls="collapseSpoiler5">
  <h3 class="accordion-header" id="headingSpoiler5">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler5" class="accordion-collapse collapse" aria-labelledby="headingSpoiler5" data-bs-parent="#accordionSpoiler5">
<div class="accordion-body">
<p>An LLM fine-tuned on legal documents would become better at
generating text that resembles legal writing.</p>
</div>
</div>
</div>
</div>
<ul>
<li>
<em>Supervised Q&amp;A Fine-Tuning</em>: Here, the LLM is trained on
a dataset of question-answer pairs to enhance its performance on Q&amp;A
tasks.</li>
</ul>
<div id="accordionSpoiler6" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler6" aria-expanded="false" aria-controls="collapseSpoiler6">
  <h3 class="accordion-header" id="headingSpoiler6">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Example</h3>
</button>
<div id="collapseSpoiler6" class="accordion-collapse collapse" aria-labelledby="headingSpoiler6" data-bs-parent="#accordionSpoiler6">
<div class="accordion-body">
<p>An LLM fine-tuned with medical Q&amp;A pairs would provide more
accurate responses to health-related inquiries.</p>
</div>
</div>
</div>
</div>
<p><strong>4. Training from Scratch:</strong></p>
<p>Builds a model specifically for a domain, using relevant data from
the ground up.</p>
<div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p>Teamwork: Which approach do you think is more computation-intensive?
Which is more accurate? How are these qualities related? Evaluate the
trade-offs between fine-tuning and other approaches.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/c4f63e42-dcf9-4cfa-86fb-cf68357df229" alt="image" class="figure"><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/cffb0c09-6578-4206-94c3-ba90eca87515" alt="image" class="figure"><a href="https://medium.com/@pandey.vikesh/should-you-prompt-rag-tune-or-train-a-guide-to-choose-the-right-generative-ai-approach-5e264043bd7d" class="external-link">source</a></p>
</div>
</div>
</div>
</div>
<div id="discussion-1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-1" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-1"></a>
</h3>
<div class="callout-content">
<p>Teamwork: What is DSL and why are they useful for research tasks?
Think of some examples of NLP tasks that require domain-specific LLMs,
such as literature review, patent analysis, or material discovery. How
do domain-specific LLMs improve the performance and accuracy of these
tasks?</p>
<figure><img src="../fig/dsllms_2.png" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</section><section id="prompting"><h2 class="section-heading">7.2. Prompting<a class="anchor" aria-label="anchor" href="#prompting"></a>
</h2>
<hr class="half-width">
<p>For research applications where highly reliable answers are crucial,
Prompt Engineering combined with Retrieval-Augmented Generation (RAG) is
often the most suitable approach. This combination allows for
flexibility and high-quality outputs by leveraging both the generative
capabilities of LLMs and the precision of domain-specific data
sources:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>Install the Hugging Face libraries</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="op">!</span>pip install transformers datasets</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># Initialize the zero-shot classification pipeline</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"zero-shot-classification"</span>, model<span class="op">=</span><span class="st">"facebook/bart-large-mnli"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co"># Example research question</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What is the role of CRISPR-Cas9 in genome editing?"</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># Candidate topics to classify the question</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>topics <span class="op">=</span> [<span class="st">"Biology"</span>, <span class="st">"Technology"</span>, <span class="st">"Healthcare"</span>, <span class="st">"Genetics"</span>, <span class="st">"Ethics"</span>]</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co"># Perform zero-shot classification</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>result <span class="op">=</span> classifier(question, candidate_labels<span class="op">=</span>topics)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="co"># Output the results</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Classified under topics with the following scores:"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="cf">for</span> label, score <span class="kw">in</span> <span class="bu">zip</span>(result[<span class="st">'labels'</span>], result[<span class="st">'scores'</span>]):</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div id="accordionSpoiler7" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler7" aria-expanded="false" aria-controls="collapseSpoiler7">
  <h3 class="accordion-header" id="headingSpoiler7">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Be careful when fine-tuning a model</h3>
</button>
<div id="collapseSpoiler7" class="accordion-collapse collapse" aria-labelledby="headingSpoiler7" data-bs-parent="#accordionSpoiler7">
<div class="accordion-body">
<p>When fine-tuning a BERT model from Hugging Face, for instance, it is
essential to approach the process with precision and care.</p>
<ul>
<li><p>Begin by thoroughly understanding <strong>BERT’s
architecture</strong> and the specific task at hand to select the most
suitable model variant and hyperparameters.</p></li>
<li><p><strong>Prepare your dataset</strong> meticulously, ensuring it
is clean, well-represented, and split correctly to avoid <strong>data
leakage and overfitting</strong>.</p></li>
<li><p>Hyperparameter selection, such as learning rates and batch sizes,
should be made with consideration, and <strong>regularization</strong>
techniques like dropout should be employed to enhance the model’s
ability to generalize.</p></li>
<li><p><strong>Evaluate</strong> the model’s performance using
appropriate metrics and address any class imbalances with weighted loss
functions or similar strategies. Save checkpoints to preserve progress
and document every step of the fine-tuning process for transparency and
reproducibility.</p></li>
<li><p><strong>Ethical considerations</strong> are paramount; strive for
a model that is fair and unbiased. Ensure compliance with data
protection regulations, especially when handling sensitive
information.</p></li>
<li><p>Lastly, manage <strong>computational resources</strong> wisely
and engage with the Hugging Face community for additional support.
Fine-tuning is iterative, and success often comes through continuous
experimentation and learning.</p></li>
</ul>
</div>
</div>
</div>
</div>
<div id="challenge" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge"></a>
</h3>
<div class="callout-content">
<p>Check the following structure. Guess which optimization strategy is
represented in these architectures.</p>
<p><img src="https://github.com/qcif-training/intro_nlp_lmm_v1.0/assets/45458783/cfec9a3a-e334-4a87-8318-5c7495e0f6b7" alt="image" class="figure"><a href="https://www.linkedin.com/pulse/fine-tuning-prompt-engineering-rag-lokesh--kjaie/" class="external-link">source</a></p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>RAG addresses the challenge of <strong>real-time</strong> data
fetching by merging the generative talents of these models with the
ability to consult a broad document corpus, enhancing their responses.
The potential for live-RAG in chatbots suggests a future where AI can
conduct on-the-spot searches, access up-to-date information, and rival
search engines in answering timely questions.</p>
</div>
</div>
</div>
</div>
<div id="discussion-2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-2" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-2"></a>
</h3>
<div class="callout-content">
<p>Teamwork: What are the challenges and trade-offs of domain-specific
LLMs, such as data availability, model size, and complexity?</p>
<p>Consider some of the factors that affect the quality and reliability
of domain-specific LLMs, such as the amount and quality of
domain-specific data, the computational resources and time required for
training or fine-tuning, and the generalization and robustness of the
model. How do these factors pose problems or difficulties for
domain-specific LLMs and how can we overcome them?</p>
</div>
</div>
</div>
<div id="discussion-3" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-3" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion-3"></a>
</h3>
<div class="callout-content">
<p>What are some available approaches for creating domain-specific LLMs,
such as fine-tuning and knowledge distillation?</p>
<p>Consider some of the main steps and techniques for creating
domain-specific LLMs, such as selecting a general LLM, collecting and
preparing domain-specific data, training or fine-tuning the model, and
evaluating and deploying the model. How do these approaches differ from
each other and what are their advantages and disadvantages?</p>
</div>
</div>
</div>
<p>Now let’s try One-shot and Few-shot prompting examples and see how
they can help us to enhance the sensitivity of the LLM to our field of
study: One-shot prompting involves providing the model with a single
example to follow. It is like giving the model a hint about what you
expect. We will go through an example using Hugging Face’s transformers
library:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># Load a pre-trained model and tokenizer</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"gpt2"</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span>model_name)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co"># One-shot example</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Translate 'Hello, how are you?' to French:</span><span class="ch">\n</span><span class="st">Bonjour, comment ça va?</span><span class="ch">\n</span><span class="st">Translate 'I am learning new things every day' to French:"</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>result <span class="op">=</span> generator(prompt, max_length<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co"># Output the result</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="bu">print</span>(result[<span class="dv">0</span>][<span class="st">'generated_text'</span>])</span></code></pre>
</div>
<p>In this example, we provide the model with one translation example
and then ask it to translate a new sentence. The model uses the context
from the one-shot example to generate the translation.</p>
<p>But what if we have a Few-Shot Prompting? Few-shot prompting gives
the model several examples to learn from. This can improve the model’s
ability to understand and complete the task.</p>
<p>Here is how you can implement few-shot prompting:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># Load a pre-trained model and tokenizer</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"gpt2"</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span>model_name)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="co"># Few-shot examples</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"""</span><span class="ch">\</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="st">Q: What is the capital of France?</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a><span class="st">A: Paris.</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a><span class="st">Q: What is the largest mammal?</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a><span class="st">A: Blue whale.</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a><span class="st">Q: What is the human body's largest organ?</span></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a><span class="st">A: The skin.</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a><span class="st">Q: What is the currency of Japan?</span></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a><span class="st">A:"""</span></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>result <span class="op">=</span> generator(prompt, max_length<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a><span class="co"># Output the result</span></span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a><span class="bu">print</span>(result[<span class="dv">0</span>][<span class="st">'generated_text'</span>])</span></code></pre>
</div>
<p>In this few-shot example, we provide the model with three
question-answer pairs before posing a new question. The model uses the
pattern it learned from the examples to answer the new question.</p>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge-1"></a>
</h3>
<div class="callout-content">
<p>To summarize this approach in a few steps, fill in the following
gaps: 1. Choose a Model: Select a <strong>—</strong> model from Hugging
Face that suits your task.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Load the Model: Use the <strong>—</strong> function to load the
model and tokenizer.</p></li>
<li><p>Craft Your Prompt: Write a <strong>—</strong> that includes one
or more examples, depending on whether you’re doing one-shot or few-shot
prompting.</p></li>
<li><p>Generate Text: Call the <strong>—</strong> with your prompt to
generate the <strong>—</strong>.</p></li>
<li><p>Review the Output: Check the generated text to see if the model
followed the <strong>—</strong> correctly.</p></li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li><p>Choose a Model: Select a <strong>pre-trained</strong> model from
Hugging Face that suits your task.</p></li>
<li><p>Load the Model: Use the <strong>pipeline</strong> function to
load the model and tokenizer.</p></li>
<li><p>Craft Your Prompt: Write a <strong>prompt</strong> that includes
one or more examples, depending on whether you’re doing one-shot or
few-shot prompting.</p></li>
<li><p>Generate Text: Call the <strong>generator</strong> with your
prompt to generate the <strong>output</strong>.</p></li>
<li><p>Review the Output: Check the generated text to see if the model
followed the <strong>examples</strong> correctly.</p></li>
<li>
</li>
</ol>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler8" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler8" aria-expanded="false" aria-controls="collapseSpoiler8">
  <h3 class="accordion-header" id="headingSpoiler8">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Prompting Quality</h3>
</button>
<div id="collapseSpoiler8" class="accordion-collapse collapse" aria-labelledby="headingSpoiler8" data-bs-parent="#accordionSpoiler8">
<div class="accordion-body">
<p>Remember, the quality of the output heavily depends on the quality
and relevance of the examples you provide. It’s also important to note
that larger models tend to perform better at these tasks due to their
greater capacity to understand and generalize from examples.</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Domain-specific LLMs are essential for tasks that require
specialized knowledge.</li>
<li>Prompt engineering, RAG, fine-tuning, and training from scratch are
viable approaches to create DSLs.</li>
<li>A mixed prompting-RAG approach is often preferred for its balance
between performance and resource efficiency.</li>
<li>Training from scratch offers the highest quality output but requires
significant resources.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-08-conclusion-final-project"><p>Content from <a href="08-conclusion-final-project.html">Wrap-up and Final Project</a></p>
<hr>
<p>Last updated on 2024-05-12 |
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/episodes/08-conclusion-final-project.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 11 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are the core concepts and techniques we’ve learned about NLP
and LLMs?</li>
<li>How can these techniques be applied to solve real-world
problems?</li>
<li>What are the future directions and opportunities in NLP?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>To be able to synthesize the key concepts from each episode.</li>
<li>To plan a path for further learning and exploration in NLP and
LLMs.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="takeaway-from-this-workshop"><h2 class="section-heading">8.1. Takeaway from This Workshop<a class="anchor" aria-label="anchor" href="#takeaway-from-this-workshop"></a>
</h2>
<hr class="half-width">
<p>We have covered a vast landscape of NLP, starting with the basics and
moving towards the intricacies of LLMs. Here is a brief recap to
illustrate our journey:</p>
<ul>
<li>
<strong>Text Preprocessing</strong>: Imagine cleaning a dataset of
tweets for sentiment analysis. We learned how to remove noise and
prepare the text for accurate classification.</li>
<li>
<strong>Text Analysis</strong>: Consider the task of extracting key
information from news articles. Techniques like Named Entity Recognition
helped us identify and categorize entities within the text.</li>
<li>
<strong>Word Embedding</strong>: We explored how words can be
converted into vectors, enabling us to capture semantic relationships,
as seen in the Word2Vec algorithm.</li>
<li>
<strong>Transformers and LLMs</strong>: We saw how transformers like
BERT and GPT can be fine-tuned for tasks such as summarizing medical
research papers and showcasing their power and flexibility.</li>
</ul>
<div id="quiz" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="quiz" class="callout-inner">
<h3 class="callout-title">Quiz<a class="anchor" aria-label="anchor" href="#quiz"></a>
</h3>
<div class="callout-content">
<p><strong>A)</strong> Stemming</p>
<p><strong>B)</strong> Word2Vec</p>
<p><strong>C)</strong> Text Preprocessing</p>
<p><strong>D)</strong> Part-of-Speech Tagging</p>
<p><strong>E)</strong> Stop-words Removal</p>
<p><strong>F)</strong> Transformers</p>
<p><strong>G)</strong> Bag of Words</p>
<p><strong>H)</strong> Tokenization</p>
<p><strong>I)</strong> BERT</p>
<p><strong>J)</strong> Lemmatization</p>
<p><strong>1. A statistical approach to modeling the meaning of words
based on their context.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>2. “A process of reducing words to their root form, enabling
the analysis of word frequency.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>3. An algorithm that uses neural networks to understand the
relationships and meanings in human language.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>4. A technique for identifying the parts of speech for each
word in a given sentence.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>5. A method for cleaning and preparing text data before
analysis.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>6. A library that provides tools for machine learning and
statistical modeling.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>7. A model that predicts the next word in a sentence based on
the words that come before it.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>8. A framework for building and training neural networks to
understand and generate human language.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>9. A technique that groups similar words together in vector
space.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
<p><strong>10. A method for removing commonly used words that carry
little meaning.</strong></p>
<p>[ ] A - [ ] B - [ ] C - [ ] D - [ ] E - [ ] F - [ ] G - [ ] H - [ ] I
- [ ] J</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>A: 1</p>
</div>
</div>
</div>
</div>
<div id="discussion" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="discussion" class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h3>
<div class="callout-content">
<p><strong>Field of Interest</strong></p>
<p>Teamwork: Share insights on how NLP can be applied in your field of
interest.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p><strong><em>Environmental Science</em></strong></p>
<ul>
<li>NLP for Climate Change Research: How can NLP help in analyzing large
volumes of research papers on climate change to identify trends and gaps
in the literature?</li>
<li>Social Media Analysis for Environmental Campaigns: Discuss the use
of sentiment analysis to gauge public opinion on environmental
policies.</li>
<li>Automating Environmental Compliance: Share insights on how NLP can
streamline the process of checking compliance with environmental
regulations in corporate documents.</li>
</ul>
<p><strong><em>Education</em></strong></p>
<ul>
<li>Personalized Learning: Explore the potential of NLP in creating
personalized learning experiences by analyzing student feedback and
performance.</li>
<li>Content Summarization: Discuss the benefits of using NLP to
summarize educational content for quick revision.</li>
<li>Language Learning: Share thoughts on the role of NLP in developing
language learning applications that adapt to the learner’s proficiency
level.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="mini-project-using-an-llm" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="mini-project-using-an-llm" class="callout-inner">
<h3 class="callout-title">Mini-Project: Using an LLM<a class="anchor" aria-label="anchor" href="#mini-project-using-an-llm"></a>
</h3>
<div class="callout-content">
<p>Context Example: Environmental science and climate change Using
Hugging Face model distilbert-base-uncased and Few-Shot Prompting: To
improve the model’s performance in answering field-specific questions,
we will use few-shot prompting by providing examples of questions and
answers related to environmental topics.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co"># Initialize the question-answering pipeline with DistilBERT</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>qa_pipeline <span class="op">=</span> pipeline(<span class="st">'question-answering'</span>, model<span class="op">=</span><span class="st">'distilbert-base-uncased'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Few-shot prompting with examples</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>context <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="st">Question: What is the greenhouse effect?</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="st">Answer: The greenhouse effect is a natural process that warms the Earth's surface.</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="st">Question: How can we reduce carbon emissions?</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="st">Answer: We can reduce carbon emissions by using renewable energy sources, improving energy efficiency, and planting trees.</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="st">Question: What are the consequences of deforestation?</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="st">Answer: Deforestation can lead to loss of biodiversity, increased greenhouse gas emissions, and disruption of water cycles.</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="co"># User's field-specific question</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>user_question <span class="op">=</span> <span class="st">"What can individuals do to combat climate change?"</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="co"># Prepare the prompt for the model</span></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>prompt <span class="op">=</span> {</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>    <span class="st">'context'</span>: context,</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>    <span class="st">'question'</span>: user_question</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>}</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a><span class="co"># Get the answer from the model</span></span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>response <span class="op">=</span> qa_pipeline(prompt)</span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="bu">print</span>(response[<span class="st">'answer'</span>])</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>The model should provide a relevant answer based on the few-shot
examples provided. For instance, it might say: “Individuals can combat
climate change by reducing their carbon footprint, using less energy,
recycling, and supporting eco-friendly policies”.</p>
<p>In this challenge, we used the
<strong>distilbert-base-uncased</strong> model from Hugging Face’s
transformers library to create a question-answering system. Few-shot
prompting is employed to give the model context about environmental
topics, which helps it generate more accurate answers to user queries.
The <strong>qa_pipeline</strong> function is used to pass the prompt to
the model, which then processes the information and returns an answer to
the user’s question.</p>
<p>This mini-project showcases how LLMs can be fine-tuned to specific
fields of interest, providing valuable assistance in answering
domain-specific queries.</p>
</div>
</div>
</div>
</div>
</section><section id="further-resources"><h2 class="section-heading">8.2. Further Resources<a class="anchor" aria-label="anchor" href="#further-resources"></a>
</h2>
<hr class="half-width">
<p>For continued learning, here are detailed resources:</p>
<ul>
<li>
<em>Natural Language Processing Specialization (Coursera)</em>: A
series of courses that cover NLP foundations, algorithms, and how to
build NLP applications.</li>
<li>
<em>Stanford NLP Group</em>: Access to pioneering NLP research,
datasets, and tools like Stanford Parser and Stanford POS Tagger.</li>
<li>
<em>Hugging Face</em>: A platform for sharing and collaborating on
ML models, with a focus on democratizing NLP technologies.</li>
<li>
<em>Kaggle</em>: An online community for data scientists, offering
datasets, notebooks, and competitions to practice and improve your NLP
skills.</li>
</ul>
<p>Each resource is a gateway to further knowledge, community
engagement, and hands-on experience.</p>
</section><section id="feedback"><h2 class="section-heading">8.3. Feedback<a class="anchor" aria-label="anchor" href="#feedback"></a>
</h2>
<hr class="half-width">
<p>Please help us improve by answering the following survey
questions:</p>
<p><strong>1. How would you rate the overall quality of the
workshop?</strong></p>
<p>[ ] Excellent, [ ] Good, [ ] Average, [ ] Below Average, [ ] Poor</p>
<p><strong>2. Was the pace of the workshop appropriate?</strong></p>
<p>[ ] Too fast, [ ] Just right, [ ] Too slow</p>
<p><strong>3. How clear were the instructions and
explanations?</strong></p>
<p>[ ] Very clear, [ ] Clear, [ ] Somewhat clear, [ ] Not clear</p>
<p><strong>4. What was the most valuable part of the workshop for
you?</strong></p>
<p><strong>5. How can we improve the workshop for future
participants?</strong></p>
<p><em>Your feedback is crucial for us to evolve and enhance the
learning experience.</em></p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Various NLP techniques from preprocessing to advanced LLMs are
reviewed.</li>
<li>NLPs’ transformative potential provides real-world applications in
diverse fields.</li>
<li>Few-shot learning can enhance the performance of LLMs for specific
fields of research.</li>
<li>Valuable resources are highlighted for continued learning and
exploration in the field of NLP.</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/edit/main/README.md" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/" class="external-link">Source</a></p>
				<p><a href="https://github.com/qcif-training/intro_nlp_lmm_v1.0/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:training@qcif.edu.au">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.4" class="external-link">sandpaper (0.16.4)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.5" class="external-link">pegboard (0.7.5)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.2" class="external-link">varnish (1.0.2)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/instructor/aio.html",
  "identifier": "https://qcif-training.github.io/intro_nlp_lmm_v1.0/instructor/aio.html",
  "dateCreated": "2024-05-10",
  "dateModified": "2024-05-12",
  "datePublished": "2024-05-12"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

